{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42189075-2b3f-407f-9a2f-69b24d66535b",
   "metadata": {},
   "source": [
    "# Testing Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27700e00-69a1-4870-aeb6-42d825e0ccde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import importlib\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e530a33-c460-42a2-951a-25f56f78f091",
   "metadata": {},
   "outputs": [],
   "source": [
    "import koopomics.model.model_loader as ko\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b7ca3d7f-1e96-41a7-aaf3-c542e1b22bad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'koopomics.model.koopmanANN' from '/Users/daviddornig/Documents/Master_Thesis/Bioinf/Code/philipp-trinh/KOOPOMICS/koopomics/model/koopmanANN.py'>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import koopomics.model.embeddingANN as em\n",
    "import koopomics.model.koopmanANN as op\n",
    "importlib.reload(em)\n",
    "importlib.reload(op)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "728a20af-6e10-4570-afb6-390ca05c643d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'koopomics.model.model_loader' from '/Users/daviddornig/Documents/Master_Thesis/Bioinf/Code/philipp-trinh/KOOPOMICS/koopomics/model/model_loader.py'>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(ko)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfb4f7df-4850-4a83-865a-2f51ac58dc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = em.FF_AE([200,100,100,20], [20,100,100,200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73866519-c876-4eec-9073-bda346b3c165",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = em.DiffeomMap([200,100,100,20],[20,100,100,200],[1,3,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "422d2826-8033-4004-98cb-7ad34b0f0ebd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DiffeomMap(\n",
       "  (encode_NN): Sequential(\n",
       "    (0): Linear(in_features=200, out_features=100, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=100, out_features=20, bias=True)\n",
       "  )\n",
       "  (deconv_liftNN): Sequential(\n",
       "    (0): Linear(in_features=20, out_features=100, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=100, out_features=200, bias=True)\n",
       "  )\n",
       "  (deconv_outputNN): ModuleList(\n",
       "    (0-199): 200 x Sequential(\n",
       "      (0): Linear(in_features=1, out_features=3, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=3, out_features=5, bias=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "872179ff-4bfd-4924-8499-1789ae70fb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "operator_model = op.LinearizingKoop(linearizer=op.FFLinearizer([20,30,50], [50,30,20]), koop=op.InvKoop(latent_dim=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2c1ec7b6-df1c-4d6a-b951-101dcf75446f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FF_AE\n"
     ]
    }
   ],
   "source": [
    "embedding_model = em.FF_AE([264,100,100,20], [20,100,100,264])\n",
    "operator_model = op.LinearizingKoop(linearizer=op.FFLinearizer([20,30,50], [50,30,20]), koop=op.InvKoop(latent_dim=50))\n",
    "\n",
    "LinKoopAE_model = ko.KoopmanModel(embedding=embedding_model, operator=operator_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0f42c5c-9665-49fe-a506-1636ff0b5264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KoopmanModel(\n",
       "  (embedding): FF_AE(\n",
       "    (encode): Sequential(\n",
       "      (0): Linear(in_features=200, out_features=100, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=100, out_features=100, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=100, out_features=20, bias=True)\n",
       "    )\n",
       "    (decode): Sequential(\n",
       "      (0): Linear(in_features=20, out_features=100, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=100, out_features=100, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=100, out_features=200, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (operator): LinearizingKoop(\n",
       "    (linearizer): FFLinearizer(\n",
       "      (lin_encode): Sequential(\n",
       "        (0): Linear(in_features=20, out_features=30, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=30, out_features=50, bias=True)\n",
       "      )\n",
       "      (lin_decode): Sequential(\n",
       "        (0): Linear(in_features=50, out_features=30, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=30, out_features=20, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (koop): InvKoop()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LinKoopAE_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ddf4bf9d-a714-4ea2-8a08-cdd2c621f361",
   "metadata": {},
   "outputs": [],
   "source": [
    "pregnancy_df = pd.read_csv('/Users/daviddornig/Documents/Master_Thesis/Bioinf/Code/philipp-trinh/KOOPOMICS/input_data/pregnancy/pregnancy_interpolated_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b08187-2e1d-4b63-879d-fdf643a3a924",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a604eda-dba3-4931-815e-766682b15e7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject ID</th>\n",
       "      <th>Cohort</th>\n",
       "      <th>Labor onset</th>\n",
       "      <th>Gestational age (GA)/days</th>\n",
       "      <th>Gestational age (GA)/weeks</th>\n",
       "      <th>Birth GA/weeks</th>\n",
       "      <th>(+)-.alpha.-Tocopherol</th>\n",
       "      <th>(R)-2-Hydroxycaprylic acid</th>\n",
       "      <th>1,4-Dihydroxybenzene</th>\n",
       "      <th>1-Methylxanthine</th>\n",
       "      <th>...</th>\n",
       "      <th>cis-5,8,11,14,17-Eicosapentaenoic acid</th>\n",
       "      <th>cis-5-Dodecenoic acid</th>\n",
       "      <th>cis-9-Palmitoleic acid</th>\n",
       "      <th>d-LIMONENE</th>\n",
       "      <th>gamma-CEHC</th>\n",
       "      <th>gamma-Glutamylleucine</th>\n",
       "      <th>ketoisocaproic acid</th>\n",
       "      <th>p-Cresol</th>\n",
       "      <th>phenylacetylglutamine</th>\n",
       "      <th>trans-Vaccenic acid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DP01</td>\n",
       "      <td>Discovery</td>\n",
       "      <td>natural</td>\n",
       "      <td>35</td>\n",
       "      <td>5</td>\n",
       "      <td>38.714286</td>\n",
       "      <td>12.187246</td>\n",
       "      <td>10.075398</td>\n",
       "      <td>9.315506</td>\n",
       "      <td>10.907293</td>\n",
       "      <td>...</td>\n",
       "      <td>11.183291</td>\n",
       "      <td>10.799411</td>\n",
       "      <td>12.059483</td>\n",
       "      <td>5.643133</td>\n",
       "      <td>10.495975</td>\n",
       "      <td>12.157863</td>\n",
       "      <td>14.377687</td>\n",
       "      <td>10.635153</td>\n",
       "      <td>14.263246</td>\n",
       "      <td>11.150043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DP01</td>\n",
       "      <td>Discovery</td>\n",
       "      <td>natural</td>\n",
       "      <td>42</td>\n",
       "      <td>6</td>\n",
       "      <td>38.714286</td>\n",
       "      <td>12.643866</td>\n",
       "      <td>9.871381</td>\n",
       "      <td>7.523938</td>\n",
       "      <td>10.954945</td>\n",
       "      <td>...</td>\n",
       "      <td>11.018465</td>\n",
       "      <td>10.230143</td>\n",
       "      <td>11.844556</td>\n",
       "      <td>5.497642</td>\n",
       "      <td>9.847823</td>\n",
       "      <td>12.110144</td>\n",
       "      <td>14.217800</td>\n",
       "      <td>10.610887</td>\n",
       "      <td>14.001904</td>\n",
       "      <td>10.924683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DP01</td>\n",
       "      <td>Discovery</td>\n",
       "      <td>natural</td>\n",
       "      <td>49</td>\n",
       "      <td>7</td>\n",
       "      <td>38.714286</td>\n",
       "      <td>13.632884</td>\n",
       "      <td>10.025761</td>\n",
       "      <td>4.431244</td>\n",
       "      <td>9.715949</td>\n",
       "      <td>...</td>\n",
       "      <td>10.537209</td>\n",
       "      <td>9.326564</td>\n",
       "      <td>11.616709</td>\n",
       "      <td>6.736850</td>\n",
       "      <td>9.342694</td>\n",
       "      <td>12.174268</td>\n",
       "      <td>14.225116</td>\n",
       "      <td>10.848017</td>\n",
       "      <td>14.178476</td>\n",
       "      <td>10.705275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DP01</td>\n",
       "      <td>Discovery</td>\n",
       "      <td>natural</td>\n",
       "      <td>56</td>\n",
       "      <td>8</td>\n",
       "      <td>38.714286</td>\n",
       "      <td>12.125875</td>\n",
       "      <td>10.253546</td>\n",
       "      <td>5.250766</td>\n",
       "      <td>9.805404</td>\n",
       "      <td>...</td>\n",
       "      <td>10.672192</td>\n",
       "      <td>8.790673</td>\n",
       "      <td>11.874221</td>\n",
       "      <td>7.195789</td>\n",
       "      <td>10.022338</td>\n",
       "      <td>12.604769</td>\n",
       "      <td>14.749239</td>\n",
       "      <td>10.571532</td>\n",
       "      <td>14.042197</td>\n",
       "      <td>11.155579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DP01</td>\n",
       "      <td>Discovery</td>\n",
       "      <td>natural</td>\n",
       "      <td>63</td>\n",
       "      <td>9</td>\n",
       "      <td>38.714286</td>\n",
       "      <td>12.657642</td>\n",
       "      <td>10.162621</td>\n",
       "      <td>8.130034</td>\n",
       "      <td>10.349446</td>\n",
       "      <td>...</td>\n",
       "      <td>10.788551</td>\n",
       "      <td>8.972928</td>\n",
       "      <td>11.831649</td>\n",
       "      <td>6.579638</td>\n",
       "      <td>10.032035</td>\n",
       "      <td>12.329358</td>\n",
       "      <td>14.939134</td>\n",
       "      <td>10.303965</td>\n",
       "      <td>13.876263</td>\n",
       "      <td>11.085732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>696</th>\n",
       "      <td>DP30</td>\n",
       "      <td>Validation (Test Set 1)</td>\n",
       "      <td>natural</td>\n",
       "      <td>245</td>\n",
       "      <td>35</td>\n",
       "      <td>38.857143</td>\n",
       "      <td>12.115260</td>\n",
       "      <td>11.037353</td>\n",
       "      <td>6.541048</td>\n",
       "      <td>10.235494</td>\n",
       "      <td>...</td>\n",
       "      <td>11.053052</td>\n",
       "      <td>9.530414</td>\n",
       "      <td>12.100110</td>\n",
       "      <td>6.634040</td>\n",
       "      <td>9.352964</td>\n",
       "      <td>12.271830</td>\n",
       "      <td>14.366054</td>\n",
       "      <td>10.805234</td>\n",
       "      <td>14.887704</td>\n",
       "      <td>11.551004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>DP30</td>\n",
       "      <td>Validation (Test Set 1)</td>\n",
       "      <td>natural</td>\n",
       "      <td>252</td>\n",
       "      <td>36</td>\n",
       "      <td>38.857143</td>\n",
       "      <td>12.469027</td>\n",
       "      <td>11.543126</td>\n",
       "      <td>6.903586</td>\n",
       "      <td>10.341836</td>\n",
       "      <td>...</td>\n",
       "      <td>11.048241</td>\n",
       "      <td>9.437662</td>\n",
       "      <td>12.121246</td>\n",
       "      <td>6.724391</td>\n",
       "      <td>9.135123</td>\n",
       "      <td>11.884873</td>\n",
       "      <td>14.472117</td>\n",
       "      <td>10.867624</td>\n",
       "      <td>14.675015</td>\n",
       "      <td>10.905095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>DP30</td>\n",
       "      <td>Validation (Test Set 1)</td>\n",
       "      <td>natural</td>\n",
       "      <td>259</td>\n",
       "      <td>37</td>\n",
       "      <td>38.857143</td>\n",
       "      <td>14.027342</td>\n",
       "      <td>10.293517</td>\n",
       "      <td>7.147000</td>\n",
       "      <td>10.042527</td>\n",
       "      <td>...</td>\n",
       "      <td>10.755617</td>\n",
       "      <td>9.205752</td>\n",
       "      <td>12.077126</td>\n",
       "      <td>6.181048</td>\n",
       "      <td>9.846376</td>\n",
       "      <td>12.123202</td>\n",
       "      <td>14.680196</td>\n",
       "      <td>10.440013</td>\n",
       "      <td>14.345236</td>\n",
       "      <td>10.808394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>DP30</td>\n",
       "      <td>Validation (Test Set 1)</td>\n",
       "      <td>natural</td>\n",
       "      <td>266</td>\n",
       "      <td>38</td>\n",
       "      <td>38.857143</td>\n",
       "      <td>12.575002</td>\n",
       "      <td>10.068778</td>\n",
       "      <td>9.200918</td>\n",
       "      <td>9.555815</td>\n",
       "      <td>...</td>\n",
       "      <td>10.292866</td>\n",
       "      <td>9.692128</td>\n",
       "      <td>12.429983</td>\n",
       "      <td>6.789644</td>\n",
       "      <td>8.926737</td>\n",
       "      <td>12.228364</td>\n",
       "      <td>14.663350</td>\n",
       "      <td>10.208924</td>\n",
       "      <td>13.998535</td>\n",
       "      <td>11.138388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>DP30</td>\n",
       "      <td>Validation (Test Set 1)</td>\n",
       "      <td>natural</td>\n",
       "      <td>273</td>\n",
       "      <td>39</td>\n",
       "      <td>38.857143</td>\n",
       "      <td>13.453395</td>\n",
       "      <td>10.114277</td>\n",
       "      <td>8.743941</td>\n",
       "      <td>8.244443</td>\n",
       "      <td>...</td>\n",
       "      <td>9.950208</td>\n",
       "      <td>9.645435</td>\n",
       "      <td>12.059390</td>\n",
       "      <td>7.880887</td>\n",
       "      <td>9.496995</td>\n",
       "      <td>12.610548</td>\n",
       "      <td>14.818217</td>\n",
       "      <td>10.620437</td>\n",
       "      <td>14.579306</td>\n",
       "      <td>11.317374</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>701 rows × 270 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Subject ID                   Cohort Labor onset  \\\n",
       "0         DP01                Discovery     natural   \n",
       "1         DP01                Discovery     natural   \n",
       "2         DP01                Discovery     natural   \n",
       "3         DP01                Discovery     natural   \n",
       "4         DP01                Discovery     natural   \n",
       "..         ...                      ...         ...   \n",
       "696       DP30  Validation (Test Set 1)     natural   \n",
       "697       DP30  Validation (Test Set 1)     natural   \n",
       "698       DP30  Validation (Test Set 1)     natural   \n",
       "699       DP30  Validation (Test Set 1)     natural   \n",
       "700       DP30  Validation (Test Set 1)     natural   \n",
       "\n",
       "     Gestational age (GA)/days  Gestational age (GA)/weeks  Birth GA/weeks  \\\n",
       "0                           35                           5       38.714286   \n",
       "1                           42                           6       38.714286   \n",
       "2                           49                           7       38.714286   \n",
       "3                           56                           8       38.714286   \n",
       "4                           63                           9       38.714286   \n",
       "..                         ...                         ...             ...   \n",
       "696                        245                          35       38.857143   \n",
       "697                        252                          36       38.857143   \n",
       "698                        259                          37       38.857143   \n",
       "699                        266                          38       38.857143   \n",
       "700                        273                          39       38.857143   \n",
       "\n",
       "     (+)-.alpha.-Tocopherol  (R)-2-Hydroxycaprylic acid  1,4-Dihydroxybenzene  \\\n",
       "0                 12.187246                   10.075398              9.315506   \n",
       "1                 12.643866                    9.871381              7.523938   \n",
       "2                 13.632884                   10.025761              4.431244   \n",
       "3                 12.125875                   10.253546              5.250766   \n",
       "4                 12.657642                   10.162621              8.130034   \n",
       "..                      ...                         ...                   ...   \n",
       "696               12.115260                   11.037353              6.541048   \n",
       "697               12.469027                   11.543126              6.903586   \n",
       "698               14.027342                   10.293517              7.147000   \n",
       "699               12.575002                   10.068778              9.200918   \n",
       "700               13.453395                   10.114277              8.743941   \n",
       "\n",
       "     1-Methylxanthine  ...  cis-5,8,11,14,17-Eicosapentaenoic acid  \\\n",
       "0           10.907293  ...                               11.183291   \n",
       "1           10.954945  ...                               11.018465   \n",
       "2            9.715949  ...                               10.537209   \n",
       "3            9.805404  ...                               10.672192   \n",
       "4           10.349446  ...                               10.788551   \n",
       "..                ...  ...                                     ...   \n",
       "696         10.235494  ...                               11.053052   \n",
       "697         10.341836  ...                               11.048241   \n",
       "698         10.042527  ...                               10.755617   \n",
       "699          9.555815  ...                               10.292866   \n",
       "700          8.244443  ...                                9.950208   \n",
       "\n",
       "     cis-5-Dodecenoic acid  cis-9-Palmitoleic acid  d-LIMONENE  gamma-CEHC  \\\n",
       "0                10.799411               12.059483    5.643133   10.495975   \n",
       "1                10.230143               11.844556    5.497642    9.847823   \n",
       "2                 9.326564               11.616709    6.736850    9.342694   \n",
       "3                 8.790673               11.874221    7.195789   10.022338   \n",
       "4                 8.972928               11.831649    6.579638   10.032035   \n",
       "..                     ...                     ...         ...         ...   \n",
       "696               9.530414               12.100110    6.634040    9.352964   \n",
       "697               9.437662               12.121246    6.724391    9.135123   \n",
       "698               9.205752               12.077126    6.181048    9.846376   \n",
       "699               9.692128               12.429983    6.789644    8.926737   \n",
       "700               9.645435               12.059390    7.880887    9.496995   \n",
       "\n",
       "     gamma-Glutamylleucine  ketoisocaproic acid   p-Cresol  \\\n",
       "0                12.157863            14.377687  10.635153   \n",
       "1                12.110144            14.217800  10.610887   \n",
       "2                12.174268            14.225116  10.848017   \n",
       "3                12.604769            14.749239  10.571532   \n",
       "4                12.329358            14.939134  10.303965   \n",
       "..                     ...                  ...        ...   \n",
       "696              12.271830            14.366054  10.805234   \n",
       "697              11.884873            14.472117  10.867624   \n",
       "698              12.123202            14.680196  10.440013   \n",
       "699              12.228364            14.663350  10.208924   \n",
       "700              12.610548            14.818217  10.620437   \n",
       "\n",
       "     phenylacetylglutamine  trans-Vaccenic acid  \n",
       "0                14.263246            11.150043  \n",
       "1                14.001904            10.924683  \n",
       "2                14.178476            10.705275  \n",
       "3                14.042197            11.155579  \n",
       "4                13.876263            11.085732  \n",
       "..                     ...                  ...  \n",
       "696              14.887704            11.551004  \n",
       "697              14.675015            10.905095  \n",
       "698              14.345236            10.808394  \n",
       "699              13.998535            11.138388  \n",
       "700              14.579306            11.317374  \n",
       "\n",
       "[701 rows x 270 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pregnancy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe5f3233-d83f-486b-8308-a07d1a0ba040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subject ID = Sample identifier\n",
    "# feature list\n",
    "# Time identifier\n",
    "\n",
    "dfs = {subject_id: group for subject_id, group in pregnancy_df.groupby('Subject ID')}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9eb88013-ab1a-48e8-8423-43fa869a1727",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, df, feature_list, sample_id='Subject ID', time_id=''):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df (pd.DataFrame): The dataframe containing all the data.\n",
    "            feature_list (list): List of columns to be used as features.\n",
    "            sample_id (str): The column name representing the sample grouping (e.g., 'Subject ID').\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.feature_list = feature_list\n",
    "        self.sample_id = sample_id\n",
    "        \n",
    "        self.grouped = self.df.groupby(sample_id)\n",
    "        self.sample_ids = list(self.grouped.groups.keys()) \n",
    "        self.time_id = time_id\n",
    "    \n",
    "    def __len__(self):\n",
    "        # Return the number of unique samples (i.e., groups based on sample_id)\n",
    "        return len(self.sample_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get the sample ID based on the index\n",
    "        current_sample_id = self.sample_ids[idx]\n",
    "        \n",
    "        # Retrieve all rows for this sample (time points) and filter by features\n",
    "        sample_df = self.grouped.get_group(current_sample_id)[self.feature_list]\n",
    "        row_indices = self.grouped.get_group(current_sample_id).index\n",
    "        time_indices = self.grouped.get_group(current_sample_id)[self.time_id].round().astype(int).values\n",
    "        \n",
    "        # Convert the filtered DataFrame (time points x features) to a tensor [num_time_points [num_features]]\n",
    "        input_data = torch.tensor(sample_df.values.astype(np.float32))  # Shape: (num_time_points, num_features)\n",
    "        \n",
    "        \n",
    "        return {\n",
    "            'input_data': input_data,  # Input data as a 2D tensor (time points, features)\n",
    "            'row_ids': row_indices.tolist(),\n",
    "            'sample_id': current_sample_id,  # The sample ID (e.g., 'Subject ID')\n",
    "            'time_ids' : time_indices.tolist()\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Collect the input_data for each sample\n",
    "    input_data = [item['input_data'] for item in batch]  # This will be a list of tensors of varying size\n",
    "    \n",
    "    # Collect the sample IDs and row indices for each sample\n",
    "    sample_ids = [item['sample_id'] for item in batch]\n",
    "    row_indices = [item['row_ids'] for item in batch]\n",
    "    time_indices = [item['time_ids'] for item in batch]\n",
    "    \n",
    "    return {\n",
    "        'input_data': input_data,  # List of 2D tensors, each with (num_time_points, num_features)\n",
    "        'sample_id': sample_ids,  # List of sample IDs\n",
    "        'row_ids': row_indices,  # List of row index lists\n",
    "        'time_ids': time_indices # List of time index lists\n",
    "    }\n",
    "\n",
    "\n",
    "# Initialize dataset\n",
    "feature_list = pregnancy_df.columns[6:]\n",
    "\n",
    "dataset = TimeSeriesDataset(pregnancy_df, feature_list, sample_id='Subject ID', time_id='Gestational age (GA)/weeks')\n",
    "\n",
    "# Create a DataLoader for batch processing\n",
    "dataloader = DataLoader(dataset, batch_size=5, shuffle=True, collate_fn=collate_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "564baf4c-d715-4708-8c0d-7a142918fad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def get_dynamic_targets(dataframe, feature_list, sample_id, sample_ids, time_id, time_ids, fwd=0, bwd=0):\n",
    "    \"\"\"\n",
    "    Get dynamic targets based on forward and backward prediction indices.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): The original DataFrame containing all data.\n",
    "        sample_id (string): The df identifier for samples (e.g. 'Subject ID').\n",
    "        sample_ids (list): List of unique sample IDs to filter the DataFrame.\n",
    "        time_id (string): The df identifier for timepoints (e.g. 'week').\n",
    "        time_ids (list): List of time indices for the target calculation.\n",
    "        fwd (int, optional): Number of forward time steps to look ahead. Should be > 0.\n",
    "        bwd (int, optional): Number of backward time steps to look back. Should be > 0.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tensors, each containing the dynamic targets for a sample.\n",
    "    \"\"\"\n",
    "\n",
    "    # Validate inputs\n",
    "    if (fwd <= 0 and bwd <= 0) or (fwd > 0 and bwd > 0):\n",
    "        raise ValueError(\"At least one of 'fwd' or 'bwd' must be specified, not both and > 0.\")\n",
    "\n",
    "    dynamic_target_rows = []\n",
    "    dynamic_target_ids = []\n",
    "    dynamic_target_time_ids = []\n",
    "    comparable_booleans = []\n",
    "\n",
    "    # Iterate over each sample ID\n",
    "    for i, sample in enumerate(sample_ids):\n",
    "        # Filter the DataFrame for the current sample\n",
    "        sample_df = dataframe[dataframe[sample_id] == sample]\n",
    "        filtered_sample_df = sample_df[feature_list]\n",
    "\n",
    "        # Initialize a list to collect target rows\n",
    "        target_rows = []\n",
    "        target_indices = []\n",
    "        comparable_targets = []\n",
    "\n",
    "        # Calculate the forward or backward indices\n",
    "        if fwd != 0:\n",
    "            target_time_idx_fwd = [time + fwd for time in time_ids[i]]\n",
    "            \n",
    "            dynamic_target_time_ids.append(target_time_idx_fwd)\n",
    "            \n",
    "            target_time_ids = target_time_idx_fwd\n",
    "\n",
    "        else:\n",
    "            target_idx_fwd = None\n",
    "        \n",
    "        if bwd != 0:\n",
    "            target_time_idx_bwd = [time - bwd for time in time_ids[i]]\n",
    "            \n",
    "            dynamic_target_time_ids.append(target_time_idx_bwd)\n",
    "            \n",
    "            target_time_ids = target_time_idx_bwd\n",
    "\n",
    "        else:\n",
    "            target_idx_bwd = None\n",
    "            \n",
    "        # Retrieve the rows for the calculated indices, or NaN if out of bounds\n",
    "        for time in target_time_ids:\n",
    "\n",
    "            if time in time_ids[i]: # Check if there exist gaps in timepoints.\n",
    "                row_id = sample_df[sample_df[time_id] == time].index[0]\n",
    "                target_rows.append(filtered_sample_df.loc[row_id].values)  # Add existing row values\n",
    "                comparable_targets.append(True)\n",
    "                target_indices.append(row_id)\n",
    "\n",
    "            else:\n",
    "                target_rows.append([np.nan] * len(filtered_sample_df.columns))  # Add NaN row values\n",
    "                comparable_targets.append(False)\n",
    "\n",
    "        # Add list of target_indices to total indices list.\n",
    "        dynamic_target_ids.append(target_indices)\n",
    "\n",
    "        # Convert target rows for the current sample into a tensor\n",
    "        target_array = np.array(target_rows)\n",
    "        dynamic_target_rows.append(torch.tensor(target_array, dtype=torch.float32))\n",
    "\n",
    "        # Add comparability booleans\n",
    "        comparable_booleans.append(comparable_targets)\n",
    "\n",
    "    return dynamic_target_rows, dynamic_target_ids, dynamic_target_time_ids, comparable_booleans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29ef7d2c-19f3-4d2c-a9af-26f1127ae527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>(+)-.alpha.-Tocopherol</th>\n",
       "      <th>(R)-2-Hydroxycaprylic acid</th>\n",
       "      <th>1,4-Dihydroxybenzene</th>\n",
       "      <th>1-Methylxanthine</th>\n",
       "      <th>1-Naphthylamine</th>\n",
       "      <th>1-Oleoyl-sn-glycero-3-phosphoethanolamin...</th>\n",
       "      <th>1-Palmitoyl-2-hydroxy-sn-glycero-3-phosp...</th>\n",
       "      <th>1-Stearoyl-sn-glycero-3-phosphocholine</th>\n",
       "      <th>1-Stearoyl-sn-glycerol</th>\n",
       "      <th>1-pentadecanoyl-2-hydroxy-sn-glycero-3-p...</th>\n",
       "      <th>...</th>\n",
       "      <th>cis-5,8,11,14,17-Eicosapentaenoic acid</th>\n",
       "      <th>cis-5-Dodecenoic acid</th>\n",
       "      <th>cis-9-Palmitoleic acid</th>\n",
       "      <th>d-LIMONENE</th>\n",
       "      <th>gamma-CEHC</th>\n",
       "      <th>gamma-Glutamylleucine</th>\n",
       "      <th>ketoisocaproic acid</th>\n",
       "      <th>p-Cresol</th>\n",
       "      <th>phenylacetylglutamine</th>\n",
       "      <th>trans-Vaccenic acid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>12.425802</td>\n",
       "      <td>9.940151</td>\n",
       "      <td>9.346458</td>\n",
       "      <td>10.948009</td>\n",
       "      <td>12.238548</td>\n",
       "      <td>13.257731</td>\n",
       "      <td>12.973654</td>\n",
       "      <td>11.607291</td>\n",
       "      <td>14.632432</td>\n",
       "      <td>14.793555</td>\n",
       "      <td>...</td>\n",
       "      <td>10.890095</td>\n",
       "      <td>10.409081</td>\n",
       "      <td>12.320187</td>\n",
       "      <td>7.020098</td>\n",
       "      <td>10.516925</td>\n",
       "      <td>12.065996</td>\n",
       "      <td>15.078192</td>\n",
       "      <td>11.697351</td>\n",
       "      <td>15.374608</td>\n",
       "      <td>11.736492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>13.488137</td>\n",
       "      <td>9.854098</td>\n",
       "      <td>9.444458</td>\n",
       "      <td>10.368734</td>\n",
       "      <td>12.140843</td>\n",
       "      <td>13.950330</td>\n",
       "      <td>14.065568</td>\n",
       "      <td>11.729910</td>\n",
       "      <td>15.957605</td>\n",
       "      <td>15.858723</td>\n",
       "      <td>...</td>\n",
       "      <td>10.721470</td>\n",
       "      <td>9.898654</td>\n",
       "      <td>12.230804</td>\n",
       "      <td>5.331199</td>\n",
       "      <td>10.157811</td>\n",
       "      <td>11.956406</td>\n",
       "      <td>14.367358</td>\n",
       "      <td>11.957828</td>\n",
       "      <td>15.641462</td>\n",
       "      <td>11.514334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>13.246029</td>\n",
       "      <td>10.221606</td>\n",
       "      <td>9.384963</td>\n",
       "      <td>10.536216</td>\n",
       "      <td>12.199070</td>\n",
       "      <td>13.619825</td>\n",
       "      <td>13.807421</td>\n",
       "      <td>12.153983</td>\n",
       "      <td>15.820099</td>\n",
       "      <td>15.464307</td>\n",
       "      <td>...</td>\n",
       "      <td>10.536693</td>\n",
       "      <td>10.425716</td>\n",
       "      <td>12.393004</td>\n",
       "      <td>6.748009</td>\n",
       "      <td>10.106812</td>\n",
       "      <td>12.287791</td>\n",
       "      <td>14.306134</td>\n",
       "      <td>11.710425</td>\n",
       "      <td>15.507585</td>\n",
       "      <td>11.687867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>12.467784</td>\n",
       "      <td>10.382514</td>\n",
       "      <td>4.543732</td>\n",
       "      <td>11.208936</td>\n",
       "      <td>12.414522</td>\n",
       "      <td>13.640041</td>\n",
       "      <td>13.043924</td>\n",
       "      <td>11.787866</td>\n",
       "      <td>15.949356</td>\n",
       "      <td>15.327827</td>\n",
       "      <td>...</td>\n",
       "      <td>11.003658</td>\n",
       "      <td>10.111463</td>\n",
       "      <td>12.228382</td>\n",
       "      <td>6.140041</td>\n",
       "      <td>9.717163</td>\n",
       "      <td>12.087499</td>\n",
       "      <td>13.597653</td>\n",
       "      <td>11.825458</td>\n",
       "      <td>15.374558</td>\n",
       "      <td>11.739387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>12.489635</td>\n",
       "      <td>10.195934</td>\n",
       "      <td>8.954138</td>\n",
       "      <td>11.247966</td>\n",
       "      <td>12.753416</td>\n",
       "      <td>13.273298</td>\n",
       "      <td>13.546116</td>\n",
       "      <td>11.980306</td>\n",
       "      <td>14.804769</td>\n",
       "      <td>14.768760</td>\n",
       "      <td>...</td>\n",
       "      <td>9.971164</td>\n",
       "      <td>9.401481</td>\n",
       "      <td>12.024649</td>\n",
       "      <td>6.353860</td>\n",
       "      <td>9.969766</td>\n",
       "      <td>12.896635</td>\n",
       "      <td>15.037879</td>\n",
       "      <td>11.981087</td>\n",
       "      <td>15.762672</td>\n",
       "      <td>10.928602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>13.988499</td>\n",
       "      <td>10.082340</td>\n",
       "      <td>10.240554</td>\n",
       "      <td>11.428982</td>\n",
       "      <td>12.117136</td>\n",
       "      <td>13.747037</td>\n",
       "      <td>12.546961</td>\n",
       "      <td>11.940502</td>\n",
       "      <td>15.943817</td>\n",
       "      <td>15.667817</td>\n",
       "      <td>...</td>\n",
       "      <td>10.625949</td>\n",
       "      <td>8.844947</td>\n",
       "      <td>12.172595</td>\n",
       "      <td>5.710784</td>\n",
       "      <td>9.687481</td>\n",
       "      <td>12.042356</td>\n",
       "      <td>14.091961</td>\n",
       "      <td>11.851552</td>\n",
       "      <td>15.602911</td>\n",
       "      <td>11.437537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>13.537356</td>\n",
       "      <td>10.167389</td>\n",
       "      <td>9.441791</td>\n",
       "      <td>10.891179</td>\n",
       "      <td>12.414266</td>\n",
       "      <td>13.343521</td>\n",
       "      <td>13.545699</td>\n",
       "      <td>12.234360</td>\n",
       "      <td>14.921490</td>\n",
       "      <td>15.565114</td>\n",
       "      <td>...</td>\n",
       "      <td>10.524493</td>\n",
       "      <td>9.304888</td>\n",
       "      <td>12.037641</td>\n",
       "      <td>6.210856</td>\n",
       "      <td>9.870218</td>\n",
       "      <td>12.034437</td>\n",
       "      <td>14.208508</td>\n",
       "      <td>12.034598</td>\n",
       "      <td>15.729341</td>\n",
       "      <td>11.306020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>10.178591</td>\n",
       "      <td>10.001586</td>\n",
       "      <td>9.319354</td>\n",
       "      <td>10.960209</td>\n",
       "      <td>12.337291</td>\n",
       "      <td>12.924105</td>\n",
       "      <td>13.305955</td>\n",
       "      <td>11.776858</td>\n",
       "      <td>14.529580</td>\n",
       "      <td>14.606091</td>\n",
       "      <td>...</td>\n",
       "      <td>10.343619</td>\n",
       "      <td>8.650760</td>\n",
       "      <td>11.940205</td>\n",
       "      <td>5.775387</td>\n",
       "      <td>9.787532</td>\n",
       "      <td>12.285890</td>\n",
       "      <td>15.144465</td>\n",
       "      <td>11.824410</td>\n",
       "      <td>15.621938</td>\n",
       "      <td>10.984773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>9.922932</td>\n",
       "      <td>9.987443</td>\n",
       "      <td>8.029249</td>\n",
       "      <td>10.777103</td>\n",
       "      <td>12.311446</td>\n",
       "      <td>12.821562</td>\n",
       "      <td>13.150172</td>\n",
       "      <td>12.147272</td>\n",
       "      <td>14.698695</td>\n",
       "      <td>14.658868</td>\n",
       "      <td>...</td>\n",
       "      <td>10.640779</td>\n",
       "      <td>8.961600</td>\n",
       "      <td>12.069779</td>\n",
       "      <td>6.535149</td>\n",
       "      <td>9.749425</td>\n",
       "      <td>12.231994</td>\n",
       "      <td>14.216073</td>\n",
       "      <td>11.867467</td>\n",
       "      <td>15.653524</td>\n",
       "      <td>11.106400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>10.948244</td>\n",
       "      <td>10.120422</td>\n",
       "      <td>7.635790</td>\n",
       "      <td>11.122960</td>\n",
       "      <td>12.170785</td>\n",
       "      <td>13.027633</td>\n",
       "      <td>13.303402</td>\n",
       "      <td>12.384532</td>\n",
       "      <td>14.553590</td>\n",
       "      <td>14.725716</td>\n",
       "      <td>...</td>\n",
       "      <td>10.601510</td>\n",
       "      <td>8.683241</td>\n",
       "      <td>12.093678</td>\n",
       "      <td>6.621639</td>\n",
       "      <td>10.007895</td>\n",
       "      <td>12.185041</td>\n",
       "      <td>13.787838</td>\n",
       "      <td>12.072946</td>\n",
       "      <td>15.801390</td>\n",
       "      <td>11.137249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>12.648284</td>\n",
       "      <td>10.082579</td>\n",
       "      <td>5.591463</td>\n",
       "      <td>11.069865</td>\n",
       "      <td>11.969507</td>\n",
       "      <td>13.235915</td>\n",
       "      <td>13.545979</td>\n",
       "      <td>12.323847</td>\n",
       "      <td>15.173684</td>\n",
       "      <td>15.149847</td>\n",
       "      <td>...</td>\n",
       "      <td>10.585704</td>\n",
       "      <td>8.567937</td>\n",
       "      <td>11.953050</td>\n",
       "      <td>5.939196</td>\n",
       "      <td>9.956270</td>\n",
       "      <td>12.098249</td>\n",
       "      <td>14.083548</td>\n",
       "      <td>11.969848</td>\n",
       "      <td>15.801352</td>\n",
       "      <td>11.021689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>12.825349</td>\n",
       "      <td>10.481117</td>\n",
       "      <td>4.569970</td>\n",
       "      <td>10.515028</td>\n",
       "      <td>12.139172</td>\n",
       "      <td>12.976950</td>\n",
       "      <td>13.100658</td>\n",
       "      <td>12.126591</td>\n",
       "      <td>15.193909</td>\n",
       "      <td>14.725536</td>\n",
       "      <td>...</td>\n",
       "      <td>10.732089</td>\n",
       "      <td>9.866045</td>\n",
       "      <td>12.078110</td>\n",
       "      <td>6.267692</td>\n",
       "      <td>10.439969</td>\n",
       "      <td>12.065287</td>\n",
       "      <td>14.074192</td>\n",
       "      <td>11.915319</td>\n",
       "      <td>15.856985</td>\n",
       "      <td>11.143671</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows × 264 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     (+)-.alpha.-Tocopherol  (R)-2-Hydroxycaprylic acid  1,4-Dihydroxybenzene  \\\n",
       "458               12.425802                    9.940151              9.346458   \n",
       "459               13.488137                    9.854098              9.444458   \n",
       "460               13.246029                   10.221606              9.384963   \n",
       "461               12.467784                   10.382514              4.543732   \n",
       "462               12.489635                   10.195934              8.954138   \n",
       "463               13.988499                   10.082340             10.240554   \n",
       "464               13.537356                   10.167389              9.441791   \n",
       "465               10.178591                   10.001586              9.319354   \n",
       "466                9.922932                    9.987443              8.029249   \n",
       "467               10.948244                   10.120422              7.635790   \n",
       "468               12.648284                   10.082579              5.591463   \n",
       "469               12.825349                   10.481117              4.569970   \n",
       "\n",
       "     1-Methylxanthine  1-Naphthylamine  \\\n",
       "458         10.948009        12.238548   \n",
       "459         10.368734        12.140843   \n",
       "460         10.536216        12.199070   \n",
       "461         11.208936        12.414522   \n",
       "462         11.247966        12.753416   \n",
       "463         11.428982        12.117136   \n",
       "464         10.891179        12.414266   \n",
       "465         10.960209        12.337291   \n",
       "466         10.777103        12.311446   \n",
       "467         11.122960        12.170785   \n",
       "468         11.069865        11.969507   \n",
       "469         10.515028        12.139172   \n",
       "\n",
       "     1-Oleoyl-sn-glycero-3-phosphoethanolamin...  \\\n",
       "458                                    13.257731   \n",
       "459                                    13.950330   \n",
       "460                                    13.619825   \n",
       "461                                    13.640041   \n",
       "462                                    13.273298   \n",
       "463                                    13.747037   \n",
       "464                                    13.343521   \n",
       "465                                    12.924105   \n",
       "466                                    12.821562   \n",
       "467                                    13.027633   \n",
       "468                                    13.235915   \n",
       "469                                    12.976950   \n",
       "\n",
       "     1-Palmitoyl-2-hydroxy-sn-glycero-3-phosp...  \\\n",
       "458                                    12.973654   \n",
       "459                                    14.065568   \n",
       "460                                    13.807421   \n",
       "461                                    13.043924   \n",
       "462                                    13.546116   \n",
       "463                                    12.546961   \n",
       "464                                    13.545699   \n",
       "465                                    13.305955   \n",
       "466                                    13.150172   \n",
       "467                                    13.303402   \n",
       "468                                    13.545979   \n",
       "469                                    13.100658   \n",
       "\n",
       "     1-Stearoyl-sn-glycero-3-phosphocholine  1-Stearoyl-sn-glycerol  \\\n",
       "458                               11.607291               14.632432   \n",
       "459                               11.729910               15.957605   \n",
       "460                               12.153983               15.820099   \n",
       "461                               11.787866               15.949356   \n",
       "462                               11.980306               14.804769   \n",
       "463                               11.940502               15.943817   \n",
       "464                               12.234360               14.921490   \n",
       "465                               11.776858               14.529580   \n",
       "466                               12.147272               14.698695   \n",
       "467                               12.384532               14.553590   \n",
       "468                               12.323847               15.173684   \n",
       "469                               12.126591               15.193909   \n",
       "\n",
       "     1-pentadecanoyl-2-hydroxy-sn-glycero-3-p...  ...  \\\n",
       "458                                    14.793555  ...   \n",
       "459                                    15.858723  ...   \n",
       "460                                    15.464307  ...   \n",
       "461                                    15.327827  ...   \n",
       "462                                    14.768760  ...   \n",
       "463                                    15.667817  ...   \n",
       "464                                    15.565114  ...   \n",
       "465                                    14.606091  ...   \n",
       "466                                    14.658868  ...   \n",
       "467                                    14.725716  ...   \n",
       "468                                    15.149847  ...   \n",
       "469                                    14.725536  ...   \n",
       "\n",
       "     cis-5,8,11,14,17-Eicosapentaenoic acid  cis-5-Dodecenoic acid  \\\n",
       "458                               10.890095              10.409081   \n",
       "459                               10.721470               9.898654   \n",
       "460                               10.536693              10.425716   \n",
       "461                               11.003658              10.111463   \n",
       "462                                9.971164               9.401481   \n",
       "463                               10.625949               8.844947   \n",
       "464                               10.524493               9.304888   \n",
       "465                               10.343619               8.650760   \n",
       "466                               10.640779               8.961600   \n",
       "467                               10.601510               8.683241   \n",
       "468                               10.585704               8.567937   \n",
       "469                               10.732089               9.866045   \n",
       "\n",
       "     cis-9-Palmitoleic acid  d-LIMONENE  gamma-CEHC  gamma-Glutamylleucine  \\\n",
       "458               12.320187    7.020098   10.516925              12.065996   \n",
       "459               12.230804    5.331199   10.157811              11.956406   \n",
       "460               12.393004    6.748009   10.106812              12.287791   \n",
       "461               12.228382    6.140041    9.717163              12.087499   \n",
       "462               12.024649    6.353860    9.969766              12.896635   \n",
       "463               12.172595    5.710784    9.687481              12.042356   \n",
       "464               12.037641    6.210856    9.870218              12.034437   \n",
       "465               11.940205    5.775387    9.787532              12.285890   \n",
       "466               12.069779    6.535149    9.749425              12.231994   \n",
       "467               12.093678    6.621639   10.007895              12.185041   \n",
       "468               11.953050    5.939196    9.956270              12.098249   \n",
       "469               12.078110    6.267692   10.439969              12.065287   \n",
       "\n",
       "     ketoisocaproic acid   p-Cresol  phenylacetylglutamine  \\\n",
       "458            15.078192  11.697351              15.374608   \n",
       "459            14.367358  11.957828              15.641462   \n",
       "460            14.306134  11.710425              15.507585   \n",
       "461            13.597653  11.825458              15.374558   \n",
       "462            15.037879  11.981087              15.762672   \n",
       "463            14.091961  11.851552              15.602911   \n",
       "464            14.208508  12.034598              15.729341   \n",
       "465            15.144465  11.824410              15.621938   \n",
       "466            14.216073  11.867467              15.653524   \n",
       "467            13.787838  12.072946              15.801390   \n",
       "468            14.083548  11.969848              15.801352   \n",
       "469            14.074192  11.915319              15.856985   \n",
       "\n",
       "     trans-Vaccenic acid  \n",
       "458            11.736492  \n",
       "459            11.514334  \n",
       "460            11.687867  \n",
       "461            11.739387  \n",
       "462            10.928602  \n",
       "463            11.437537  \n",
       "464            11.306020  \n",
       "465            10.984773  \n",
       "466            11.106400  \n",
       "467            11.137249  \n",
       "468            11.021689  \n",
       "469            11.143671  \n",
       "\n",
       "[12 rows x 264 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df = pregnancy_df[pregnancy_df['Subject ID'] == 'DP21'][feature_list]\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2fdb343d-4fbc-4b23-9760-a82cee2a3461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject ID</th>\n",
       "      <th>Cohort</th>\n",
       "      <th>Labor onset</th>\n",
       "      <th>Gestational age (GA)/days</th>\n",
       "      <th>Gestational age (GA)/weeks</th>\n",
       "      <th>Birth GA/weeks</th>\n",
       "      <th>(+)-.alpha.-Tocopherol</th>\n",
       "      <th>(R)-2-Hydroxycaprylic acid</th>\n",
       "      <th>1,4-Dihydroxybenzene</th>\n",
       "      <th>1-Methylxanthine</th>\n",
       "      <th>...</th>\n",
       "      <th>cis-5,8,11,14,17-Eicosapentaenoic acid</th>\n",
       "      <th>cis-5-Dodecenoic acid</th>\n",
       "      <th>cis-9-Palmitoleic acid</th>\n",
       "      <th>d-LIMONENE</th>\n",
       "      <th>gamma-CEHC</th>\n",
       "      <th>gamma-Glutamylleucine</th>\n",
       "      <th>ketoisocaproic acid</th>\n",
       "      <th>p-Cresol</th>\n",
       "      <th>phenylacetylglutamine</th>\n",
       "      <th>trans-Vaccenic acid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>DP21</td>\n",
       "      <td>Discovery</td>\n",
       "      <td>natural</td>\n",
       "      <td>126</td>\n",
       "      <td>18</td>\n",
       "      <td>38.571429</td>\n",
       "      <td>12.425802</td>\n",
       "      <td>9.940151</td>\n",
       "      <td>9.346458</td>\n",
       "      <td>10.948009</td>\n",
       "      <td>...</td>\n",
       "      <td>10.890095</td>\n",
       "      <td>10.409081</td>\n",
       "      <td>12.320187</td>\n",
       "      <td>7.020098</td>\n",
       "      <td>10.516925</td>\n",
       "      <td>12.065996</td>\n",
       "      <td>15.078192</td>\n",
       "      <td>11.697351</td>\n",
       "      <td>15.374608</td>\n",
       "      <td>11.736492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>DP21</td>\n",
       "      <td>Discovery</td>\n",
       "      <td>natural</td>\n",
       "      <td>133</td>\n",
       "      <td>19</td>\n",
       "      <td>38.571429</td>\n",
       "      <td>13.488137</td>\n",
       "      <td>9.854098</td>\n",
       "      <td>9.444458</td>\n",
       "      <td>10.368734</td>\n",
       "      <td>...</td>\n",
       "      <td>10.721470</td>\n",
       "      <td>9.898654</td>\n",
       "      <td>12.230804</td>\n",
       "      <td>5.331199</td>\n",
       "      <td>10.157811</td>\n",
       "      <td>11.956406</td>\n",
       "      <td>14.367358</td>\n",
       "      <td>11.957828</td>\n",
       "      <td>15.641462</td>\n",
       "      <td>11.514334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>DP21</td>\n",
       "      <td>Discovery</td>\n",
       "      <td>natural</td>\n",
       "      <td>140</td>\n",
       "      <td>20</td>\n",
       "      <td>38.571429</td>\n",
       "      <td>13.246029</td>\n",
       "      <td>10.221606</td>\n",
       "      <td>9.384963</td>\n",
       "      <td>10.536216</td>\n",
       "      <td>...</td>\n",
       "      <td>10.536693</td>\n",
       "      <td>10.425716</td>\n",
       "      <td>12.393004</td>\n",
       "      <td>6.748009</td>\n",
       "      <td>10.106812</td>\n",
       "      <td>12.287791</td>\n",
       "      <td>14.306134</td>\n",
       "      <td>11.710425</td>\n",
       "      <td>15.507585</td>\n",
       "      <td>11.687867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>DP21</td>\n",
       "      <td>Discovery</td>\n",
       "      <td>natural</td>\n",
       "      <td>147</td>\n",
       "      <td>21</td>\n",
       "      <td>38.571429</td>\n",
       "      <td>12.467784</td>\n",
       "      <td>10.382514</td>\n",
       "      <td>4.543732</td>\n",
       "      <td>11.208936</td>\n",
       "      <td>...</td>\n",
       "      <td>11.003658</td>\n",
       "      <td>10.111463</td>\n",
       "      <td>12.228382</td>\n",
       "      <td>6.140041</td>\n",
       "      <td>9.717163</td>\n",
       "      <td>12.087499</td>\n",
       "      <td>13.597653</td>\n",
       "      <td>11.825458</td>\n",
       "      <td>15.374558</td>\n",
       "      <td>11.739387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>DP21</td>\n",
       "      <td>Discovery</td>\n",
       "      <td>natural</td>\n",
       "      <td>154</td>\n",
       "      <td>22</td>\n",
       "      <td>38.571429</td>\n",
       "      <td>12.489635</td>\n",
       "      <td>10.195934</td>\n",
       "      <td>8.954138</td>\n",
       "      <td>11.247966</td>\n",
       "      <td>...</td>\n",
       "      <td>9.971164</td>\n",
       "      <td>9.401481</td>\n",
       "      <td>12.024649</td>\n",
       "      <td>6.353860</td>\n",
       "      <td>9.969766</td>\n",
       "      <td>12.896635</td>\n",
       "      <td>15.037879</td>\n",
       "      <td>11.981087</td>\n",
       "      <td>15.762672</td>\n",
       "      <td>10.928602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>DP21</td>\n",
       "      <td>Discovery</td>\n",
       "      <td>natural</td>\n",
       "      <td>161</td>\n",
       "      <td>23</td>\n",
       "      <td>38.571429</td>\n",
       "      <td>13.988499</td>\n",
       "      <td>10.082340</td>\n",
       "      <td>10.240554</td>\n",
       "      <td>11.428982</td>\n",
       "      <td>...</td>\n",
       "      <td>10.625949</td>\n",
       "      <td>8.844947</td>\n",
       "      <td>12.172595</td>\n",
       "      <td>5.710784</td>\n",
       "      <td>9.687481</td>\n",
       "      <td>12.042356</td>\n",
       "      <td>14.091961</td>\n",
       "      <td>11.851552</td>\n",
       "      <td>15.602911</td>\n",
       "      <td>11.437537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>DP21</td>\n",
       "      <td>Discovery</td>\n",
       "      <td>natural</td>\n",
       "      <td>168</td>\n",
       "      <td>24</td>\n",
       "      <td>38.571429</td>\n",
       "      <td>13.537356</td>\n",
       "      <td>10.167389</td>\n",
       "      <td>9.441791</td>\n",
       "      <td>10.891179</td>\n",
       "      <td>...</td>\n",
       "      <td>10.524493</td>\n",
       "      <td>9.304888</td>\n",
       "      <td>12.037641</td>\n",
       "      <td>6.210856</td>\n",
       "      <td>9.870218</td>\n",
       "      <td>12.034437</td>\n",
       "      <td>14.208508</td>\n",
       "      <td>12.034598</td>\n",
       "      <td>15.729341</td>\n",
       "      <td>11.306020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>DP21</td>\n",
       "      <td>Discovery</td>\n",
       "      <td>natural</td>\n",
       "      <td>210</td>\n",
       "      <td>30</td>\n",
       "      <td>38.571429</td>\n",
       "      <td>10.178591</td>\n",
       "      <td>10.001586</td>\n",
       "      <td>9.319354</td>\n",
       "      <td>10.960209</td>\n",
       "      <td>...</td>\n",
       "      <td>10.343619</td>\n",
       "      <td>8.650760</td>\n",
       "      <td>11.940205</td>\n",
       "      <td>5.775387</td>\n",
       "      <td>9.787532</td>\n",
       "      <td>12.285890</td>\n",
       "      <td>15.144465</td>\n",
       "      <td>11.824410</td>\n",
       "      <td>15.621938</td>\n",
       "      <td>10.984773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>DP21</td>\n",
       "      <td>Discovery</td>\n",
       "      <td>natural</td>\n",
       "      <td>217</td>\n",
       "      <td>31</td>\n",
       "      <td>38.571429</td>\n",
       "      <td>9.922932</td>\n",
       "      <td>9.987443</td>\n",
       "      <td>8.029249</td>\n",
       "      <td>10.777103</td>\n",
       "      <td>...</td>\n",
       "      <td>10.640779</td>\n",
       "      <td>8.961600</td>\n",
       "      <td>12.069779</td>\n",
       "      <td>6.535149</td>\n",
       "      <td>9.749425</td>\n",
       "      <td>12.231994</td>\n",
       "      <td>14.216073</td>\n",
       "      <td>11.867467</td>\n",
       "      <td>15.653524</td>\n",
       "      <td>11.106400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>DP21</td>\n",
       "      <td>Discovery</td>\n",
       "      <td>natural</td>\n",
       "      <td>224</td>\n",
       "      <td>32</td>\n",
       "      <td>38.571429</td>\n",
       "      <td>10.948244</td>\n",
       "      <td>10.120422</td>\n",
       "      <td>7.635790</td>\n",
       "      <td>11.122960</td>\n",
       "      <td>...</td>\n",
       "      <td>10.601510</td>\n",
       "      <td>8.683241</td>\n",
       "      <td>12.093678</td>\n",
       "      <td>6.621639</td>\n",
       "      <td>10.007895</td>\n",
       "      <td>12.185041</td>\n",
       "      <td>13.787838</td>\n",
       "      <td>12.072946</td>\n",
       "      <td>15.801390</td>\n",
       "      <td>11.137249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>DP21</td>\n",
       "      <td>Discovery</td>\n",
       "      <td>natural</td>\n",
       "      <td>231</td>\n",
       "      <td>33</td>\n",
       "      <td>38.571429</td>\n",
       "      <td>12.648284</td>\n",
       "      <td>10.082579</td>\n",
       "      <td>5.591463</td>\n",
       "      <td>11.069865</td>\n",
       "      <td>...</td>\n",
       "      <td>10.585704</td>\n",
       "      <td>8.567937</td>\n",
       "      <td>11.953050</td>\n",
       "      <td>5.939196</td>\n",
       "      <td>9.956270</td>\n",
       "      <td>12.098249</td>\n",
       "      <td>14.083548</td>\n",
       "      <td>11.969848</td>\n",
       "      <td>15.801352</td>\n",
       "      <td>11.021689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>DP21</td>\n",
       "      <td>Discovery</td>\n",
       "      <td>natural</td>\n",
       "      <td>259</td>\n",
       "      <td>37</td>\n",
       "      <td>38.571429</td>\n",
       "      <td>12.825349</td>\n",
       "      <td>10.481117</td>\n",
       "      <td>4.569970</td>\n",
       "      <td>10.515028</td>\n",
       "      <td>...</td>\n",
       "      <td>10.732089</td>\n",
       "      <td>9.866045</td>\n",
       "      <td>12.078110</td>\n",
       "      <td>6.267692</td>\n",
       "      <td>10.439969</td>\n",
       "      <td>12.065287</td>\n",
       "      <td>14.074192</td>\n",
       "      <td>11.915319</td>\n",
       "      <td>15.856985</td>\n",
       "      <td>11.143671</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows × 270 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Subject ID     Cohort Labor onset  Gestational age (GA)/days  \\\n",
       "458       DP21  Discovery     natural                        126   \n",
       "459       DP21  Discovery     natural                        133   \n",
       "460       DP21  Discovery     natural                        140   \n",
       "461       DP21  Discovery     natural                        147   \n",
       "462       DP21  Discovery     natural                        154   \n",
       "463       DP21  Discovery     natural                        161   \n",
       "464       DP21  Discovery     natural                        168   \n",
       "465       DP21  Discovery     natural                        210   \n",
       "466       DP21  Discovery     natural                        217   \n",
       "467       DP21  Discovery     natural                        224   \n",
       "468       DP21  Discovery     natural                        231   \n",
       "469       DP21  Discovery     natural                        259   \n",
       "\n",
       "     Gestational age (GA)/weeks  Birth GA/weeks  (+)-.alpha.-Tocopherol  \\\n",
       "458                          18       38.571429               12.425802   \n",
       "459                          19       38.571429               13.488137   \n",
       "460                          20       38.571429               13.246029   \n",
       "461                          21       38.571429               12.467784   \n",
       "462                          22       38.571429               12.489635   \n",
       "463                          23       38.571429               13.988499   \n",
       "464                          24       38.571429               13.537356   \n",
       "465                          30       38.571429               10.178591   \n",
       "466                          31       38.571429                9.922932   \n",
       "467                          32       38.571429               10.948244   \n",
       "468                          33       38.571429               12.648284   \n",
       "469                          37       38.571429               12.825349   \n",
       "\n",
       "     (R)-2-Hydroxycaprylic acid  1,4-Dihydroxybenzene  1-Methylxanthine  ...  \\\n",
       "458                    9.940151              9.346458         10.948009  ...   \n",
       "459                    9.854098              9.444458         10.368734  ...   \n",
       "460                   10.221606              9.384963         10.536216  ...   \n",
       "461                   10.382514              4.543732         11.208936  ...   \n",
       "462                   10.195934              8.954138         11.247966  ...   \n",
       "463                   10.082340             10.240554         11.428982  ...   \n",
       "464                   10.167389              9.441791         10.891179  ...   \n",
       "465                   10.001586              9.319354         10.960209  ...   \n",
       "466                    9.987443              8.029249         10.777103  ...   \n",
       "467                   10.120422              7.635790         11.122960  ...   \n",
       "468                   10.082579              5.591463         11.069865  ...   \n",
       "469                   10.481117              4.569970         10.515028  ...   \n",
       "\n",
       "     cis-5,8,11,14,17-Eicosapentaenoic acid  cis-5-Dodecenoic acid  \\\n",
       "458                               10.890095              10.409081   \n",
       "459                               10.721470               9.898654   \n",
       "460                               10.536693              10.425716   \n",
       "461                               11.003658              10.111463   \n",
       "462                                9.971164               9.401481   \n",
       "463                               10.625949               8.844947   \n",
       "464                               10.524493               9.304888   \n",
       "465                               10.343619               8.650760   \n",
       "466                               10.640779               8.961600   \n",
       "467                               10.601510               8.683241   \n",
       "468                               10.585704               8.567937   \n",
       "469                               10.732089               9.866045   \n",
       "\n",
       "     cis-9-Palmitoleic acid  d-LIMONENE  gamma-CEHC  gamma-Glutamylleucine  \\\n",
       "458               12.320187    7.020098   10.516925              12.065996   \n",
       "459               12.230804    5.331199   10.157811              11.956406   \n",
       "460               12.393004    6.748009   10.106812              12.287791   \n",
       "461               12.228382    6.140041    9.717163              12.087499   \n",
       "462               12.024649    6.353860    9.969766              12.896635   \n",
       "463               12.172595    5.710784    9.687481              12.042356   \n",
       "464               12.037641    6.210856    9.870218              12.034437   \n",
       "465               11.940205    5.775387    9.787532              12.285890   \n",
       "466               12.069779    6.535149    9.749425              12.231994   \n",
       "467               12.093678    6.621639   10.007895              12.185041   \n",
       "468               11.953050    5.939196    9.956270              12.098249   \n",
       "469               12.078110    6.267692   10.439969              12.065287   \n",
       "\n",
       "     ketoisocaproic acid   p-Cresol  phenylacetylglutamine  \\\n",
       "458            15.078192  11.697351              15.374608   \n",
       "459            14.367358  11.957828              15.641462   \n",
       "460            14.306134  11.710425              15.507585   \n",
       "461            13.597653  11.825458              15.374558   \n",
       "462            15.037879  11.981087              15.762672   \n",
       "463            14.091961  11.851552              15.602911   \n",
       "464            14.208508  12.034598              15.729341   \n",
       "465            15.144465  11.824410              15.621938   \n",
       "466            14.216073  11.867467              15.653524   \n",
       "467            13.787838  12.072946              15.801390   \n",
       "468            14.083548  11.969848              15.801352   \n",
       "469            14.074192  11.915319              15.856985   \n",
       "\n",
       "     trans-Vaccenic acid  \n",
       "458            11.736492  \n",
       "459            11.514334  \n",
       "460            11.687867  \n",
       "461            11.739387  \n",
       "462            10.928602  \n",
       "463            11.437537  \n",
       "464            11.306020  \n",
       "465            10.984773  \n",
       "466            11.106400  \n",
       "467            11.137249  \n",
       "468            11.021689  \n",
       "469            11.143671  \n",
       "\n",
       "[12 rows x 270 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs['DP21']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a4fa7188-10ca-4999-ac65-9eb9d5f38db4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------fwd-1-------------------------------------------------\n",
      "..........................................\n",
      "DP13\n",
      "..........................................\n",
      "tensor([[13.5883, 10.1984,  7.6945, 10.3222, 12.0061, 13.7868, 13.9872, 12.2380,\n",
      "         15.7250, 15.6905, 12.8760, 10.7517, 11.7163,  9.7841, 12.3443, 11.1824,\n",
      "         10.5885, 14.3535, 10.8572,  9.3750, 12.2466, 12.5257,  7.4795, 10.4667,\n",
      "         12.9570, 13.4048,  8.9042,  9.3882, 10.3934, 13.7103,  8.5097,  7.9762,\n",
      "         11.4436,  9.3750,  8.8835,  9.9850,  7.7326, 12.2547, 11.1747, 13.6552,\n",
      "         10.3639, 10.5769,  9.9575,  6.5693,  9.5442, 11.5130, 10.2621,  5.8943,\n",
      "          8.5639, 11.9965, 10.2038, 14.0248, 10.8936,  8.5130, 15.2027, 13.8696,\n",
      "         11.0883, 12.2318,  9.6833, 10.3985, 14.1471,  9.9107, 13.9342,  9.8797,\n",
      "          9.1133, 13.0408, 12.5563, 11.0398, 14.3222, 11.1917, 13.2865,  9.4204,\n",
      "         13.4251, 14.1280, 13.0231, 14.6752, 10.9753, 12.2913, 11.3048, 14.6799,\n",
      "          9.3750,  9.3768,  9.6784, 11.8098,  8.6883, 13.4244,  8.9926, 10.7034,\n",
      "         12.1407, 14.1746, 10.5108, 13.7681, 12.7498, 14.9523,  8.9953, 13.2339,\n",
      "         11.6314, 12.9211,  9.7019, 11.9055, 11.5186, 15.0169, 13.0063, 11.5698,\n",
      "         12.2546, 10.4586, 15.5900, 20.4766, 10.0079,  9.1459, 13.2651,  9.8077,\n",
      "         10.6252,  9.8856, 11.4555, 16.3692, 10.7116,  9.8494, 11.0230,  7.4777,\n",
      "         12.2278, 11.5535, 14.3977, 11.0612, 13.9681, 14.3077, 13.7419, 10.6207,\n",
      "         10.8531, 11.4162, 11.4407, 12.6395,  9.8209,  7.3659, 12.6720, 10.5258,\n",
      "         12.2593, 11.8224, 11.4867,  9.3750, 10.1259, 12.5783,  9.7241, 11.2320,\n",
      "         12.2234, 13.2584,  8.8818, 14.5446, 12.4967, 11.3841, 13.9437,  8.4141,\n",
      "         12.9504, 18.8247, 14.7043, 15.8190, 13.9772, 14.6631, 11.1658, 10.7001,\n",
      "         11.9372, 13.3983, 13.2313, 10.4321, 11.4169, 11.6505,  9.6586, 13.0184,\n",
      "         11.9727, 12.3146, 11.7563, 12.6684,  9.8626, 10.6731, 11.8571, 11.5171,\n",
      "         10.2884, 16.3183, 19.6519, 13.3996, 13.6227,  7.1326, 11.0333, 16.4390,\n",
      "          8.2684,  9.0286,  7.7345,  8.3774,  8.5363, 11.2140, 10.2991,  8.9907,\n",
      "         12.1078,  7.5754, 12.0704,  7.9065, 14.3993,  9.6142,  6.7484, 11.0575,\n",
      "         13.2514, 11.4191,  9.9615, 14.3095, 17.9687, 11.6070, 14.1356,  9.3750,\n",
      "         11.0050, 11.3792,  8.7973, 11.3251, 13.0545, 10.1666,  3.9004, 12.8179,\n",
      "          6.5200, 13.7207, 10.6266, 12.7926, 13.7474, 12.8936, 14.7929, 14.3808,\n",
      "         17.5149,  3.6483,  9.9662, 10.3268, 11.2798, 11.8191, 11.8115, 11.8526,\n",
      "         14.8828, 11.8117, 13.8683, 10.7149, 11.2917, 10.7023, 11.3489, 14.8143,\n",
      "         11.5221, 13.5893, 14.4366, 10.7490, 14.6417, 10.0562, 14.8923, 11.4409,\n",
      "          8.2561, 14.8596,  8.7993, 12.8561, 12.8238,  8.7584, 11.1783, 10.2385,\n",
      "         11.7837,  6.1763, 10.4290, 11.7459, 14.6613, 10.7865, 14.8038, 10.8643],\n",
      "        [13.7646, 10.4481,  8.7686, 11.3521, 12.3840, 14.0120, 13.9139, 12.3280,\n",
      "         15.8603, 15.8720, 12.3402, 11.2541, 11.9175, 10.5549, 12.5731, 11.0757,\n",
      "         10.7109, 13.6524, 11.2408,  9.3568, 12.4251, 12.4817,  7.7624,  9.7655,\n",
      "         13.1080, 12.8581,  9.2274,  8.3056, 10.6610, 13.6471,  8.7236,  7.6033,\n",
      "         11.6360,  4.8166,  9.6426,  9.8542,  8.7285, 12.4724, 11.1988, 13.5512,\n",
      "         10.4373,  9.7454, 10.1230,  8.7450,  9.2883, 11.4439, 10.4615, 12.6875,\n",
      "          8.9519, 12.0827, 10.1593, 14.0548, 10.9473,  8.7897, 13.9124, 12.4816,\n",
      "          9.4303, 11.2994,  9.7973,  9.6084, 12.9641,  9.9805, 12.5719,  9.2464,\n",
      "          9.5108, 12.7407, 12.5845,  9.8811, 12.8022, 10.2662, 11.6546,  9.4762,\n",
      "         13.4641, 13.8746, 12.3124, 14.0221, 10.3320, 12.0572, 11.1570, 14.4781,\n",
      "          9.7555, 10.1124,  9.7880, 11.6302,  8.7689, 13.4225,  9.4295, 10.5184,\n",
      "         11.4364, 12.8382, 10.7860, 13.6319, 12.3415, 15.3104, 10.0830, 13.2811,\n",
      "         11.5502, 13.0128,  8.8373, 12.6818, 11.3920, 14.8967, 12.7207, 12.0056,\n",
      "         12.8681, 10.5152, 15.5332, 20.2694, 10.1085, 10.4908, 13.0247,  9.5847,\n",
      "         10.2935,  9.8815,  8.3665, 17.1185, 10.7938,  9.9732, 10.8163,  8.1428,\n",
      "         12.6508, 11.6296, 13.7084, 11.1081, 15.0538, 15.3177, 14.2077,  9.6574,\n",
      "         10.9285,  9.7471, 11.9774, 12.6564,  9.0926,  7.1962, 16.4942,  9.9333,\n",
      "         12.2655, 12.1451, 11.9605, 12.7138, 10.1378, 12.8531,  9.8414, 11.1434,\n",
      "         12.1302, 13.5894,  8.9791, 14.5093, 12.3224, 11.5011, 14.0417,  7.8886,\n",
      "         13.2801, 19.0227, 14.9346, 16.2067, 14.0229, 14.6825, 12.3429, 11.0491,\n",
      "         12.5257, 13.7908, 13.3121, 11.1573, 10.5085, 11.8263,  9.9850, 13.2995,\n",
      "         11.8739, 12.3345, 11.5629, 12.9764, 10.7413, 10.6523, 11.6083, 11.0911,\n",
      "          9.9829, 16.1020, 19.6679, 13.7177, 13.6573,  7.1675, 11.5835, 16.4595,\n",
      "          8.5316,  8.9090,  8.0453,  8.3542,  8.6868, 11.0586, 10.3094,  8.4321,\n",
      "         11.6006,  8.0132, 12.3498,  7.7352, 14.9637,  9.0118,  8.4854, 11.5705,\n",
      "         13.1093, 11.6332,  9.8460, 14.5537, 18.0838, 11.5415, 14.2415,  9.3568,\n",
      "         11.3943, 10.9880,  9.1522, 11.1468, 13.1143, 10.5550,  3.9503, 12.7645,\n",
      "          7.1601, 15.0858, 10.7690, 12.7426, 13.7875, 12.7624, 14.7825, 14.3825,\n",
      "         17.8084,  4.6430,  9.9932, 10.6574, 12.1316, 12.0502, 12.5309, 12.5905,\n",
      "         14.8295, 11.3605, 15.4859, 10.1931, 10.6673, 10.2340, 11.4386, 15.4698,\n",
      "         11.3071, 13.7513, 14.4063, 10.7435, 14.6386, 10.2386, 15.2385, 10.4101,\n",
      "         10.2647, 14.0755,  9.8591, 12.5490, 13.6206,  8.8966, 11.2509,  9.3092,\n",
      "         12.0836,  6.5349,  9.8591, 11.6942, 14.4521, 11.1543, 14.8475, 11.2309]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[13.8880, 10.3300,  9.3762, 10.7116, 12.1630, 13.7125, 14.0767, 12.5576,\n",
      "         15.2335, 15.4997, 12.0127, 10.6368, 11.3366, 11.0544, 12.0327, 11.5081,\n",
      "         12.9622, 12.8375, 11.1239,  9.3762, 12.2194, 12.6488,  7.6580,  8.5738,\n",
      "         12.6925, 11.6458,  9.1792,  8.7446, 12.1657, 14.0664,  8.2797,  7.1288,\n",
      "         11.2006,  9.3762,  7.2673, 10.5365,  8.4266, 13.9892, 11.7612, 13.5094,\n",
      "         11.2309,  9.0798, 11.2612,  5.4216,  9.7524, 11.5891, 10.9726, 12.5505,\n",
      "          8.2093, 12.8737, 10.5159, 14.3551, 10.9701,  9.4921, 13.3960, 12.9322,\n",
      "          9.6791, 10.9848,  9.7157,  9.0341, 12.5232,  9.2638, 12.0647,  8.0846,\n",
      "          9.1820, 12.1024, 11.7109,  9.0073, 12.0859,  9.5654, 11.7761,  7.3585,\n",
      "         14.9100, 13.4920, 11.6843, 13.3697,  9.9821, 10.8282, 11.7532, 14.2011,\n",
      "          9.6350, 10.2794,  8.0215, 10.2319,  8.6742, 12.2416,  9.4716,  9.7841,\n",
      "         11.1156, 12.4908,  9.8011, 13.2585, 12.2573, 16.6598,  9.8955, 13.6565,\n",
      "         11.4595, 13.8085,  8.4069, 12.5817, 11.7325, 15.2623, 13.4809, 11.8528,\n",
      "         12.7857, 10.6322, 14.7756, 20.3967, 10.9442,  9.9843, 12.7570,  8.7421,\n",
      "          9.9825,  9.7501,  9.5152, 16.7325, 10.6538, 11.0830, 10.4119, 10.9826,\n",
      "         14.2437, 11.6351, 13.5264, 10.5635, 15.3538, 14.9591, 13.0069,  9.4525,\n",
      "         10.4196,  9.2027, 11.4353, 12.6752,  7.9718,  6.8553, 16.3507,  9.6324,\n",
      "         12.3188, 11.7257, 12.3082, 11.0749, 11.4803, 12.5996,  9.5508, 10.0384,\n",
      "         12.1782, 13.8203,  9.0915, 14.6770, 12.3866, 11.4238, 13.6641,  7.4352,\n",
      "         12.8635, 18.6394, 14.6542, 14.8455, 13.9635, 14.4802,  8.9453,  9.1129,\n",
      "         12.0116, 13.2836, 12.9559, 10.5926, 11.5480, 11.0623,  8.8105, 12.9471,\n",
      "         11.5206, 11.9736, 11.2210, 12.3440,  9.1481, 10.4557, 11.5710, 10.5106,\n",
      "          9.5427, 16.2572, 19.6308, 13.2129, 13.7478,  7.7246, 11.2025, 16.1867,\n",
      "          7.9629,  8.8082,  8.0244,  8.0387,  7.0633, 10.0290,  9.7424,  8.2376,\n",
      "         10.3031,  8.1530, 13.9808,  8.3000, 15.4211,  9.2423,  8.6402, 11.6568,\n",
      "         13.7030, 11.5449, 10.3181, 14.1342, 17.6941, 11.7195, 14.2983,  9.3762,\n",
      "         10.2743,  9.7532,  8.0055, 11.0234, 13.2364, 10.4245,  3.5618, 13.0424,\n",
      "          8.5609, 13.8379, 12.8418, 13.6417, 15.2771, 12.1203, 14.7961, 14.1245,\n",
      "         16.9048,  4.1516,  9.9982, 10.3757,  9.5261, 12.3926, 12.8001, 13.4545,\n",
      "         14.1413, 11.3379, 15.8952, 10.1653, 10.7910, 10.3580, 11.2751, 15.2455,\n",
      "         12.2440, 13.7874, 14.5041, 10.6335, 14.7255, 10.1792, 15.0908, 10.1906,\n",
      "          8.7411, 14.0742,  9.7860, 12.3829, 11.1293,  9.9077, 10.5907,  8.3155,\n",
      "         11.7329,  6.6675, 10.0055, 12.0225, 14.2852, 11.0125, 15.0677, 10.9440],\n",
      "        [13.3306,  9.9865,  8.0676, 11.1428, 12.1069, 13.1677, 13.4612, 12.6361,\n",
      "         15.2649, 15.2696, 12.1300, 10.3432, 11.3246, 10.5426, 12.0711, 11.6250,\n",
      "         12.9063, 12.4144, 10.8152,  9.3258, 12.2063, 12.7316,  7.7200,  8.4342,\n",
      "         13.4734, 12.0844,  9.3131,  8.7864, 12.3005, 14.0016,  8.2066,  7.5455,\n",
      "         11.2758,  9.3258,  7.6865,  9.9569,  8.8940, 13.8120, 11.9231, 13.6950,\n",
      "         10.1657, 10.0346, 11.3041,  8.1801,  9.1765, 11.1089, 12.2162, 11.3455,\n",
      "          8.5792, 12.9439, 10.5215, 14.4170, 10.9026,  8.4779, 14.7392, 13.7557,\n",
      "         10.5920, 11.6776,  9.4294,  9.2378, 13.7078,  9.6335, 13.2261,  9.0677,\n",
      "          8.9532, 12.7345, 12.4807,  9.9798, 13.0942, 10.6624, 12.7248,  8.9002,\n",
      "         14.8787, 13.5678, 12.2650, 14.3455, 10.6954, 11.8397, 11.8333, 14.2189,\n",
      "          9.1940,  9.4870,  8.6436, 10.8381,  8.9812, 12.0616,  9.2819,  9.5181,\n",
      "         11.9967, 13.8026,  9.8783, 13.5147, 12.3848, 16.8245,  9.3845, 14.1617,\n",
      "         11.8130, 14.5425,  8.2720, 12.2721, 11.7423, 15.6229, 13.7182, 11.9047,\n",
      "         12.9446, 10.5867, 14.6873, 20.4190, 10.2380, 10.1157, 12.7169,  9.0347,\n",
      "          9.8060,  9.6796, 10.6312, 16.8129, 10.7096, 11.1317, 10.0639, 11.2424,\n",
      "         14.4705, 11.5693, 13.7652, 10.3231, 14.7079, 14.0688, 13.0252,  8.5321,\n",
      "         10.2632, 10.2925, 11.8712, 12.7516,  8.3666,  6.8614, 14.9104,  9.6849,\n",
      "         12.4637, 11.9274, 11.5459,  6.7748, 11.1398, 12.5141,  9.6237,  9.8225,\n",
      "         12.4404, 14.0200,  9.3226, 14.6330, 12.5024, 11.3722, 13.6992,  5.8459,\n",
      "         12.2694, 18.4686, 14.1833, 14.4261, 13.4624, 14.0709,  8.4805,  8.1303,\n",
      "         12.0272, 12.2693, 12.9691, 10.6085, 11.8456, 11.0097,  8.9829, 12.7680,\n",
      "         10.9909, 11.6409, 10.7570, 12.0661,  8.1863,  9.7016, 10.9329, 11.8837,\n",
      "          9.7276, 16.1648, 19.4854, 12.9766, 13.1474,  7.3702, 10.7150, 16.1280,\n",
      "          8.4155,  8.5575,  8.8560,  8.2758,  7.7285,  9.3515,  9.4116,  7.9234,\n",
      "         11.4521,  8.7117, 14.0983,  7.9861, 15.3775,  9.6480,  8.5520, 11.2979,\n",
      "         13.0102, 10.9875, 10.1926, 13.7183, 17.5001, 10.9748, 14.3081,  9.3258,\n",
      "         10.6834, 10.0450,  8.5099, 11.1526, 13.4766, 10.1723,  3.6937, 12.8137,\n",
      "          7.3456, 12.3921, 12.8969, 13.5230, 15.3060, 12.1698, 14.8829, 13.5932,\n",
      "         17.0605,  8.4916,  9.9422, 10.1829,  9.2914, 11.8672, 12.5953, 12.8875,\n",
      "         14.2266, 11.6012, 15.2412, 10.3175, 10.8022, 10.1720, 10.9346, 15.7102,\n",
      "         12.5870, 13.8150, 14.5211, 10.5285, 14.7123,  8.9662, 15.1744, 10.4313,\n",
      "          9.1789, 14.5000, 10.3409, 12.7312, 12.7080,  8.6156, 10.5990,  9.3833,\n",
      "         12.0829,  6.6823,  9.8912, 12.0675, 14.3989, 11.3231, 15.4037, 11.4579]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[12.8629, 10.5587,  7.9142, 10.9839, 12.4705, 13.3977, 13.7458, 12.0089,\n",
      "         15.2217, 15.1394, 12.0892, 11.2702, 12.0869, 10.7032, 12.2597, 11.8062,\n",
      "         12.0663, 13.8263, 11.1645,  9.2360, 12.3890, 12.8055,  7.1017,  8.9824,\n",
      "         13.2747, 12.1274,  9.4338,  9.6075, 10.5388, 13.6572,  8.7850,  7.0851,\n",
      "         11.8377,  7.4901,  8.5505,  9.9053,  7.6517, 13.1544, 11.6723, 13.7781,\n",
      "         10.5190,  9.8101, 11.1349,  8.0793,  9.6443, 11.9056, 11.1808, 10.4323,\n",
      "          8.7782, 12.5525, 10.4886, 14.8427, 11.0094,  8.3667, 13.8287, 12.9348,\n",
      "         10.0226, 11.1614, 10.0667,  9.8789, 12.6906,  9.8180, 12.6370,  9.0462,\n",
      "          9.4703, 12.3467, 12.4165,  9.6587, 12.8086, 10.5325, 11.9609,  9.1532,\n",
      "         14.1346, 13.6245, 12.3370, 14.1296, 10.6637, 11.7019, 11.3637, 14.2020,\n",
      "          9.6136,  9.7248,  8.5823, 10.7907,  8.7376, 13.1025,  9.3461, 10.1478,\n",
      "         11.3033, 12.9872, 10.3090, 13.3996, 12.1907, 16.3541,  9.5168, 13.7597,\n",
      "         11.9708, 13.6069,  9.3920, 12.1152, 11.9224, 15.6319, 13.5669, 12.2111,\n",
      "         12.6440, 10.8612, 15.7932, 20.6806, 10.3350,  9.5673, 12.9803,  8.4836,\n",
      "          9.5929,  9.9442,  9.7205, 16.6784, 10.5303, 10.1034, 10.3158, 10.2171,\n",
      "         13.6506, 11.7108, 13.6152, 10.5224, 14.2836, 14.5723, 13.7332,  9.6072,\n",
      "         10.3499,  9.8367, 11.3912, 12.9582,  8.9137,  7.2954, 14.6687, 10.5069,\n",
      "         12.2757, 12.0847, 12.9161,  9.4365,  9.9215, 13.1345,  9.6206, 10.6293,\n",
      "         11.7224, 13.5063,  9.1170, 14.5837, 12.4314, 11.8963, 14.4010,  8.5056,\n",
      "         12.3815, 18.5372, 14.3138, 15.4509, 13.4517, 14.4234,  9.4994,  9.2193,\n",
      "         11.7736, 12.1869, 12.6575, 10.6040, 10.9670, 11.4482,  9.8558, 12.7547,\n",
      "         11.1540, 11.7349, 11.0996, 12.0492,  8.9127, 10.1695, 11.4541, 10.8662,\n",
      "          9.9790, 16.4213, 19.5844, 12.7905, 13.3562,  7.7180, 11.4212, 16.2955,\n",
      "          7.6083,  8.3880,  7.4002,  7.6477,  7.1794, 10.2108,  9.2632,  8.2932,\n",
      "         11.3481,  9.0032, 12.1431,  8.5582, 14.9628,  9.4513,  9.0230, 11.4227,\n",
      "         13.1123, 10.9875,  9.5835, 13.7625, 17.7589, 11.4577, 14.4117,  9.4319,\n",
      "         10.9399, 11.4338,  8.8702, 11.1609, 13.1957, 10.5202,  3.9464, 13.0240,\n",
      "          7.3220, 14.4119, 11.8881, 13.2871, 14.6966, 12.7924, 14.9335, 13.9269,\n",
      "         16.9856,  7.2551, 10.0739, 10.6898, 10.7005, 11.6583, 12.3880, 12.2195,\n",
      "         14.5544, 11.8283, 14.0107, 10.5759, 10.8379, 10.0402, 11.0820, 15.1320,\n",
      "         12.5682, 14.1251, 14.6676, 10.8628, 14.9316,  8.8941, 15.2913, 10.0145,\n",
      "          9.5968, 14.4541, 10.8408, 12.9786, 12.5538,  8.4348, 11.0662,  9.3180,\n",
      "         12.0477,  6.7056,  9.8487, 12.2638, 14.7847, 10.8055, 15.0082, 11.1348],\n",
      "        [12.8960, 10.5837,  7.9338, 11.0117, 12.5033, 13.4317, 13.7815, 12.0411,\n",
      "         15.2597, 15.1809, 12.1189, 11.2982, 12.1152, 10.7291, 12.2925, 11.8355,\n",
      "         12.0978, 13.8588, 11.1932,  9.2579, 12.4217, 12.8405,  7.1179,  9.0041,\n",
      "         13.3106, 12.1534,  9.4578,  9.6304, 10.5662, 13.6923,  8.8053,  7.1033,\n",
      "         11.8671,  7.5093,  8.5708,  9.9310,  7.6697, 13.1898, 11.6999, 13.8118,\n",
      "         10.5441,  9.8342, 11.1620,  8.0997,  9.6659, 11.9344, 11.2096, 10.4588,\n",
      "          8.7994, 12.5817, 10.5143, 14.8796, 11.0373,  8.3868, 13.8662, 12.9657,\n",
      "         10.0481, 11.1928, 10.0920,  9.9026, 12.7242,  9.8420, 12.6695,  9.0658,\n",
      "          9.4927, 12.3797, 12.4458,  9.6836, 12.8438, 10.5594, 11.9920,  9.1753,\n",
      "         14.1698, 13.6581, 12.3682, 14.1636, 10.6891, 11.7323, 11.3943, 14.2389,\n",
      "          9.6363,  9.7506,  8.6043, 10.8198,  8.7580, 13.1344,  9.3670, 10.1718,\n",
      "         11.3309, 13.0190, 10.3333, 13.4333, 12.2241, 16.3956,  9.5406, 13.7932,\n",
      "         12.0020, 13.6402,  9.4117, 12.1450, 11.9530, 15.6751, 13.6016, 12.2440,\n",
      "         12.6749, 10.8871, 15.8317, 20.7344, 10.3603,  9.5921, 13.0113,  8.5038,\n",
      "          9.6173,  9.9690,  9.7438, 16.7209, 10.5575, 10.1270, 10.3421, 10.2406,\n",
      "         13.6871, 11.7394, 13.6487, 10.5488, 14.3237, 14.6107, 13.7664,  9.6321,\n",
      "         10.3758,  9.8615, 11.4202, 12.9906,  8.9374,  7.3133, 14.7037, 10.5325,\n",
      "         12.3076, 12.1183, 12.9468,  9.4628,  9.9453, 13.1668,  9.6443, 10.6551,\n",
      "         11.7490, 13.5402,  9.1372, 14.6207, 12.4623, 11.9249, 14.4389,  8.5261,\n",
      "         12.4133, 18.5844, 14.3494, 15.4930, 13.4884, 14.4606,  9.5218,  9.2411,\n",
      "         11.8035, 12.2184, 12.6903, 10.6313, 10.9967, 11.4763,  9.8788, 12.7880,\n",
      "         11.1823, 11.7657, 11.1261, 12.0774,  8.9340, 10.1975, 11.4809, 10.8951,\n",
      "         10.0033, 16.4628, 19.6330, 12.8220, 13.3911,  7.7366, 11.4498, 16.3333,\n",
      "          7.6260,  8.4078,  7.4185,  7.6658,  7.1970, 10.2391,  9.2856,  8.3135,\n",
      "         11.3769,  9.0236, 12.1741,  8.5808, 15.0042,  9.4747,  9.0440, 11.4526,\n",
      "         13.1441, 11.0139,  9.6067, 13.7978, 17.8031, 11.4838, 14.4485,  9.4567,\n",
      "         10.9668, 11.4611,  8.8919, 11.1908, 13.2316, 10.5475,  3.9552, 13.0550,\n",
      "          7.3402, 14.4486, 11.9190, 13.3206, 14.7349, 12.8250, 14.9750, 13.9608,\n",
      "         17.0304,  7.2727, 10.1001, 10.7162, 10.7256, 11.6878, 12.4214, 12.2473,\n",
      "         14.5898, 11.8604, 14.0458, 10.6055, 10.8621, 10.0644, 11.1104, 15.1706,\n",
      "         12.5982, 14.1601, 14.7057, 10.8916, 14.9667,  8.9155, 15.3265, 10.0402,\n",
      "          9.6234, 14.4894, 10.8669, 13.0150, 12.5873,  8.4557, 11.0954,  9.3403,\n",
      "         12.0811,  6.7218,  9.8720, 12.2955, 14.8218, 10.8330, 15.0456, 11.1648]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor(0.4130, grad_fn=<MseLossBackward0>)\n",
      "..........................................\n",
      "..........................................\n",
      "------------------------------bwd-1-------------------------------------------------\n",
      "tensor([[13.7646, 10.4481,  8.7686,  ..., 11.1543, 14.8475, 11.2309],\n",
      "        [11.5341, 10.4913,  9.4685,  ..., 11.4091, 15.1702, 10.9748],\n",
      "        [12.6887, 10.3459,  9.3912,  ..., 11.3675, 15.3213, 11.2646],\n",
      "        ...,\n",
      "        [12.9691, 10.1537,  9.3631,  ..., 11.2597, 15.2860, 10.8817],\n",
      "        [13.8880, 10.3300,  9.3762,  ..., 11.0125, 15.0677, 10.9440],\n",
      "        [13.3306,  9.9865,  8.0676,  ..., 11.3231, 15.4037, 11.4579]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[13.5883, 10.1984,  7.6945,  ..., 10.7865, 14.8038, 10.8643],\n",
      "        [13.7646, 10.4481,  8.7686,  ..., 11.1543, 14.8475, 11.2309],\n",
      "        [11.5341, 10.4913,  9.4685,  ..., 11.4091, 15.1702, 10.9748],\n",
      "        ...,\n",
      "        [11.7106, 10.1600,  6.2551,  ..., 10.9439, 15.2575, 11.1083],\n",
      "        [12.9691, 10.1537,  9.3631,  ..., 11.2597, 15.2860, 10.8817],\n",
      "        [13.8880, 10.3300,  9.3762,  ..., 11.0125, 15.0677, 10.9440]])\n",
      "..........................................\n",
      "------------------------------fwd-1-------------------------------------------------\n",
      "..........................................\n",
      "DP10\n",
      "..........................................\n",
      "tensor([[12.7144, 10.5138,  9.4524,  ..., 11.2818, 14.8629, 11.2268],\n",
      "        [12.7054, 10.6209, 10.2497,  ..., 11.0516, 14.4843, 11.1907],\n",
      "        [12.9823, 10.3785,  8.5916,  ..., 10.7968, 14.3697, 11.2922],\n",
      "        [13.2523, 10.0611,  8.0390,  ..., 10.7110, 14.5225, 11.4764],\n",
      "        [13.1332, 10.0733,  9.2670,  ..., 10.8695, 14.6970, 11.5660],\n",
      "        [13.4173, 10.2919,  9.3573,  ..., 10.9425, 14.6475, 11.4611]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[13.0099, 10.5988,  8.8006,  ..., 10.8149, 14.8249, 11.4062],\n",
      "        [12.5724, 10.9583,  7.9027,  ..., 10.5996, 14.5049, 11.2634],\n",
      "        [12.3400, 10.9834,  9.3581,  ..., 11.4608, 15.3181, 10.9974],\n",
      "        [12.3325, 10.5217,  9.2985,  ..., 10.7814, 14.8339, 11.1222],\n",
      "        [11.6807, 10.6829,  9.3080,  ..., 11.1532, 15.1538, 11.1009],\n",
      "        [11.1991, 10.6694,  8.9394,  ...,  9.8147, 14.6271, 11.2918]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[12.8399, 10.5414,  7.9007,  ..., 10.7864, 14.9825, 11.1141],\n",
      "        [12.8171, 10.5240,  7.8872,  ..., 10.7674, 14.9567, 11.0935],\n",
      "        [12.8245, 10.5297,  7.8916,  ..., 10.7736, 14.9651, 11.1002],\n",
      "        [12.8639, 10.5595,  7.9148,  ..., 10.8063, 15.0094, 11.1358],\n",
      "        [12.8704, 10.5644,  7.9187,  ..., 10.8118, 15.0168, 11.1417],\n",
      "        [12.8773, 10.5697,  7.9228,  ..., 10.8175, 15.0246, 11.1479]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor(0.4028, grad_fn=<MseLossBackward0>)\n",
      "..........................................\n",
      "..........................................\n",
      "------------------------------bwd-1-------------------------------------------------\n",
      "tensor([[12.7054, 10.6209, 10.2497,  ..., 11.0516, 14.4843, 11.1907],\n",
      "        [12.9823, 10.3785,  8.5916,  ..., 10.7968, 14.3697, 11.2922],\n",
      "        [13.2523, 10.0611,  8.0390,  ..., 10.7110, 14.5225, 11.4764],\n",
      "        ...,\n",
      "        [12.3325, 10.5217,  9.2985,  ..., 10.7814, 14.8339, 11.1222],\n",
      "        [11.6807, 10.6829,  9.3080,  ..., 11.1532, 15.1538, 11.1009],\n",
      "        [11.1991, 10.6694,  8.9394,  ...,  9.8147, 14.6271, 11.2918]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[12.7144, 10.5138,  9.4524,  ..., 11.2818, 14.8629, 11.2268],\n",
      "        [12.7054, 10.6209, 10.2497,  ..., 11.0516, 14.4843, 11.1907],\n",
      "        [12.9823, 10.3785,  8.5916,  ..., 10.7968, 14.3697, 11.2922],\n",
      "        ...,\n",
      "        [12.3400, 10.9834,  9.3581,  ..., 11.4608, 15.3181, 10.9974],\n",
      "        [12.3325, 10.5217,  9.2985,  ..., 10.7814, 14.8339, 11.1222],\n",
      "        [11.6807, 10.6829,  9.3080,  ..., 11.1532, 15.1538, 11.1009]])\n",
      "..........................................\n",
      "------------------------------fwd-1-------------------------------------------------\n",
      "..........................................\n",
      "DP02\n",
      "..........................................\n",
      "tensor([[ 9.1223, 10.5136,  9.3148,  ..., 11.2633, 15.2362, 10.9433],\n",
      "        [12.4299, 10.1898,  7.4922,  ..., 11.5232, 15.4331, 10.9856],\n",
      "        [12.7646, 10.4152,  7.2629,  ..., 10.8281, 14.9445, 10.8709],\n",
      "        [11.1552, 10.3027,  7.9161,  ..., 11.2000, 15.2689, 10.8725],\n",
      "        [12.0696,  9.9425,  9.0799,  ..., 11.0128, 15.0969, 10.9918],\n",
      "        [12.7368, 10.2920,  9.3418,  ..., 10.8989, 15.1721, 10.9477]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[12.9184, 10.4408,  7.8769,  ..., 11.5261, 15.5226, 10.8580],\n",
      "        [13.2917, 10.4997,  9.1934,  ..., 11.0544, 15.3563, 10.8083],\n",
      "        [13.2376, 10.7112,  9.5147,  ..., 11.2372, 15.5674, 10.8654],\n",
      "        [11.4342, 10.8248,  8.9292,  ..., 11.3758, 15.6805, 10.8193],\n",
      "        [12.0074, 10.2084,  8.5734,  ..., 11.1598, 15.3798, 10.7904],\n",
      "        [13.2745, 10.4554,  6.6500,  ..., 10.9576, 14.9503, 10.8507]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[12.7492, 10.4725,  7.8471,  ..., 10.7108, 14.8801, 11.0323],\n",
      "        [12.7624, 10.4824,  7.8548,  ..., 10.7218, 14.8950, 11.0442],\n",
      "        [12.8720, 10.5654,  7.9195,  ..., 10.8130, 15.0184, 11.1432],\n",
      "        [12.8017, 10.5122,  7.8780,  ..., 10.7545, 14.9392, 11.0797],\n",
      "        [12.8042, 10.5137,  7.8794,  ..., 10.7564, 14.9418, 11.0821],\n",
      "        [12.8143, 10.5216,  7.8854,  ..., 10.7649, 14.9533, 11.0911]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor(0.5875, grad_fn=<MseLossBackward0>)\n",
      "..........................................\n",
      "..........................................\n",
      "------------------------------bwd-1-------------------------------------------------\n",
      "tensor([[12.0431, 10.8580,  9.3257,  ..., 11.1067, 15.2406, 11.2376],\n",
      "        [ 9.1223, 10.5136,  9.3148,  ..., 11.2633, 15.2362, 10.9433],\n",
      "        [10.9462, 10.6201,  7.3629,  ..., 11.0968, 15.3309, 11.0018],\n",
      "        ...,\n",
      "        [11.4342, 10.8248,  8.9292,  ..., 11.3758, 15.6805, 10.8193],\n",
      "        [12.0074, 10.2084,  8.5734,  ..., 11.1598, 15.3798, 10.7904],\n",
      "        [13.2745, 10.4554,  6.6500,  ..., 10.9576, 14.9503, 10.8507]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[12.2424, 10.9745,  7.6398,  ..., 11.1459, 15.3905, 11.0769],\n",
      "        [12.0431, 10.8580,  9.3257,  ..., 11.1067, 15.2406, 11.2376],\n",
      "        [ 9.1223, 10.5136,  9.3148,  ..., 11.2633, 15.2362, 10.9433],\n",
      "        ...,\n",
      "        [13.2376, 10.7112,  9.5147,  ..., 11.2372, 15.5674, 10.8654],\n",
      "        [11.4342, 10.8248,  8.9292,  ..., 11.3758, 15.6805, 10.8193],\n",
      "        [12.0074, 10.2084,  8.5734,  ..., 11.1598, 15.3798, 10.7904]])\n",
      "..........................................\n",
      "------------------------------fwd-1-------------------------------------------------\n",
      "------------------------------bwd-1-------------------------------------------------\n",
      "tensor([[13.4881,  9.8541,  9.4445,  ..., 11.9578, 15.6415, 11.5143],\n",
      "        [13.2460, 10.2216,  9.3850,  ..., 11.7104, 15.5076, 11.6879],\n",
      "        [12.4678, 10.3825,  4.5437,  ..., 11.8255, 15.3746, 11.7394],\n",
      "        ...,\n",
      "        [ 9.9229,  9.9874,  8.0292,  ..., 11.8675, 15.6535, 11.1064],\n",
      "        [10.9482, 10.1204,  7.6358,  ..., 12.0729, 15.8014, 11.1372],\n",
      "        [12.6483, 10.0826,  5.5915,  ..., 11.9698, 15.8014, 11.0217]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[12.4258,  9.9402,  9.3465,  ..., 11.6974, 15.3746, 11.7365],\n",
      "        [13.4881,  9.8541,  9.4445,  ..., 11.9578, 15.6415, 11.5143],\n",
      "        [13.2460, 10.2216,  9.3850,  ..., 11.7104, 15.5076, 11.6879],\n",
      "        ...,\n",
      "        [10.1786, 10.0016,  9.3194,  ..., 11.8244, 15.6219, 10.9848],\n",
      "        [ 9.9229,  9.9874,  8.0292,  ..., 11.8675, 15.6535, 11.1064],\n",
      "        [10.9482, 10.1204,  7.6358,  ..., 12.0729, 15.8014, 11.1372]])\n",
      "..........................................\n",
      "------------------------------fwd-1-------------------------------------------------\n",
      "------------------------------bwd-1-------------------------------------------------\n",
      "tensor([[13.2095, 10.8337,  8.1436,  ..., 10.7010, 15.7904, 11.0233],\n",
      "        [12.8080, 10.4660,  9.2629,  ..., 11.1492, 15.5583, 11.4949],\n",
      "        [13.3106, 10.4846,  9.4444,  ..., 11.0096, 15.6004, 11.0907],\n",
      "        [13.3936, 10.7652,  9.7862,  ..., 10.3069, 15.5408, 11.7374],\n",
      "        [12.5625, 10.5598,  9.3351,  ..., 11.1543, 15.9761, 11.1787],\n",
      "        [13.0324, 10.3409,  7.4959,  ..., 10.7019, 15.6109, 11.5189]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[14.7105, 10.8487,  9.7304,  ..., 10.1375, 15.4545, 11.4894],\n",
      "        [13.2095, 10.8337,  8.1436,  ..., 10.7010, 15.7904, 11.0233],\n",
      "        [12.8080, 10.4660,  9.2629,  ..., 11.1492, 15.5583, 11.4949],\n",
      "        [13.3106, 10.4846,  9.4444,  ..., 11.0096, 15.6004, 11.0907],\n",
      "        [13.3936, 10.7652,  9.7862,  ..., 10.3069, 15.5408, 11.7374],\n",
      "        [12.5625, 10.5598,  9.3351,  ..., 11.1543, 15.9761, 11.1787]])\n",
      "..........................................\n",
      "------------------------------fwd-1-------------------------------------------------\n",
      "..........................................\n",
      "DP09\n",
      "..........................................\n",
      "tensor([[12.5154,  9.1281,  9.3397,  9.9715, 12.3613, 13.6073, 13.7874, 12.1574,\n",
      "         15.4802, 15.3568, 12.2385, 11.3835, 12.0131, 10.9507, 10.7393, 11.5772,\n",
      "         11.7108, 14.0559, 11.3312,  9.3397, 12.5101, 12.4613,  6.6359,  8.2407,\n",
      "         12.7837, 10.7088,  9.3367,  9.0403,  9.7008, 14.2719,  8.5920,  7.1158,\n",
      "         11.6085,  9.3397,  7.0212, 10.2758,  8.7588, 12.7718, 11.2797, 13.8311,\n",
      "         10.2144,  7.6933, 10.3410,  8.1383,  9.6529, 11.7529, 10.9882, 12.4202,\n",
      "          9.2899, 11.9435,  9.8162, 14.6160, 11.1658,  6.7482, 14.3796, 13.2278,\n",
      "         10.4130, 11.2031,  9.9755,  9.8581, 13.0593,  9.8540, 12.9873, 10.2059,\n",
      "          9.4916, 12.7860, 12.6071, 10.3684, 13.0245, 10.6027, 12.2619,  8.5461,\n",
      "         15.3326, 13.7881, 12.6708, 12.6714, 10.7213,  7.0255,  9.3567, 14.5652,\n",
      "         10.0329,  9.9582,  9.0997, 11.0953,  8.7590, 13.3961,  9.7117,  9.8847,\n",
      "         11.3976, 13.4581, 10.3752, 13.0948, 11.7809, 17.4206,  8.9938, 12.5343,\n",
      "         10.9763, 13.2996,  7.5955, 12.3121, 11.2260, 15.2921, 13.3042, 11.5158,\n",
      "         12.7882, 11.1307, 15.9173, 20.6277, 10.5804, 10.3917, 12.9915,  8.6777,\n",
      "          9.8773,  9.5063, 10.0174, 16.9656, 10.3018, 10.2811,  9.9884,  7.0023,\n",
      "         12.4723, 11.6575, 13.1484, 11.1886, 14.3327, 14.4068, 14.4072,  9.5897,\n",
      "         10.7448, 10.4079, 11.5602, 12.9368,  8.8622,  7.9548, 16.1510, 10.6121,\n",
      "         11.9710, 11.9552, 11.2248, 10.2415, 10.3587, 12.4562,  9.5632, 11.8744,\n",
      "         12.6833, 13.7669,  7.9669, 14.9051, 12.2606, 12.1254, 14.0895,  7.4009,\n",
      "         12.8607, 18.9058, 14.5589, 16.5461, 13.4633, 14.4203, 10.8739, 10.0294,\n",
      "         12.0837, 12.5053, 13.0166, 11.1103, 10.3817, 11.8907,  9.7204, 12.9159,\n",
      "         10.1222, 11.7640, 11.4368, 12.2400,  9.8076, 10.3885, 10.6947, 11.0653,\n",
      "          9.7682, 16.3304, 19.7386, 12.9216, 12.7722,  8.1958, 10.9768, 16.5737,\n",
      "          5.9834,  8.1260,  8.4346,  8.4031,  8.7619, 11.4051,  9.6700,  8.1988,\n",
      "         11.5432,  8.6725, 11.3309,  8.5598, 15.2231,  7.3697,  9.3757, 11.9757,\n",
      "         12.5184, 11.1260, 10.5211, 14.2043, 18.0129, 11.2809, 14.2335,  9.3397,\n",
      "         11.2721, 11.0712,  9.3311, 10.9359, 11.7709, 10.1309,  3.9082, 13.1347,\n",
      "          7.6533, 15.6255, 10.4995,  9.3802, 13.9866, 12.8329, 14.9838, 13.7888,\n",
      "         16.8304, 12.3465, 10.0417, 10.6959,  7.4666, 11.8913, 12.8941, 12.5385,\n",
      "         13.0656, 11.8919, 12.6953,  9.6366, 10.7757,  9.5397, 11.5324, 14.8348,\n",
      "         13.2711, 14.4012, 14.6962, 10.7064, 14.8891,  9.9109, 15.2194,  9.1959,\n",
      "          9.2563, 14.5214, 11.3172, 12.9086, 13.3820,  8.4426, 11.1837,  6.2620,\n",
      "         12.1997,  6.2947,  9.6536, 11.8261, 13.7113,  8.4757, 13.5592, 11.0681],\n",
      "        [12.8039,  9.3423,  9.3749,  9.6760, 12.8478, 13.7463, 13.9109, 12.1854,\n",
      "         15.7572, 15.1747, 12.1705, 11.2606, 11.6849, 10.6101, 10.9982, 11.3759,\n",
      "         11.6191, 13.7717, 11.1595,  9.3749, 12.6070, 12.6259,  6.9183,  8.9215,\n",
      "         12.4624, 11.0633,  9.2050,  8.7491,  9.7249, 14.2630,  8.4544,  6.5328,\n",
      "         12.1669,  9.3749,  7.4526,  9.7158,  8.5537, 12.6052, 11.1797, 13.8369,\n",
      "         11.0477,  7.9725, 10.8532,  8.8429,  9.2681, 11.7967, 11.5840, 12.5297,\n",
      "          9.2488, 11.8389,  9.4756, 14.4902, 11.0138,  7.2674, 13.9055, 13.1104,\n",
      "         10.3487, 10.8882, 10.0109,  9.6670, 12.7699,  9.4200, 12.6888,  8.9646,\n",
      "          9.4848, 12.5252, 12.1323, 10.4547, 12.9730, 10.4104, 12.1980,  7.9525,\n",
      "         15.1941, 13.8502, 12.6282, 12.6554, 10.1847,  8.0619, 10.0991, 14.5740,\n",
      "          9.8891,  9.4186,  8.9110, 10.6253,  8.7482, 13.2326,  9.0352, 10.3606,\n",
      "         11.2098, 13.0323, 10.1320, 13.3032, 11.5347, 17.3165,  9.1564, 12.6007,\n",
      "         11.0237, 13.6376,  7.7096, 12.4243, 10.6121, 15.0892, 13.1137, 11.5535,\n",
      "         12.8171, 11.0728, 15.7492, 20.3336, 10.4052, 10.4454, 12.8518,  8.8778,\n",
      "          9.6413,  9.4284,  9.9436, 17.1148, 10.2169, 10.3227, 10.2210,  5.7402,\n",
      "         12.2827, 11.5423, 13.1953, 11.3158, 14.5859, 14.7577, 14.3057,  9.9347,\n",
      "         10.6596, 10.5773, 11.2561, 12.8558,  9.0518,  7.7275, 16.0037, 10.6554,\n",
      "         11.7939, 11.7847, 11.6541,  8.8577, 10.1512, 12.6165, 10.0545, 11.9118,\n",
      "         12.7695, 13.5997,  8.0531, 14.9511, 12.2184, 12.0444, 14.8814,  7.3587,\n",
      "         13.0769, 19.0286, 14.6681, 16.4803, 13.5946, 14.4616, 11.4824, 10.7195,\n",
      "         11.9514, 12.2320, 13.1362, 10.4247, 10.2247, 11.8015,  9.5255, 13.0087,\n",
      "         10.9335, 11.9573, 11.4501, 12.2691,  9.4184, 10.4069, 10.9110, 10.7759,\n",
      "         10.7670, 16.2185, 19.8236, 13.0910, 13.1627,  8.4463, 10.8365, 16.5719,\n",
      "          6.0400,  8.6771,  9.0263,  8.6566,  8.9818, 11.8716, 10.3094,  8.4226,\n",
      "         10.6697,  8.2861, 11.3518,  8.9939, 14.9821,  7.8924,  9.1266, 11.4128,\n",
      "         13.1045, 11.1160, 10.5778, 14.3162, 18.0421, 11.8987, 14.2093,  9.3749,\n",
      "         10.9125, 11.0032,  9.3583, 11.0158, 11.7767,  9.9373,  3.5740, 13.1199,\n",
      "          7.5530, 15.9778, 10.4071, 10.3638, 13.9025, 12.5982, 14.9145, 14.0168,\n",
      "         16.6930, 11.4898,  9.9438, 10.6070,  7.9854, 11.9904, 13.1403, 12.3875,\n",
      "         13.4791, 11.4641, 13.3082,  9.7609, 10.5298,  9.6305, 11.1995, 14.6410,\n",
      "         13.0871, 14.2635, 14.6000, 10.6205, 14.7929, 10.4742, 15.2128,  9.4251,\n",
      "          9.9802, 14.3778, 11.4829, 12.7660, 13.5289,  8.4174, 11.2335,  6.4517,\n",
      "         11.9088,  6.5634,  9.4339, 12.2724, 14.3756,  8.4851, 13.5574, 10.9695],\n",
      "        [12.3535, 10.3164,  9.3939,  9.6519, 13.1077, 13.6582, 13.6453, 12.1396,\n",
      "         15.2820, 14.8127, 11.9083, 11.1580, 11.4526, 10.5531, 12.0036, 11.1720,\n",
      "         11.4697, 13.4510, 11.0403,  9.3939, 12.5270, 12.9316,  7.4862,  9.9770,\n",
      "         12.4368, 11.8050,  9.1850,  8.8134,  9.7575, 14.0396,  8.3548,  5.8095,\n",
      "         12.5486,  9.3939,  8.2549,  9.7138,  8.3598, 12.7059, 11.1788, 13.8122,\n",
      "         10.8857,  9.2475, 11.1677,  9.8422,  9.3700, 11.8956, 11.9231, 12.0634,\n",
      "          9.5463, 11.7610,  9.4795, 14.9267, 10.8704,  8.3910, 13.7664, 13.1935,\n",
      "         10.5577, 10.9684, 10.0870,  9.8164, 12.8246,  9.1993, 12.6530,  7.5433,\n",
      "          9.3767, 12.4360, 11.6827, 10.7954, 13.1941, 10.5880, 12.3559,  7.7226,\n",
      "         14.9499, 13.7243, 12.5998, 13.2070,  9.7049, 11.0065, 11.8464, 14.3796,\n",
      "          9.8189,  9.3098,  8.1103, 10.0435,  8.8552, 12.9399,  8.7020, 10.7823,\n",
      "         11.3437, 12.9368, 10.0790, 13.6231, 11.8291, 17.3237,  9.8198, 13.3236,\n",
      "         11.7167, 13.7898,  8.5411, 12.6260, 10.3055, 14.9631, 12.9765, 11.8003,\n",
      "         13.0135, 11.0208, 15.7386, 20.2046, 10.2605, 10.7053, 12.7497,  8.2585,\n",
      "          9.1322,  9.7132, 10.1794, 17.3889, 10.2522, 10.6030,  9.6434,  5.1470,\n",
      "         11.9690, 11.5280, 13.4965, 11.2823, 14.8292, 15.0283, 14.1455, 10.6433,\n",
      "         10.3196, 10.8328, 11.1054, 12.8553,  9.2664,  7.3931, 15.0977, 10.7654,\n",
      "         11.7135, 11.6602, 11.0918,  8.0157, 10.5318, 12.8698, 10.2715, 11.6921,\n",
      "         13.0624, 13.6051,  8.3952, 15.0016, 12.2881, 12.0651, 15.3249,  8.0284,\n",
      "         13.1611, 19.0619, 14.5206, 16.1653, 13.4668, 14.3852,  9.8509,  9.8848,\n",
      "         11.6987, 11.7559, 13.1520, 10.1618, 10.5294, 11.5177,  9.4930, 12.9987,\n",
      "         11.7552, 12.0997, 11.3116, 11.9045,  8.8375, 10.2510, 11.0910, 10.6566,\n",
      "         11.4389, 16.1851, 19.7320, 12.9545, 13.5047,  8.8556, 10.8935, 16.4163,\n",
      "          7.1624,  9.5473,  9.7714,  9.1349,  9.1396, 11.9021, 10.4559,  8.3774,\n",
      "          9.9475,  8.2848, 11.4123,  9.0616, 14.6611,  9.1114,  9.1239, 10.8301,\n",
      "         13.1048, 10.9713, 10.3635, 13.9734, 17.9480, 11.8265, 14.2291,  9.3939,\n",
      "         10.4640, 10.6385,  9.3975, 11.1468, 12.0308, 10.0030,  3.6861, 13.1414,\n",
      "          7.6687, 15.9780, 10.4602, 13.0453, 13.8468, 12.3472, 14.9000, 13.7914,\n",
      "         16.2624, 10.6130,  9.9310, 10.6034,  9.7372, 11.6376, 12.6816, 11.5712,\n",
      "         14.3712, 11.1917, 13.7385, 10.0525, 10.0342,  9.4850, 10.9104, 14.6334,\n",
      "         13.0603, 14.2606, 14.5712, 10.7230, 14.7711,  9.8500, 15.3063, 10.1454,\n",
      "         11.1244, 14.3424, 11.6125, 12.7077, 13.2706,  7.7935, 11.1203,  8.4331,\n",
      "         11.6975,  6.6521,  9.1075, 12.6689, 15.5590,  8.8713, 13.8538, 10.8979]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[10.8144, 10.6953,  9.3158, 10.6574, 12.7902, 13.2523, 13.2060, 12.0029,\n",
      "         14.4558, 14.5192, 12.1642, 11.5082, 12.2112, 10.9497, 12.2692, 11.9302,\n",
      "         12.3483, 14.0304, 11.0984,  9.3158, 12.3703, 12.5259,  6.4392,  8.9491,\n",
      "         14.0175, 12.1017,  9.5415,  9.0335, 10.5217, 13.9673,  8.4955,  6.0573,\n",
      "         11.9110,  9.3158,  5.8421,  9.7985,  8.6711, 12.7294, 11.5426, 13.7101,\n",
      "         10.1456,  8.8687, 11.2516,  7.8606, 10.3950, 11.9839, 10.4380, 11.8744,\n",
      "          9.6954, 12.3168,  9.7111, 14.5312, 10.9434,  7.6248, 13.3398, 12.6260,\n",
      "         10.3026, 11.1559, 10.0332,  9.3311, 12.3875,  9.7305, 12.3076,  8.7390,\n",
      "          9.3069, 12.1546, 12.4322, 10.0458, 12.8718, 10.8123, 12.1887,  9.5745,\n",
      "         15.1428, 12.9335, 12.1341, 13.9822, 11.0061, 11.8674, 11.4330, 14.0487,\n",
      "         10.5156,  9.9886,  8.2457, 10.8528,  8.2391, 13.1568,  9.5377,  9.9787,\n",
      "         11.1772, 12.4641, 10.4146, 13.1710, 11.9324, 18.0703,  9.4996, 13.1554,\n",
      "         11.2803, 13.0636,  9.3484, 13.1150, 11.0982, 15.7629, 13.5538, 12.7904,\n",
      "         13.1178, 11.0961, 15.3111, 20.5992, 10.3949, 11.1465, 12.8731,  7.4661,\n",
      "          9.4586, 10.3101, 10.1887, 17.5922, 10.4624,  9.9060,  9.8400, 10.4670,\n",
      "         13.2000, 11.8214, 13.0849, 11.4259, 15.0025, 14.5982, 14.1302, 10.1810,\n",
      "         10.1782, 10.5043, 11.3075, 12.9826,  8.2231,  9.0664, 15.2642, 10.6643,\n",
      "         11.8532, 11.7153, 10.7734,  9.3158,  9.6099, 12.4705,  9.1499, 12.0411,\n",
      "         12.8537, 13.8340,  7.9707, 14.8085, 12.4954, 12.1056, 14.2098,  6.6087,\n",
      "         12.0735, 18.4013, 13.8375, 15.5947, 12.8094, 13.9289,  8.9675,  8.5411,\n",
      "         11.5439, 11.4731, 12.8936, 10.4550, 10.8248, 10.6993,  9.9256, 12.7920,\n",
      "         10.1948, 11.2884, 10.7102, 11.9578,  8.1100,  9.3989, 10.7649, 11.0869,\n",
      "          9.7422, 16.1672, 19.2789, 12.6001, 12.6904,  6.7595, 11.2985, 16.2420,\n",
      "          6.5399,  8.0978,  8.5609,  9.0507,  8.2510, 11.1423,  8.7462,  8.1011,\n",
      "         11.3145,  8.8573, 12.1612,  8.8894, 15.1142,  9.2248,  9.4173, 11.6325,\n",
      "         12.8652, 10.5417, 10.0767, 13.2471, 17.3214, 11.5092, 14.4750,  9.3158,\n",
      "         11.0037, 10.2316,  9.1806, 10.8740, 11.1297,  9.9451,  4.9997, 12.9262,\n",
      "          7.3995, 14.7358, 11.8334, 13.3408, 14.9026, 12.6649, 15.0226, 12.8761,\n",
      "         15.7508, 11.2556, 10.2222, 10.7082,  9.6741, 11.2240, 12.4157, 11.5010,\n",
      "         14.2958, 12.4059, 13.4711, 10.9904, 10.8672,  9.6092, 11.0349, 15.3945,\n",
      "         14.0035, 14.4502, 14.7007, 11.2617, 14.8844,  8.2438, 15.3198,  9.0731,\n",
      "         10.5021, 15.3537, 11.4867, 13.4927, 12.7052,  8.1397, 11.1621,  9.5094,\n",
      "         12.2102,  6.8668,  9.1967, 12.0920, 14.2901,  7.9378, 13.1095, 11.2745],\n",
      "        [12.4787, 10.1300,  9.4233, 10.3494, 12.0973, 13.1414, 13.4249, 11.7052,\n",
      "         15.1639, 14.4642, 13.1627, 11.1865, 12.2873, 10.5414, 12.4561, 11.6411,\n",
      "         12.3373, 13.9422, 10.9250,  9.4233, 12.4052, 12.1712,  6.8131,  8.7591,\n",
      "         14.3291, 13.1280,  9.4816,  8.8359, 10.4920, 13.8884,  8.5993,  7.4897,\n",
      "         11.1748,  9.4233,  7.2775, 10.2334,  8.6552, 12.8076, 11.5102, 13.9941,\n",
      "         10.3217, 10.4987, 11.4210,  9.0030, 10.4063, 11.7679, 12.3915, 11.5079,\n",
      "         10.1055, 12.3646,  9.4448, 14.3620, 10.8833,  8.1102, 14.1919, 13.1848,\n",
      "         10.5938, 11.1897, 10.0002, 10.1264, 13.3810, 10.1335, 13.5993, 10.0929,\n",
      "          9.2989, 12.8488, 13.1491, 11.3040, 14.1173, 11.4358, 13.3830, 10.5696,\n",
      "         15.0078, 13.5210, 13.0602, 15.0075, 11.8400, 12.7169, 11.3050, 14.3060,\n",
      "          9.5027,  9.3630,  9.3377, 11.6373,  9.7056, 13.8579,  9.9895, 10.5577,\n",
      "         11.6797, 13.2702, 10.8621, 13.2815, 11.9499, 17.7277,  9.4519, 13.8490,\n",
      "         12.1126, 14.2103, 10.1182, 12.4006, 11.1912, 15.4285, 13.3242, 12.3040,\n",
      "         12.7571, 10.8184, 15.0131, 20.5268, 10.2290, 10.6161, 13.4680,  9.6132,\n",
      "         10.1284,  9.6212, 11.3615, 17.0784, 10.4250,  9.6105, 11.3369, 10.4964,\n",
      "         14.1298, 11.6591, 13.4180, 11.7449, 15.2848, 14.8505, 13.9065, 11.2034,\n",
      "         10.4748, 11.9015, 11.8793, 12.8916,  8.6325,  8.4284, 14.7068, 10.4553,\n",
      "         11.6994, 12.0145, 11.8405,  9.4233,  9.7245, 12.3274,  9.1385, 11.7344,\n",
      "         12.8902, 13.7225,  8.6140, 14.5569, 12.5769, 11.9832, 13.5999,  6.9332,\n",
      "         12.0842, 18.3085, 13.9036, 15.4181, 13.0462, 14.0173, 10.2730,  6.9404,\n",
      "         10.9087, 10.7595, 12.9870, 10.3778, 10.3960, 10.6913,  9.9879, 12.4978,\n",
      "          9.8992, 11.0596, 11.1123, 11.3188,  8.0712,  9.6794, 11.0156, 11.6763,\n",
      "          9.5800, 16.0870, 19.2467, 12.8132, 12.9690,  7.2321, 11.0640, 15.8171,\n",
      "          7.5652,  6.9874,  7.3838,  8.3506,  7.8320, 11.0500,  8.8982,  8.0441,\n",
      "         12.6149,  8.4290, 12.2490,  8.6602, 14.8637,  8.9568,  8.6887, 11.5052,\n",
      "         12.5875, 10.6273,  9.7632, 13.7815, 17.3266, 12.0556, 13.9723,  9.4233,\n",
      "         11.5453, 10.2535,  9.0234, 10.8189, 12.4297,  9.3753,  4.9914, 12.8642,\n",
      "          7.0976, 14.2861, 11.9354, 12.9671, 14.9167, 12.8376, 14.8694, 12.9347,\n",
      "         15.6949,  6.9547, 10.0671, 10.6967,  9.6211, 11.4750, 13.2079, 12.0681,\n",
      "         14.5673, 12.5436, 13.5620, 11.9050, 11.5051, 10.4554, 11.4420, 15.1533,\n",
      "         13.6894, 14.2479, 14.5477, 10.9453, 14.7814,  8.8239, 15.1224,  8.9847,\n",
      "          9.4930, 15.7841, 11.2929, 13.7151, 14.4643,  8.0337, 11.8606, 10.8672,\n",
      "         12.3677,  7.0924,  9.3929, 11.7362, 14.5026,  9.2255, 14.3400, 11.6395],\n",
      "        [13.6799, 10.2654,  9.3528, 10.3717, 11.9956, 13.3529, 13.8210, 11.7409,\n",
      "         15.8979, 15.3592, 13.2392, 11.1190, 12.1561, 10.5161, 12.6353, 11.6554,\n",
      "         12.4596, 13.7653, 11.0174,  9.3528, 12.5505, 12.3045,  7.1109,  9.0225,\n",
      "         13.8003, 13.4634,  9.3420,  9.0263, 10.5137, 14.2326,  8.5557,  6.7195,\n",
      "         11.1647,  9.3528,  8.1339, 10.3431,  8.8129, 12.9181, 11.4090, 13.7075,\n",
      "         11.2118, 10.1983, 11.2501,  7.0035, 10.2370, 11.9873, 12.2427, 11.4292,\n",
      "         10.1413, 12.3460,  9.5110, 14.5327, 10.9231,  7.8349, 13.7585, 12.8256,\n",
      "         10.2001, 10.8535, 10.1361, 10.1998, 13.1426, 10.1553, 13.2124,  9.9416,\n",
      "          9.3162, 12.8909, 13.0075, 11.0422, 14.0248, 11.0689, 13.2259, 10.2473,\n",
      "         15.7247, 13.8673, 13.1498, 14.9985, 11.7227, 12.6533, 11.4403, 14.7510,\n",
      "          9.7549,  9.6273,  9.8507, 11.9366,  9.4072, 13.5349, 10.1593, 10.5680,\n",
      "         11.6536, 12.8301, 11.2066, 13.2209, 11.6011, 17.7505, 10.0877, 13.4893,\n",
      "         11.7765, 14.0660,  9.9213, 12.3628, 11.2543, 15.4669, 13.3760, 12.3634,\n",
      "         12.6616, 10.7899, 15.1789, 20.5499, 10.2196, 10.6685, 13.3850, 10.2706,\n",
      "         10.6085,  9.7536, 11.0969, 16.9904, 10.4660,  9.9876, 11.5487, 10.6578,\n",
      "         14.1483, 11.7509, 13.4998, 11.2691, 15.6854, 15.2366, 13.8895, 10.2746,\n",
      "         10.9779, 11.9213, 11.9302, 12.8470,  8.2822,  9.0664, 14.7808, 10.4064,\n",
      "         11.6005, 11.8304, 11.8560,  9.3528, 10.0644, 12.2459,  9.0768, 10.8238,\n",
      "         12.7121, 13.5859,  8.5341, 14.6140, 12.4468, 12.0858, 13.6067,  6.5566,\n",
      "         12.2559, 18.4296, 14.5192, 15.0440, 13.4404, 14.1808, 11.2292,  9.7944,\n",
      "         11.6795, 12.4197, 12.7451, 10.4496,  9.8683, 10.7738, 10.0212, 12.9387,\n",
      "         10.6332, 11.3219, 11.2023, 12.3440,  7.4668,  9.7447, 11.0411, 11.4448,\n",
      "          9.4372, 16.0765, 19.6317, 13.0289, 13.2680,  6.0165, 10.9294, 16.3519,\n",
      "          7.3343,  7.9989,  7.2061,  8.1226,  6.5825, 10.5899,  8.9680,  8.2520,\n",
      "         12.4824,  8.6108, 12.2397,  8.3192, 15.2488,  8.8561,  9.0373, 11.5021,\n",
      "         12.7229, 11.4439, 10.4052, 14.2243, 17.6111, 11.7725, 14.2066,  9.3528,\n",
      "         11.3384, 10.3108,  9.1475, 10.7415, 12.1789,  9.5317,  3.9607, 12.8146,\n",
      "          6.8512, 14.1140, 12.0869, 13.1606, 14.8560, 12.6005, 14.8499, 13.7259,\n",
      "         17.1552,  5.9255, 10.1336, 10.9117,  9.5928, 11.7987, 12.9637, 12.5904,\n",
      "         14.8898, 12.3932, 13.9742, 11.5334, 11.6859, 10.9049, 11.4927, 15.0493,\n",
      "         13.6630, 14.2669, 14.5401, 11.0893, 14.7452,  9.5361, 15.1276,  9.4492,\n",
      "          9.5409, 15.6941, 11.0616, 13.3737, 13.0479,  7.0647, 11.2769, 10.7801,\n",
      "         12.2034,  6.8507,  9.3641, 11.7007, 14.4428,  9.1190, 14.0380, 11.4984]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[12.8365, 10.5385,  7.8986, 10.9618, 12.4448, 13.3707, 13.7176, 11.9835,\n",
      "         15.1915, 15.1069, 12.0655, 11.2478, 12.0640, 10.6824, 12.2339, 11.7828,\n",
      "         12.0414, 13.8001, 11.1418,  9.2183, 12.3634, 12.7781,  7.0885,  8.9650,\n",
      "         13.2465, 12.1060,  9.4147,  9.5891, 10.5171, 13.6294,  8.7685,  7.0707,\n",
      "         11.8142,  7.4749,  8.5342,  9.8849,  7.6372, 13.1266, 11.6498, 13.7512,\n",
      "         10.4988,  9.7907, 11.1132,  8.0630,  9.6266, 11.8824, 11.1581, 10.4112,\n",
      "          8.7612, 12.5290, 10.4681, 14.8131, 10.9873,  8.3504, 13.7991, 12.9099,\n",
      "         10.0024, 11.1370, 10.0464,  9.8598, 12.6643,  9.7987, 12.6113,  9.0302,\n",
      "          9.4524, 12.3208, 12.3928,  9.6389, 12.7810, 10.5110, 11.9362,  9.1355,\n",
      "         14.1067, 13.5975, 12.3123, 14.1022, 10.6431, 11.6778, 11.3397, 14.1727,\n",
      "          9.5953,  9.7045,  8.5650, 10.7678,  8.7211, 13.0769,  9.3290, 10.1284,\n",
      "         11.2811, 12.9618, 10.2895, 13.3729, 12.1647, 16.3210,  9.4978, 13.7328,\n",
      "         11.9461, 13.5801,  9.3756, 12.0914, 11.8982, 15.5981, 13.5394, 12.1853,\n",
      "         12.6191, 10.8404, 15.7624, 20.6382, 10.3147,  9.5477, 12.9553,  8.4672,\n",
      "          9.5735,  9.9244,  9.7017, 16.6446, 10.5088, 10.0844, 10.2948, 10.1981,\n",
      "         13.6221, 11.6879, 13.5884, 10.5015, 14.2523, 14.5420, 13.7065,  9.5875,\n",
      "         10.3292,  9.8169, 11.3682, 12.9323,  8.8952,  7.2809, 14.6405, 10.4864,\n",
      "         12.2506, 12.0585, 12.8914,  9.4159,  9.9024, 13.1085,  9.6016, 10.6085,\n",
      "         11.7007, 13.4793,  9.1006, 14.5542, 12.4067, 11.8734, 14.3711,  8.4892,\n",
      "         12.3563, 18.4996, 14.2852, 15.4179, 13.4230, 14.3940,  9.4812,  9.2016,\n",
      "         11.7497, 12.1620, 12.6315, 10.5824, 10.9437, 11.4258,  9.8372, 12.7285,\n",
      "         11.1316, 11.7106, 11.0783, 12.0263,  8.8954, 10.1476, 11.4325, 10.8434,\n",
      "          9.9596, 16.3883, 19.5459, 12.7652, 13.3286,  7.7030, 11.3983, 16.2649,\n",
      "          7.5939,  8.3720,  7.3856,  7.6331,  7.1652, 10.1888,  9.2453,  8.2769,\n",
      "         11.3253,  8.9865, 12.1185,  8.5405, 14.9306,  9.4325,  9.0060, 11.3991,\n",
      "         13.0868, 10.9662,  9.5649, 13.7344, 17.7237, 11.4365, 14.3824,  9.4123,\n",
      "         10.9184, 11.4117,  8.8530, 11.1373, 13.1676, 10.4986,  3.9392, 12.9992,\n",
      "          7.3074, 14.3827, 11.8637, 13.2603, 14.6664, 12.7665, 14.9012, 13.8996,\n",
      "         16.9504,  7.2410, 10.0531, 10.6686, 10.6802, 11.6348, 12.3619, 12.1970,\n",
      "         14.5260, 11.8032, 13.9827, 10.5528, 10.8182, 10.0208, 11.0594, 15.1014,\n",
      "         12.5442, 14.0971, 14.6376, 10.8402, 14.9032,  8.8769, 15.2626,  9.9940,\n",
      "          9.5761, 14.4259, 10.8199, 12.9501, 12.5276,  8.4181, 11.0433,  9.3000,\n",
      "         12.0216,  6.6926,  9.8299, 12.2386, 14.7551, 10.7835, 14.9784, 11.1112],\n",
      "        [12.8296, 10.5332,  7.8945, 10.9559, 12.4380, 13.3636, 13.7101, 11.9768,\n",
      "         15.1835, 15.0983, 12.0592, 11.2418, 12.0580, 10.6769, 12.2271, 11.7766,\n",
      "         12.0347, 13.7932, 11.1358,  9.2137, 12.3565, 12.7708,  7.0850,  8.9604,\n",
      "         13.2390, 12.1004,  9.4096,  9.5843, 10.5114, 13.6220,  8.7641,  7.0669,\n",
      "         11.8080,  7.4709,  8.5299,  9.8795,  7.6334, 13.1192, 11.6439, 13.7441,\n",
      "         10.4935,  9.7856, 11.1075,  8.0587,  9.6219, 11.8763, 11.1521, 10.4057,\n",
      "          8.7568, 12.5228, 10.4627, 14.8053, 10.9814,  8.3462, 13.7913, 12.9033,\n",
      "          9.9971, 11.1305, 10.0410,  9.8548, 12.6572,  9.7936, 12.6044,  9.0260,\n",
      "          9.4477, 12.3139, 12.3866,  9.6336, 12.7736, 10.5054, 11.9296,  9.1309,\n",
      "         14.0993, 13.5904, 12.3057, 14.0950, 10.6377, 11.6714, 11.3333, 14.1650,\n",
      "          9.5905,  9.6991,  8.5604, 10.7617,  8.7167, 13.0701,  9.3245, 10.1233,\n",
      "         11.2753, 12.9551, 10.2843, 13.3658, 12.1578, 16.3123,  9.4928, 13.7257,\n",
      "         11.9395, 13.5731,  9.3714, 12.0851, 11.8918, 15.5891, 13.5321, 12.1784,\n",
      "         12.6126, 10.8349, 15.7542, 20.6269, 10.3094,  9.5425, 12.9487,  8.4629,\n",
      "          9.5684,  9.9191,  9.6967, 16.6356, 10.5031, 10.0794, 10.2893, 10.1932,\n",
      "         13.6144, 11.6818, 13.5813, 10.4959, 14.2440, 14.5340, 13.6995,  9.5823,\n",
      "         10.3237,  9.8117, 11.3621, 12.9255,  8.8903,  7.2772, 14.6331, 10.4810,\n",
      "         12.2439, 12.0515, 12.8850,  9.4104,  9.8973, 13.1016,  9.5966, 10.6030,\n",
      "         11.6951, 13.4722,  9.0963, 14.5464, 12.4002, 11.8673, 14.3632,  8.4849,\n",
      "         12.3496, 18.4897, 14.2777, 15.4091, 13.4154, 14.3862,  9.4764,  9.1970,\n",
      "         11.7434, 12.1554, 12.6246, 10.5767, 10.9375, 11.4199,  9.8323, 12.7216,\n",
      "         11.1257, 11.7042, 11.0727, 12.0203,  8.8909, 10.1417, 11.4268, 10.8374,\n",
      "          9.9545, 16.3796, 19.5357, 12.7586, 13.3213,  7.6991, 11.3923, 16.2569,\n",
      "          7.5902,  8.3679,  7.3817,  7.6293,  7.1615, 10.1829,  9.2405,  8.2726,\n",
      "         11.3193,  8.9822, 12.1120,  8.5357, 14.9220,  9.4275,  9.0015, 11.3929,\n",
      "         13.0801, 10.9606,  9.5600, 13.7269, 17.7144, 11.4309, 14.3747,  9.4071,\n",
      "         10.9127, 11.4059,  8.8484, 11.1310, 13.1601, 10.4929,  3.9373, 12.9926,\n",
      "          7.3036, 14.3750, 11.8572, 13.2532, 14.6583, 12.7596, 14.8926, 13.8924,\n",
      "         16.9410,  7.2373, 10.0476, 10.6630, 10.6749, 11.6286, 12.3549, 12.1911,\n",
      "         14.5185, 11.7966, 13.9753, 10.5466, 10.8130, 10.0157, 11.0535, 15.0932,\n",
      "         12.5378, 14.0897, 14.6296, 10.8342, 14.8957,  8.8724, 15.2551,  9.9886,\n",
      "          9.5706, 14.4184, 10.8144, 12.9425, 12.5206,  8.4137, 11.0372,  9.2952,\n",
      "         12.0146,  6.6892,  9.8249, 12.2320, 14.7473, 10.7776, 14.9706, 11.1049],\n",
      "        [12.8424, 10.5428,  7.9020, 10.9666, 12.4507, 13.3767, 13.7239, 11.9893,\n",
      "         15.1981, 15.1144, 12.0707, 11.2526, 12.0688, 10.6869, 12.2398, 11.7878,\n",
      "         12.0469, 13.8057, 11.1469,  9.2221, 12.3692, 12.7844,  7.0913,  8.9687,\n",
      "         13.2529, 12.1102,  9.4189,  9.5931, 10.5220, 13.6356,  8.7719,  7.0739,\n",
      "         11.8194,  7.4783,  8.5376,  9.8894,  7.6403, 13.1330, 11.6545, 13.7571,\n",
      "         10.5031,  9.7949, 11.1179,  8.0666,  9.6302, 11.8874, 11.1632, 10.4159,\n",
      "          8.7649, 12.5339, 10.4726, 14.8195, 10.9922,  8.3538, 13.8058, 12.9152,\n",
      "         10.0069, 11.1427, 10.0508,  9.8639, 12.6703,  9.8028, 12.6170,  9.0335,\n",
      "          9.4563, 12.3267, 12.3978,  9.6432, 12.7873, 10.5158, 11.9417,  9.1393,\n",
      "         14.1128, 13.6033, 12.3177, 14.1080, 10.6474, 11.6831, 11.3452, 14.1793,\n",
      "          9.5992,  9.7092,  8.5688, 10.7730,  8.7245, 13.0823,  9.3325, 10.1324,\n",
      "         11.2859, 12.9674, 10.2936, 13.3788, 12.1708, 16.3283,  9.5019, 13.7386,\n",
      "         11.9516, 13.5858,  9.3788, 12.0965, 11.9036, 15.6059, 13.5455, 12.1912,\n",
      "         12.6245, 10.8448, 15.7690, 20.6477, 10.3191,  9.5521, 12.9606,  8.4707,\n",
      "          9.5778,  9.9287,  9.7057, 16.6520, 10.5136, 10.0884, 10.2994, 10.2021,\n",
      "         13.6286, 11.6928, 13.5942, 10.5061, 14.2596, 14.5488, 13.7122,  9.5919,\n",
      "         10.3336,  9.8212, 11.3732, 12.9380,  8.8995,  7.2840, 14.6465, 10.4908,\n",
      "         12.2563, 12.0646, 12.8967,  9.4206,  9.9064, 13.1141,  9.6057, 10.6129,\n",
      "         11.7052, 13.4853,  9.1040, 14.5607, 12.4121, 11.8783, 14.3778,  8.4928,\n",
      "         12.3619, 18.5079, 14.2914, 15.4255, 13.4297, 14.4005,  9.4850,  9.2053,\n",
      "         11.7549, 12.1676, 12.6372, 10.5872, 10.9491, 11.4306,  9.8411, 12.7345,\n",
      "         11.1366, 11.7161, 11.0829, 12.0311,  8.8991, 10.1527, 11.4370, 10.8486,\n",
      "          9.9639, 16.3955, 19.5544, 12.7707, 13.3348,  7.7062, 11.4032, 16.2713,\n",
      "          7.5969,  8.3754,  7.3887,  7.6362,  7.1683, 10.1939,  9.2491,  8.2804,\n",
      "         11.3304,  8.9899, 12.1240,  8.5445, 14.9381,  9.4365,  9.0095, 11.4044,\n",
      "         13.0923, 10.9707,  9.5689, 13.7405, 17.7314, 11.4408, 14.3889,  9.4167,\n",
      "         10.9231, 11.4164,  8.8568, 11.1426, 13.1740, 10.5035,  3.9407, 13.0045,\n",
      "          7.3106, 14.3892, 11.8691, 13.2661, 14.6731, 12.7722, 14.9087, 13.9054,\n",
      "         16.9583,  7.2441, 10.0577, 10.6732, 10.6844, 11.6400, 12.3679, 12.2017,\n",
      "         14.5320, 11.8090, 13.9888, 10.5582, 10.8222, 10.0249, 11.0644, 15.1081,\n",
      "         12.5493, 14.1031, 14.6443, 10.8453, 14.9091,  8.8806, 15.2685,  9.9985,\n",
      "          9.5809, 14.4320, 10.8245, 12.9567, 12.5336,  8.4217, 11.0485,  9.3037,\n",
      "         12.0276,  6.6954,  9.8338, 12.2442, 14.7616, 10.7883, 14.9849, 11.1165]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor(0.4779, grad_fn=<MseLossBackward0>)\n",
      "..........................................\n",
      "..........................................\n",
      "------------------------------bwd-1-------------------------------------------------\n",
      "tensor([[12.8039,  9.3423,  9.3749,  ...,  8.4851, 13.5574, 10.9695],\n",
      "        [12.3535, 10.3164,  9.3939,  ...,  8.8713, 13.8538, 10.8979],\n",
      "        [12.8707, 10.5261,  6.7361,  ...,  9.2763, 14.2041, 11.0265],\n",
      "        ...,\n",
      "        [12.4787, 10.1300,  9.4233,  ...,  9.2255, 14.3400, 11.6395],\n",
      "        [13.6799, 10.2654,  9.3528,  ...,  9.1190, 14.0380, 11.4984],\n",
      "        [13.1822, 10.3069,  9.3127,  ...,  8.8492, 13.8164, 11.4326]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[12.5154,  9.1281,  9.3397,  ...,  8.4757, 13.5592, 11.0681],\n",
      "        [12.8039,  9.3423,  9.3749,  ...,  8.4851, 13.5574, 10.9695],\n",
      "        [13.6686, 10.3491,  8.7091,  ...,  8.4368, 13.7771, 10.8331],\n",
      "        ...,\n",
      "        [10.8144, 10.6953,  9.3158,  ...,  7.9378, 13.1095, 11.2745],\n",
      "        [12.4787, 10.1300,  9.4233,  ...,  9.2255, 14.3400, 11.6395],\n",
      "        [13.6799, 10.2654,  9.3528,  ...,  9.1190, 14.0380, 11.4984]])\n",
      "..........................................\n",
      "------------------------------fwd-1-------------------------------------------------\n",
      "..........................................\n",
      "DP30\n",
      "..........................................\n",
      "tensor([[12.5643,  9.8858,  9.2916,  ..., 10.4699, 14.2729, 11.4123],\n",
      "        [12.6370, 10.2126,  8.5770,  ..., 11.0107, 14.6941, 11.2297],\n",
      "        [16.6716,  9.8858,  4.1114,  ..., 10.6120, 14.2646, 10.9489],\n",
      "        ...,\n",
      "        [11.9616, 10.4346,  6.9919,  ..., 11.2072, 15.4022, 11.2202],\n",
      "        [13.1362, 11.0091,  9.3585,  ..., 10.6613, 14.5381, 11.0048],\n",
      "        [11.3929,  9.9188,  9.0868,  ..., 11.1852, 14.0658, 11.0798]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[12.5504, 10.1110,  9.3837,  ..., 10.7526, 14.8512, 10.9567],\n",
      "        [12.8716, 10.1247,  7.6744,  ..., 10.8541, 14.5843, 11.5420],\n",
      "        [12.5987, 10.5542,  5.1192,  ..., 10.7440, 14.8823, 10.9712],\n",
      "        ...,\n",
      "        [14.0273, 10.2935,  7.1470,  ..., 10.4400, 14.3452, 10.8084],\n",
      "        [12.5750, 10.0688,  9.2009,  ..., 10.2089, 13.9985, 11.1384],\n",
      "        [13.4534, 10.1143,  8.7439,  ..., 10.6204, 14.5793, 11.3174]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[12.6988, 10.4340,  7.8172,  ..., 10.6687, 14.8231, 10.9868],\n",
      "        [12.8079, 10.5168,  7.8816,  ..., 10.7596, 14.9461, 11.0853],\n",
      "        [12.8431, 10.5434,  7.9024,  ..., 10.7889, 14.9858, 11.1172],\n",
      "        ...,\n",
      "        [12.8271, 10.5317,  7.8931,  ..., 10.7758, 14.9680, 11.1026],\n",
      "        [12.9385, 10.6158,  7.9588,  ..., 10.8683, 15.0933, 11.2032],\n",
      "        [12.6439, 10.3927,  7.7849,  ..., 10.6232, 14.7615, 10.9371]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor(0.4805, grad_fn=<MseLossBackward0>)\n",
      "..........................................\n",
      "..........................................\n",
      "------------------------------bwd-1-------------------------------------------------\n",
      "tensor([[12.6370, 10.2126,  8.5770,  ..., 11.0107, 14.6941, 11.2297],\n",
      "        [16.6716,  9.8858,  4.1114,  ..., 10.6120, 14.2646, 10.9489],\n",
      "        [13.1236, 10.4111,  6.3277,  ..., 10.6507, 14.5418, 10.9686],\n",
      "        ...,\n",
      "        [14.0273, 10.2935,  7.1470,  ..., 10.4400, 14.3452, 10.8084],\n",
      "        [12.5750, 10.0688,  9.2009,  ..., 10.2089, 13.9985, 11.1384],\n",
      "        [13.4534, 10.1143,  8.7439,  ..., 10.6204, 14.5793, 11.3174]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[12.5643,  9.8858,  9.2916,  ..., 10.4699, 14.2729, 11.4123],\n",
      "        [12.6370, 10.2126,  8.5770,  ..., 11.0107, 14.6941, 11.2297],\n",
      "        [16.6716,  9.8858,  4.1114,  ..., 10.6120, 14.2646, 10.9489],\n",
      "        ...,\n",
      "        [12.4690, 11.5431,  6.9036,  ..., 10.8676, 14.6750, 10.9051],\n",
      "        [14.0273, 10.2935,  7.1470,  ..., 10.4400, 14.3452, 10.8084],\n",
      "        [12.5750, 10.0688,  9.2009,  ..., 10.2089, 13.9985, 11.1384]])\n",
      "..........................................\n",
      "------------------------------fwd-1-------------------------------------------------\n",
      "------------------------------bwd-1-------------------------------------------------\n",
      "tensor([[13.0970, 10.3321,  9.3637,  ..., 10.7520, 14.7959, 11.0294],\n",
      "        [15.0204, 10.2795,  9.2925,  ..., 10.6055, 14.0391, 11.0204],\n",
      "        [13.0507, 10.6427,  9.2097,  ..., 10.7296, 14.6136, 11.2381],\n",
      "        ...,\n",
      "        [12.1884, 10.3945,  9.0931,  ..., 11.0563, 15.5813, 11.0770],\n",
      "        [13.8808, 11.0597,  7.5241,  ..., 11.1831, 15.8740, 10.8331],\n",
      "        [12.2133, 10.9596, 11.6617,  ..., 10.8702, 15.6132, 11.1174]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[12.9817, 10.3434,  9.2704,  ..., 11.0193, 14.6874, 11.0462],\n",
      "        [13.0970, 10.3321,  9.3637,  ..., 10.7520, 14.7959, 11.0294],\n",
      "        [15.0204, 10.2795,  9.2925,  ..., 10.6055, 14.0391, 11.0204],\n",
      "        ...,\n",
      "        [11.4367, 10.5801,  9.3124,  ..., 11.3855, 15.6862, 11.5310],\n",
      "        [12.1884, 10.3945,  9.0931,  ..., 11.0563, 15.5813, 11.0770],\n",
      "        [13.8808, 11.0597,  7.5241,  ..., 11.1831, 15.8740, 10.8331]])\n",
      "..........................................\n",
      "------------------------------fwd-1-------------------------------------------------\n",
      "..........................................\n",
      "DP28\n",
      "..........................................\n",
      "tensor([[14.1561, 10.6038,  7.5238,  ..., 11.1548, 15.0585, 11.2208],\n",
      "        [14.3376, 10.4433,  6.5008,  ..., 11.0691, 14.9429, 11.0203],\n",
      "        [13.3700, 10.3952,  6.6862,  ..., 11.0360, 14.7115, 10.9604],\n",
      "        ...,\n",
      "        [12.2541, 10.3402,  8.1256,  ..., 11.4802, 15.4155, 11.1430],\n",
      "        [12.7264, 10.2808,  9.3697,  ..., 11.6452, 15.4907, 11.0336],\n",
      "        [14.3189, 10.1806,  8.6080,  ..., 11.5480, 15.3201, 10.9019]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[12.6117,  9.8736,  9.3659,  ..., 10.0018, 14.5276, 10.9678],\n",
      "        [13.0738, 10.0739,  7.9195,  ..., 10.6705, 14.5486, 11.0838],\n",
      "        [13.3327, 10.1250,  6.1209,  ..., 10.2372, 14.2719, 10.8109],\n",
      "        ...,\n",
      "        [12.6332, 10.2575,  5.4947,  ..., 10.4890, 14.7371, 10.5241],\n",
      "        [12.3818,  9.9794,  7.5218,  ..., 10.3982, 14.7235, 10.5289],\n",
      "        [13.9828,  9.7084,  6.9109,  ...,  9.9422, 14.2233, 10.9011]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[12.8757, 10.5681,  7.9217,  ..., 10.8160, 15.0225, 11.1466],\n",
      "        [12.8324, 10.5353,  7.8961,  ..., 10.7800, 14.9737, 11.1075],\n",
      "        [12.7907, 10.5037,  7.8715,  ..., 10.7453, 14.9268, 11.0698],\n",
      "        ...,\n",
      "        [12.8541, 10.5518,  7.9089,  ..., 10.7980, 14.9982, 11.1270],\n",
      "        [12.8720, 10.5654,  7.9195,  ..., 10.8130, 15.0184, 11.1432],\n",
      "        [12.8687, 10.5628,  7.9175,  ..., 10.8102, 15.0146, 11.1403]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor(0.4757, grad_fn=<MseLossBackward0>)\n",
      "..........................................\n",
      "..........................................\n",
      "------------------------------bwd-1-------------------------------------------------\n",
      "tensor([[14.3376, 10.4433,  6.5008,  ..., 11.0691, 14.9429, 11.0203],\n",
      "        [13.3700, 10.3952,  6.6862,  ..., 11.0360, 14.7115, 10.9604],\n",
      "        [12.3101, 10.3019,  7.4795,  ..., 11.3332, 14.9436, 11.0995],\n",
      "        ...,\n",
      "        [12.6332, 10.2575,  5.4947,  ..., 10.4890, 14.7371, 10.5241],\n",
      "        [12.3818,  9.9794,  7.5218,  ..., 10.3982, 14.7235, 10.5289],\n",
      "        [13.9828,  9.7084,  6.9109,  ...,  9.9422, 14.2233, 10.9011]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[14.1561, 10.6038,  7.5238,  ..., 11.1548, 15.0585, 11.2208],\n",
      "        [14.3376, 10.4433,  6.5008,  ..., 11.0691, 14.9429, 11.0203],\n",
      "        [13.3700, 10.3952,  6.6862,  ..., 11.0360, 14.7115, 10.9604],\n",
      "        ...,\n",
      "        [12.9308,  9.9988,  4.9644,  ...,  9.9149, 14.2170, 10.6428],\n",
      "        [12.6332, 10.2575,  5.4947,  ..., 10.4890, 14.7371, 10.5241],\n",
      "        [12.3818,  9.9794,  7.5218,  ..., 10.3982, 14.7235, 10.5289]])\n",
      "..........................................\n",
      "------------------------------fwd-1-------------------------------------------------\n",
      "..........................................\n",
      "DP19\n",
      "..........................................\n",
      "tensor([[12.8785, 10.5630,  9.3562,  ..., 10.0010, 14.0008, 11.0983],\n",
      "        [12.6865, 10.8159,  6.2808,  ...,  9.6878, 14.0647, 11.2486],\n",
      "        [13.9217, 10.2089,  8.8752,  ...,  9.8867, 13.8222, 11.1468],\n",
      "        [13.1763, 10.2370,  6.8632,  ...,  9.5680, 13.7353, 11.2579],\n",
      "        [13.4577, 10.8234,  5.0891,  ...,  9.9246, 13.9094, 11.0616],\n",
      "        [12.8412, 10.8474,  7.4083,  ..., 10.4844, 14.4478, 11.0628]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[13.1638, 10.8061,  7.5189,  ...,  9.3732, 13.9534, 10.9237],\n",
      "        [12.5016, 10.9675,  9.2513,  ...,  9.9488, 14.3732, 11.0236],\n",
      "        [13.6311, 10.2928,  7.0554,  ..., 10.5367, 14.4805, 10.8727],\n",
      "        [11.4229, 10.3283,  6.1861,  ...,  9.9231, 14.0234, 10.9623],\n",
      "        [12.8233, 10.1966,  8.2754,  ...,  9.9639, 14.0221, 11.0566],\n",
      "        [13.1800, 10.3433,  9.4208,  ..., 10.3595, 14.3462, 11.2861]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[12.8998, 10.5865,  7.9359,  ..., 10.8361, 15.0497, 11.1683],\n",
      "        [12.8592, 10.5559,  7.9120,  ..., 10.8024, 15.0041, 11.1315],\n",
      "        [12.8461, 10.5457,  7.9042,  ..., 10.7913, 14.9891, 11.1198],\n",
      "        [12.8912, 10.5799,  7.9309,  ..., 10.8289, 15.0400, 11.1606],\n",
      "        [12.8160, 10.5229,  7.8864,  ..., 10.7663, 14.9553, 11.0926],\n",
      "        [12.8487, 10.5476,  7.9057,  ..., 10.7935, 14.9921, 11.1223]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor(0.5248, grad_fn=<MseLossBackward0>)\n",
      "..........................................\n",
      "..........................................\n",
      "------------------------------bwd-1-------------------------------------------------\n",
      "tensor([[12.6865, 10.8159,  6.2808,  ...,  9.6878, 14.0647, 11.2486],\n",
      "        [13.9217, 10.2089,  8.8752,  ...,  9.8867, 13.8222, 11.1468],\n",
      "        [13.1763, 10.2370,  6.8632,  ...,  9.5680, 13.7353, 11.2579],\n",
      "        ...,\n",
      "        [11.4229, 10.3283,  6.1861,  ...,  9.9231, 14.0234, 10.9623],\n",
      "        [12.8233, 10.1966,  8.2754,  ...,  9.9639, 14.0221, 11.0566],\n",
      "        [13.1800, 10.3433,  9.4208,  ..., 10.3595, 14.3462, 11.2861]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[12.8785, 10.5630,  9.3562,  ..., 10.0010, 14.0008, 11.0983],\n",
      "        [12.6865, 10.8159,  6.2808,  ...,  9.6878, 14.0647, 11.2486],\n",
      "        [13.9217, 10.2089,  8.8752,  ...,  9.8867, 13.8222, 11.1468],\n",
      "        ...,\n",
      "        [13.6311, 10.2928,  7.0554,  ..., 10.5367, 14.4805, 10.8727],\n",
      "        [11.4229, 10.3283,  6.1861,  ...,  9.9231, 14.0234, 10.9623],\n",
      "        [12.8233, 10.1966,  8.2754,  ...,  9.9639, 14.0221, 11.0566]])\n",
      "..........................................\n",
      "------------------------------fwd-1-------------------------------------------------\n",
      "..........................................\n",
      "DP11\n",
      "..........................................\n",
      "tensor([[12.2316, 10.1973,  9.2912,  ...,  9.9853, 14.6115, 10.9997],\n",
      "        [12.2223, 10.5381,  7.8530,  ..., 10.2480, 14.5067, 10.9921],\n",
      "        [12.5851, 10.6187,  9.3323,  ..., 10.3179, 14.6689, 11.0733],\n",
      "        ...,\n",
      "        [11.6441, 10.6391,  9.5466,  ...,  9.7987, 14.6884, 11.1828],\n",
      "        [12.5528, 10.4288,  7.0429,  ...,  9.3565, 14.7414, 11.0959],\n",
      "        [12.6712, 10.5994,  7.5948,  ...,  9.5207, 14.7324, 11.1057]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[12.9359, 10.7357,  8.0751,  ...,  7.9458, 14.4897, 11.2706],\n",
      "        [13.2745, 10.3955,  9.2245,  ...,  8.9887, 14.6061, 11.0744],\n",
      "        [13.4915, 10.5214,  9.3704,  ...,  8.5206, 14.3942, 11.5446],\n",
      "        ...,\n",
      "        [13.2432, 10.1065,  8.8120,  ...,  6.9091, 14.9125, 11.2948],\n",
      "        [13.3176, 10.4011,  9.3813,  ...,  8.3593, 14.9193, 11.1527],\n",
      "        [12.9016, 10.4519,  9.3198,  ...,  8.4746, 14.8090, 11.4620]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[12.8191, 10.5254,  7.8883,  ..., 10.7690, 14.9589, 11.0954],\n",
      "        [12.8389, 10.5405,  7.9000,  ..., 10.7855, 14.9812, 11.1132],\n",
      "        [12.8524, 10.5508,  7.9080,  ..., 10.7968, 14.9964, 11.1253],\n",
      "        ...,\n",
      "        [12.8626, 10.5585,  7.9140,  ..., 10.8052, 15.0079, 11.1346],\n",
      "        [12.8403, 10.5416,  7.9009,  ..., 10.7867, 14.9828, 11.1144],\n",
      "        [12.8238, 10.5290,  7.8911,  ..., 10.7729, 14.9642, 11.0996]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor(0.5084, grad_fn=<MseLossBackward0>)\n",
      "..........................................\n",
      "..........................................\n",
      "------------------------------bwd-1-------------------------------------------------\n",
      "tensor([[12.2223, 10.5381,  7.8530,  ..., 10.2480, 14.5067, 10.9921],\n",
      "        [12.5851, 10.6187,  9.3323,  ..., 10.3179, 14.6689, 11.0733],\n",
      "        [12.2929, 10.6280,  9.5946,  ...,  9.9781, 14.9559, 11.2099],\n",
      "        ...,\n",
      "        [13.2432, 10.1065,  8.8120,  ...,  6.9091, 14.9125, 11.2948],\n",
      "        [13.3176, 10.4011,  9.3813,  ...,  8.3593, 14.9193, 11.1527],\n",
      "        [12.9016, 10.4519,  9.3198,  ...,  8.4746, 14.8090, 11.4620]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[12.2316, 10.1973,  9.2912,  ...,  9.9853, 14.6115, 10.9997],\n",
      "        [12.2223, 10.5381,  7.8530,  ..., 10.2480, 14.5067, 10.9921],\n",
      "        [12.5851, 10.6187,  9.3323,  ..., 10.3179, 14.6689, 11.0733],\n",
      "        ...,\n",
      "        [13.8345, 11.1797,  6.3762,  ...,  8.6880, 14.4715, 10.6335],\n",
      "        [13.2432, 10.1065,  8.8120,  ...,  6.9091, 14.9125, 11.2948],\n",
      "        [13.3176, 10.4011,  9.3813,  ...,  8.3593, 14.9193, 11.1527]])\n",
      "..........................................\n",
      "------------------------------fwd-1-------------------------------------------------\n",
      "..........................................\n",
      "DP14\n",
      "..........................................\n",
      "tensor([[12.7899, 10.1022,  9.3084,  ..., 10.0617, 14.3188, 11.2678],\n",
      "        [12.1483, 10.2183,  8.4642,  ..., 10.2732, 14.2715, 11.1586],\n",
      "        [11.9564, 10.3186,  8.1032,  ..., 10.4606, 14.4568, 11.0714],\n",
      "        ...,\n",
      "        [13.9434, 10.5843,  9.3089,  ..., 10.3417, 14.2043, 10.8914],\n",
      "        [12.3891, 10.4636,  9.2841,  ..., 10.3058, 14.5240, 11.0095],\n",
      "        [12.4724, 10.5500,  9.3486,  ..., 10.2311, 14.8104, 10.9526]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[12.5672, 10.4958,  6.8718,  ..., 10.9270, 15.1362, 11.0620],\n",
      "        [12.9476, 10.4976,  6.1668,  ..., 11.1196, 15.5084, 11.1139],\n",
      "        [13.0968, 10.3741,  4.6065,  ..., 10.4431, 14.9489, 11.2733],\n",
      "        ...,\n",
      "        [13.6249, 10.5114,  7.3825,  ..., 11.0791, 15.2460, 11.2353],\n",
      "        [13.9132, 10.4597,  8.4449,  ..., 11.2822, 15.2250, 11.0923],\n",
      "        [14.3846, 10.3149,  8.3633,  ..., 11.0314, 14.9279, 10.9659]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[12.8661, 10.5608,  7.9160,  ..., 10.8080, 15.0116, 11.1379],\n",
      "        [12.8125, 10.5202,  7.8844,  ..., 10.7634, 14.9513, 11.0895],\n",
      "        [12.8216, 10.5273,  7.8898,  ..., 10.7711, 14.9617, 11.0977],\n",
      "        ...,\n",
      "        [12.8261, 10.5304,  7.8923,  ..., 10.7746, 14.9665, 11.1018],\n",
      "        [12.8123, 10.5201,  7.8842,  ..., 10.7632, 14.9511, 11.0893],\n",
      "        [12.8114, 10.5195,  7.8838,  ..., 10.7626, 14.9502, 11.0884]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor(0.5604, grad_fn=<MseLossBackward0>)\n",
      "..........................................\n",
      "..........................................\n",
      "------------------------------bwd-1-------------------------------------------------\n",
      "tensor([[12.1483, 10.2183,  8.4642,  ..., 10.2732, 14.2715, 11.1586],\n",
      "        [11.9564, 10.3186,  8.1032,  ..., 10.4606, 14.4568, 11.0714],\n",
      "        [13.2413, 10.2763,  9.3458,  ..., 10.5984, 14.7255, 10.9916],\n",
      "        ...,\n",
      "        [13.6249, 10.5114,  7.3825,  ..., 11.0791, 15.2460, 11.2353],\n",
      "        [13.9132, 10.4597,  8.4449,  ..., 11.2822, 15.2250, 11.0923],\n",
      "        [14.3846, 10.3149,  8.3633,  ..., 11.0314, 14.9279, 10.9659]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[12.7899, 10.1022,  9.3084,  ..., 10.0617, 14.3188, 11.2678],\n",
      "        [12.1483, 10.2183,  8.4642,  ..., 10.2732, 14.2715, 11.1586],\n",
      "        [11.9564, 10.3186,  8.1032,  ..., 10.4606, 14.4568, 11.0714],\n",
      "        ...,\n",
      "        [13.7970, 10.4556,  4.4252,  ..., 10.7964, 14.9612, 11.1810],\n",
      "        [13.6249, 10.5114,  7.3825,  ..., 11.0791, 15.2460, 11.2353],\n",
      "        [13.9132, 10.4597,  8.4449,  ..., 11.2822, 15.2250, 11.0923]])\n",
      "..........................................\n",
      "------------------------------fwd-1-------------------------------------------------\n",
      "..........................................\n",
      "DP23\n",
      "..........................................\n",
      "tensor([[12.2284, 10.7884,  7.5149,  ..., 11.4885, 15.3714, 11.6118],\n",
      "        [12.5680, 11.1869,  6.8918,  ..., 10.4855, 14.6176, 12.2109],\n",
      "        [12.8519, 10.5869,  7.7631,  ..., 11.9121, 15.9670, 10.4944],\n",
      "        [12.6297, 10.4853,  5.1430,  ..., 11.9197, 16.1248, 10.7737]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[11.7067, 10.6859,  9.3832,  ..., 11.2112, 15.9652, 10.8104],\n",
      "        [11.7722, 10.4412,  7.9459,  ..., 11.2777, 15.8780, 10.5866],\n",
      "        [13.0890, 11.1809,  7.5183,  ..., 11.5597, 16.0035, 10.9325],\n",
      "        [12.0328, 10.4508,  7.5185,  ..., 11.3250, 15.7434, 10.8620]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[12.8427, 10.5433,  7.9022,  ..., 10.7886, 14.9854, 11.1167],\n",
      "        [12.8727, 10.5661,  7.9200,  ..., 10.8136, 15.0192, 11.1437],\n",
      "        [12.7889, 10.5024,  7.8705,  ..., 10.7438, 14.9248, 11.0681],\n",
      "        [12.7534, 10.4755,  7.8495,  ..., 10.7142, 14.8848, 11.0361]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor(0.4793, grad_fn=<MseLossBackward0>)\n",
      "..........................................\n",
      "..........................................\n",
      "------------------------------bwd-1-------------------------------------------------\n",
      "tensor([[12.5680, 11.1869,  6.8918,  ..., 10.4855, 14.6176, 12.2109],\n",
      "        [12.8519, 10.5869,  7.7631,  ..., 11.9121, 15.9670, 10.4944],\n",
      "        [13.1175, 10.0860,  4.3800,  ..., 11.6820, 15.9130, 10.5314],\n",
      "        ...,\n",
      "        [11.7067, 10.6859,  9.3832,  ..., 11.2112, 15.9652, 10.8104],\n",
      "        [11.7722, 10.4412,  7.9459,  ..., 11.2777, 15.8780, 10.5866],\n",
      "        [13.0890, 11.1809,  7.5183,  ..., 11.5597, 16.0035, 10.9325]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[12.2284, 10.7884,  7.5149,  ..., 11.4885, 15.3714, 11.6118],\n",
      "        [12.5680, 11.1869,  6.8918,  ..., 10.4855, 14.6176, 12.2109],\n",
      "        [12.8519, 10.5869,  7.7631,  ..., 11.9121, 15.9670, 10.4944],\n",
      "        ...,\n",
      "        [11.8235, 11.0721,  9.4040,  ..., 11.4014, 16.0104, 10.6766],\n",
      "        [11.7067, 10.6859,  9.3832,  ..., 11.2112, 15.9652, 10.8104],\n",
      "        [11.7722, 10.4412,  7.9459,  ..., 11.2777, 15.8780, 10.5866]])\n",
      "..........................................\n",
      "------------------------------fwd-1-------------------------------------------------\n",
      "..........................................\n",
      "DP22\n",
      "..........................................\n",
      "tensor([[12.6239, 10.8291,  7.4963,  ..., 11.0770, 14.8146, 11.0254],\n",
      "        [14.2647, 10.1014,  9.3714,  ..., 11.4201, 14.7793, 10.8247],\n",
      "        [12.1503,  9.9787,  6.3763,  ..., 11.3743, 14.7201, 10.7246],\n",
      "        [11.1917, 10.3419,  8.6713,  ..., 10.8738, 14.7663, 10.9895],\n",
      "        [11.4106, 10.3691,  6.9088,  ..., 11.0447, 14.8826, 11.1917]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[12.2118, 10.6774,  7.6072,  ..., 11.0290, 15.0409, 10.9214],\n",
      "        [13.8496, 10.3541,  5.5471,  ..., 10.8524, 14.4039,  9.3520],\n",
      "        [13.9753, 10.2596,  9.3494,  ..., 10.9222, 13.3541, 10.8570],\n",
      "        [12.9711, 10.4246,  5.7257,  ..., 10.0867, 13.5847, 11.1068],\n",
      "        [13.6300, 10.2249,  7.0049,  ..., 10.9299, 13.8078, 11.0520]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[12.8075, 10.5164,  7.8814,  ..., 10.7593, 14.9457, 11.0850],\n",
      "        [12.8621, 10.5577,  7.9136,  ..., 10.8047, 15.0072, 11.1344],\n",
      "        [12.8176, 10.5239,  7.8873,  ..., 10.7676, 14.9569, 11.0942],\n",
      "        [12.8134, 10.5207,  7.8848,  ..., 10.7641, 14.9522, 11.0903],\n",
      "        [12.8540, 10.5516,  7.9088,  ..., 10.7979, 14.9980, 11.1270]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor(0.6284, grad_fn=<MseLossBackward0>)\n",
      "..........................................\n",
      "..........................................\n",
      "------------------------------bwd-1-------------------------------------------------\n",
      "tensor([[14.2647, 10.1014,  9.3714,  ..., 11.4201, 14.7793, 10.8247],\n",
      "        [12.1503,  9.9787,  6.3763,  ..., 11.3743, 14.7201, 10.7246],\n",
      "        [11.1917, 10.3419,  8.6713,  ..., 10.8738, 14.7663, 10.9895],\n",
      "        ...,\n",
      "        [13.9753, 10.2596,  9.3494,  ..., 10.9222, 13.3541, 10.8570],\n",
      "        [12.9711, 10.4246,  5.7257,  ..., 10.0867, 13.5847, 11.1068],\n",
      "        [13.6300, 10.2249,  7.0049,  ..., 10.9299, 13.8078, 11.0520]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[12.6239, 10.8291,  7.4963,  ..., 11.0770, 14.8146, 11.0254],\n",
      "        [14.2647, 10.1014,  9.3714,  ..., 11.4201, 14.7793, 10.8247],\n",
      "        [12.1503,  9.9787,  6.3763,  ..., 11.3743, 14.7201, 10.7246],\n",
      "        ...,\n",
      "        [13.8496, 10.3541,  5.5471,  ..., 10.8524, 14.4039,  9.3520],\n",
      "        [13.9753, 10.2596,  9.3494,  ..., 10.9222, 13.3541, 10.8570],\n",
      "        [12.9711, 10.4246,  5.7257,  ..., 10.0867, 13.5847, 11.1068]])\n",
      "..........................................\n",
      "------------------------------fwd-1-------------------------------------------------\n",
      "..........................................\n",
      "DP18\n",
      "..........................................\n",
      "tensor([[13.8373, 10.4579,  9.3472, 11.5373, 12.9734, 13.8802, 14.3001, 12.0584,\n",
      "         15.5782, 15.8916, 11.6730, 11.2340, 11.9770, 10.3697, 12.2675, 11.4076,\n",
      "         11.4520, 12.9762, 11.3425,  9.3472, 12.8394, 12.8766,  7.2594,  8.2270,\n",
      "         12.6825, 11.6584,  9.0905,  8.7243, 10.1861, 13.4025,  8.8154,  6.9002,\n",
      "         12.3934,  9.3472,  8.4627,  9.7518,  8.5510, 12.2622, 11.2630, 13.8176,\n",
      "         11.1207,  9.9884, 10.7964,  8.8149,  9.3155, 11.5712, 10.2579, 10.9253,\n",
      "          8.6390, 12.2038, 10.3302, 14.6621, 10.9676,  9.2927, 13.5066, 12.4632,\n",
      "         10.4353, 10.9686,  9.6767,  9.8775, 12.8186,  9.4336, 12.3754,  8.1669,\n",
      "          9.6612, 12.5629, 12.1773,  9.5501, 12.6601, 11.3205, 11.6113,  8.7572,\n",
      "         13.4381, 14.0086, 12.3330, 13.3853,  9.7128, 11.2705, 10.9985, 14.2735,\n",
      "         10.2217,  9.5874,  9.1879, 10.9470,  8.0076, 14.3150,  9.4370, 11.7148,\n",
      "         10.9241, 12.4484, 10.1958, 13.5619, 12.1195, 15.3650,  9.7790, 13.3962,\n",
      "         11.8310, 12.8039,  8.4307, 12.4804, 11.5257, 15.0966, 12.9946, 11.6653,\n",
      "         13.0077, 10.7622, 15.9368, 20.2572, 10.0875, 10.1248, 12.6893,  8.6453,\n",
      "          8.8871,  9.7200,  9.8110, 17.2827, 10.9960,  9.8047, 10.9184,  9.0026,\n",
      "         13.2377, 11.5745, 13.2723,  9.7938, 13.8864, 14.6501, 15.1717, 10.3978,\n",
      "         10.8120,  9.6642, 11.3053, 12.7661,  9.7701,  5.9886, 14.3204, 10.4574,\n",
      "         12.2905, 12.1001, 11.3860,  9.3472,  8.6895, 13.4942,  9.7248, 12.0797,\n",
      "         12.5256, 13.7709,  9.1911, 15.0485, 12.2002, 11.9719, 15.2458,  8.1681,\n",
      "         12.8487, 18.7808, 14.8243, 17.2867, 13.7394, 15.1262, 10.4728, 11.4059,\n",
      "         12.7953, 13.8082, 13.2330, 11.1286, 10.4195, 12.0462,  9.3333, 12.9610,\n",
      "         12.3047, 12.5496, 11.0013, 12.8302, 10.2993, 10.6515, 11.8778, 10.5328,\n",
      "         10.0781, 16.1213, 19.7550, 13.4073, 14.0113,  8.8714, 10.9394, 16.4861,\n",
      "          8.5565,  8.4961,  8.7231,  7.9215,  9.0500, 12.0457, 10.9621,  8.1879,\n",
      "         10.5820,  8.9527, 11.7505,  8.6449, 14.8542,  8.8846,  9.2710, 11.0954,\n",
      "         13.7035, 11.6627,  9.2080, 14.1255, 18.0722, 12.1745, 14.3708,  9.3472,\n",
      "         11.0033, 11.9177,  9.3145, 11.1038, 13.3694, 11.1687,  3.0470, 13.1459,\n",
      "          7.0088, 13.6525, 11.1336, 12.5742, 13.7925, 12.4108, 14.8459, 14.4730,\n",
      "         16.7603,  4.8124,  9.9903, 10.5399,  9.9833, 11.8554, 12.8053, 12.0836,\n",
      "         14.3293, 11.6867, 14.3419, 10.8023, 10.9245,  9.1580, 11.1348, 15.4121,\n",
      "         12.1167, 14.0577, 14.4520, 10.5923, 14.6687, 10.7300, 15.3749,  9.6576,\n",
      "         10.4375, 13.9130, 11.5818, 12.5818, 13.1927,  9.1024, 11.8369,  8.4821,\n",
      "         11.8528,  7.0308, 10.0844, 12.6470, 14.8342, 11.3862, 15.0673, 10.8571],\n",
      "        [11.0275, 10.4368,  5.8126, 10.6296, 12.8829, 13.2636, 13.6648, 11.3642,\n",
      "         15.0572, 14.6905, 11.3507, 11.1695, 12.0165,  9.8033, 12.1845, 11.8063,\n",
      "         11.3455, 13.7975, 11.0609,  8.5834, 13.1904, 13.0546,  7.3004,  8.5190,\n",
      "         12.6282, 10.8811,  9.4579,  8.6478,  9.2016, 12.7053,  8.8133,  7.0211,\n",
      "         12.2011,  9.2857,  9.9212, 10.0658,  8.7212, 12.3320, 11.7019, 11.6683,\n",
      "          9.4504,  9.8178, 10.4836,  8.3490,  9.4006, 11.7291, 11.3064, 12.2295,\n",
      "          8.2382,  9.0812, 10.5080, 14.4439, 11.3155,  8.7961, 14.0170, 13.0270,\n",
      "         11.0087, 11.3274,  9.9202,  9.8471, 12.8392,  9.5736, 12.9050,  8.4020,\n",
      "          9.6352,  9.2098, 12.1521,  9.7738, 10.8098, 11.4810, 11.9470,  8.8756,\n",
      "         14.1905,  9.9183,  7.6900, 13.3873, 10.0274, 11.1016, 11.1096, 14.2465,\n",
      "          9.9905,  9.9538,  8.0820, 10.2022,  8.3476, 14.5312, 10.0301,  9.8277,\n",
      "         11.1055, 13.0468, 10.1984, 13.8970, 12.4733, 15.9091,  9.6693, 12.8944,\n",
      "         12.0285, 12.6465,  8.8125, 12.0799, 11.4479, 15.4975, 13.4902, 12.0365,\n",
      "         12.8488, 11.2734, 15.9010, 20.8865, 11.6251, 10.4088, 12.6689,  6.3754,\n",
      "          8.4912,  9.5106, 10.6047, 16.9994, 10.8723,  9.1513,  9.9055,  9.6638,\n",
      "         13.2179, 11.6824, 12.9731,  9.7589, 13.2303, 14.8017, 13.2419,  9.8861,\n",
      "         10.5850,  9.7630, 11.0724, 13.0715, 10.9860,  7.3302, 16.1189, 10.3962,\n",
      "         12.6753, 12.6350, 12.8293,  9.4522,  8.5482, 13.9486,  9.4442, 10.3765,\n",
      "         12.2836, 14.0739,  9.0640, 15.1933, 12.1727, 12.0374, 14.7255,  4.3288,\n",
      "         11.8763, 18.8460, 15.3832, 16.4796, 11.2975, 14.6263,  7.3208,  9.0640,\n",
      "         10.9089, 11.8866, 12.3041,  8.0160,  9.8521, 11.3571, 12.8992, 10.7206,\n",
      "         11.2513, 11.8829, 10.4451, 12.0067,  9.3198, 10.0241, 11.3475, 10.6200,\n",
      "         10.1118, 14.8628, 19.3527, 13.5960, 13.2465,  9.4074, 11.1947, 16.3392,\n",
      "          7.2121,  8.5291,  8.8955,  6.3712,  8.8680, 11.8948, 10.2880,  8.3618,\n",
      "         10.6216,  8.9795, 10.0260,  8.5700, 15.0932,  8.4455,  9.7156, 11.8956,\n",
      "         13.2357, 10.4310,  7.7769, 14.6184, 16.9852, 11.8125, 13.9045,  9.2857,\n",
      "         10.7192, 11.7766,  9.2977, 11.4480, 13.2633, 10.7559,  4.3045, 12.9147,\n",
      "          4.9909,  9.1834, 11.5634, 12.4962, 12.9775, 12.4401, 15.2505, 10.1633,\n",
      "         16.2824,  5.6385, 10.0989, 10.5927,  9.9266, 11.0970, 14.5478, 10.0026,\n",
      "         14.1375, 11.9004, 14.6336, 10.7327, 10.5641,  8.6381, 10.7589, 15.0806,\n",
      "         12.5130, 14.5039, 14.9238, 10.5679, 15.1038,  9.5589, 15.3297,  9.0532,\n",
      "          9.9929, 13.9506,  9.0012, 12.8287, 13.1927,  9.1541, 10.3965,  8.6304,\n",
      "         12.3104,  6.8359, 10.3579, 12.8265, 14.3715, 11.5191, 15.2868, 10.4014]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[13.0194, 10.8077,  6.9815, 11.6238, 12.4107, 13.2645, 13.7935, 11.9630,\n",
      "         15.3629, 15.0951, 11.4994, 11.1138, 11.6615, 10.7168, 12.1459, 11.9880,\n",
      "         12.9267, 12.7770, 10.8096,  9.3615, 12.5204, 12.7177,  7.1773,  8.1973,\n",
      "         13.2178, 11.4403,  9.5551, 10.1978, 11.4382, 13.4057,  9.2031,  7.7849,\n",
      "         11.6032,  9.3615,  9.1918,  9.8162,  3.3221, 13.1963, 11.9839, 13.8591,\n",
      "         10.9216,  9.9157, 11.4685,  9.8652,  8.8323, 11.8105, 10.3519,  6.9552,\n",
      "          7.7742, 13.2121, 10.5096, 14.7216, 11.0901,  8.1848, 13.2861, 12.6216,\n",
      "         10.5893, 10.7313,  9.8901,  9.4694, 12.2237,  9.3734, 12.2115,  8.0848,\n",
      "          9.6121, 12.0019, 12.0263,  9.2183, 12.3270, 11.1557, 11.5002,  8.6152,\n",
      "         13.3081, 13.2791, 12.0296, 13.7801, 10.2912, 11.0782, 11.2523, 13.8000,\n",
      "          9.4690,  9.5872,  7.9211,  9.8719,  7.6124, 14.1483,  9.5231,  9.3146,\n",
      "         10.8568, 12.3728,  9.9505, 13.7015, 12.0489, 16.4794,  9.2541, 12.8325,\n",
      "         10.5548, 13.0118,  8.4025, 12.0103, 11.9611, 15.4888, 13.9170, 12.5966,\n",
      "         11.8461, 11.0362, 15.3310, 20.8389, 10.3841,  7.6340, 12.6341,  7.7967,\n",
      "          8.5123,  9.7202,  8.8675, 16.5431, 10.8341, 10.2027,  9.8598, 11.3102,\n",
      "         13.9768, 11.7695, 13.3654,  9.0825, 14.0607, 14.2406, 14.6834, 10.2357,\n",
      "         10.1909,  9.1833, 10.9662, 12.9257,  9.7696,  7.7721, 13.5474, 10.6422,\n",
      "         12.4497, 12.5737, 12.7500,  9.3615,  9.7597, 13.2012,  9.4274, 12.3724,\n",
      "         12.0491, 13.1199,  8.7020, 13.9945, 12.5206, 12.0777, 14.1444,  8.4017,\n",
      "         12.1266, 18.3527, 14.0951, 16.5897, 13.0027, 14.7363, 10.5347, 10.7388,\n",
      "         12.3780, 11.6488, 12.6201, 10.3967, 10.0876, 11.0252,  9.7258, 12.6941,\n",
      "         10.8891, 11.7994, 10.3867, 11.7782,  7.6254,  9.6642, 11.3257, 10.5199,\n",
      "          9.8451, 16.1014, 19.4900, 12.6226, 13.3561,  8.3344, 10.7536, 16.3151,\n",
      "          5.8098,  6.8844,  8.5639,  7.8695,  7.3563, 10.9013,  9.3842,  8.2969,\n",
      "         10.5356,  9.6219, 13.0965,  8.7635, 15.1803,  8.8851,  9.7468, 11.2193,\n",
      "         13.2658, 11.1256,  9.4517, 13.7183, 17.5872, 11.5217, 14.5074,  9.3615,\n",
      "         10.5128, 11.2540,  8.9312, 10.9900, 12.5926, 10.5150,  3.7477, 12.8968,\n",
      "          7.8368, 14.8627, 12.6901, 13.2942, 15.1074, 12.5896, 15.0313, 13.3548,\n",
      "         17.1835,  4.6114, 10.0352, 10.7229, 10.2737, 11.6275, 12.5323, 12.0950,\n",
      "         14.2746, 11.9335, 15.0437, 10.8950, 11.2213,  9.3987, 10.6024, 15.7002,\n",
      "         12.4536, 14.0271, 14.6736, 10.8561, 14.9358,  9.2105, 15.1804,  9.7600,\n",
      "          9.8606, 14.0593, 12.0423, 12.5826, 12.2905,  8.2419, 11.8535,  8.2722,\n",
      "         12.0424,  7.0483,  9.8079, 12.1731, 13.9459, 10.3257, 14.3844, 11.0583],\n",
      "        [12.8175, 10.7528,  2.5950, 11.9104, 12.3813, 12.7740, 13.4278, 11.9311,\n",
      "         14.8399, 14.7605, 11.8087, 11.2650, 11.4431, 10.9173, 12.3772, 12.1286,\n",
      "         12.6302, 13.4497, 11.1892,  9.3775, 11.6637, 12.9477,  7.0736,  8.4678,\n",
      "         13.3326, 11.6278,  9.4065,  9.1397, 11.3170, 13.2434,  9.5816,  8.6577,\n",
      "         11.4899,  9.3775,  8.3536,  9.8922,  3.7135, 13.3892, 11.9391, 13.7022,\n",
      "          9.5956, 10.1827, 11.3477, 11.3293,  9.0158, 11.7683,  9.6652, 11.3984,\n",
      "          6.4112, 13.3571, 10.9973, 14.7899, 10.9773,  8.6698, 13.9984, 13.2424,\n",
      "         10.8349, 11.1871,  9.9379,  9.9286, 12.9824,  9.9576, 13.0122,  8.9986,\n",
      "          9.4929, 12.4200, 12.1242, 10.3356, 13.1569, 11.4718, 12.2161,  9.1861,\n",
      "         12.5140, 13.4454, 12.4802, 14.1398, 10.8147, 11.4133, 11.6599, 13.7742,\n",
      "         10.6648, 10.4153,  7.9865, 10.1927,  8.9750, 14.4620,  9.8513,  7.8665,\n",
      "         11.3982, 13.0775, 10.3666, 13.6312, 12.1309, 15.8338,  9.5169, 12.9753,\n",
      "         10.8335, 12.8348,  9.7645, 11.6101, 12.1101, 15.5918, 13.7756, 12.4173,\n",
      "         11.5756, 10.9640, 15.4993, 20.8335, 10.3380,  8.4338, 12.7814,  7.8375,\n",
      "          9.0585,  9.8367, 10.4821, 15.9453, 10.3984, 10.5311,  9.7375, 11.7704,\n",
      "         14.7757, 11.2908, 13.3607,  8.4194, 13.2477, 13.4330, 14.5762, 10.6494,\n",
      "          9.3282,  9.9828, 11.9331, 12.8561,  9.9891,  7.2676, 15.7679, 10.5731,\n",
      "         12.5312, 12.8127, 14.2629,  9.3775,  9.2053, 13.1454,  9.8774, 11.9039,\n",
      "          7.8485, 12.9708,  9.4843, 13.6887, 12.5472, 12.0939, 14.0303,  7.1484,\n",
      "         11.1116, 17.8896, 13.5210, 16.1517, 12.6948, 14.4373,  7.0999,  7.0776,\n",
      "         12.0821, 11.0549, 12.9554,  9.4575,  9.3453, 11.1682,  8.9236, 12.2149,\n",
      "         10.1487, 10.9801, 10.2008, 10.8244,  8.0012,  9.4541, 11.3138, 10.2796,\n",
      "          9.4883, 15.5572, 19.1155, 12.2793, 12.9365,  8.1629, 11.2181, 15.9604,\n",
      "          7.2052,  8.2924,  8.7111,  7.8520,  6.6284, 10.8128,  8.9267,  8.1772,\n",
      "         11.1291,  9.5735, 13.0204,  8.5741, 15.0804,  9.7108,  8.7213, 11.7465,\n",
      "         12.6077, 10.0043,  9.0732, 12.7357, 17.1665, 11.1703, 14.1312,  9.3775,\n",
      "         10.6851, 11.5035,  9.2437, 10.9119, 13.8538, 10.5907,  3.6174, 12.7553,\n",
      "          7.7909, 14.6337, 12.8862, 13.3233, 14.9464, 13.1211, 14.9514, 12.6345,\n",
      "         16.8030,  7.7145,  9.6538, 10.7500, 10.0862, 11.5167, 12.1705, 12.3285,\n",
      "         14.2333, 12.0568, 14.1097, 11.4352, 11.8245,  9.8521, 10.9706, 15.5865,\n",
      "         12.2404, 13.9991, 14.6277, 10.9458, 14.8628,  8.0370, 14.9851,  9.8731,\n",
      "          9.3775, 14.0929, 12.7550, 12.4376, 13.0368,  7.7679, 11.9504,  9.2247,\n",
      "         12.0543,  7.2380,  9.9145, 12.2908, 14.2974, 11.6229, 15.4788, 11.1348]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[12.8738, 10.5669,  7.9207, 10.9931, 12.4814, 13.4089, 13.7577, 12.0196,\n",
      "         15.2343, 15.1532, 12.0990, 11.2794, 12.0962, 10.7117, 12.2706, 11.8159,\n",
      "         12.0767, 13.8370, 11.1740,  9.2432, 12.3999, 12.8172,  7.1070,  8.9895,\n",
      "         13.2867, 12.1359,  9.4417,  9.6150, 10.5479, 13.6688,  8.7916,  7.0911,\n",
      "         11.8474,  7.4965,  8.5572,  9.9138,  7.6576, 13.1662, 11.6813, 13.7893,\n",
      "         10.5272,  9.8181, 11.1438,  8.0860,  9.6513, 11.9151, 11.1904, 10.4411,\n",
      "          8.7852, 12.5621, 10.4971, 14.8548, 11.0186,  8.3733, 13.8412, 12.9449,\n",
      "         10.0311, 11.1719, 10.0750,  9.8867, 12.7018,  9.8259, 12.6477,  9.0526,\n",
      "          9.4777, 12.3576, 12.4261,  9.6669, 12.8203, 10.5414, 11.9712,  9.1605,\n",
      "         14.1463, 13.6356, 12.3474, 14.1407, 10.6720, 11.7120, 11.3739, 14.2142,\n",
      "          9.6211,  9.7334,  8.5896, 10.8004,  8.7443, 13.1130,  9.3530, 10.1557,\n",
      "         11.3124, 12.9977, 10.3170, 13.4108, 12.2019, 16.3678,  9.5246, 13.7707,\n",
      "         11.9811, 13.6179,  9.3984, 12.1250, 11.9326, 15.6463, 13.5784, 12.2220,\n",
      "         12.6542, 10.8698, 15.8059, 20.6985, 10.3434,  9.5756, 12.9905,  8.4902,\n",
      "          9.6010,  9.9524,  9.7282, 16.6924, 10.5393, 10.1112, 10.3245, 10.2248,\n",
      "         13.6628, 11.7202, 13.6263, 10.5312, 14.2969, 14.5851, 13.7441,  9.6155,\n",
      "         10.3584,  9.8449, 11.4008, 12.9689,  8.9216,  7.3013, 14.6802, 10.5153,\n",
      "         12.2863, 12.0959, 12.9262,  9.4452,  9.9293, 13.1451,  9.6284, 10.6378,\n",
      "         11.7311, 13.5175,  9.1236, 14.5959, 12.4416, 11.9058, 14.4136,  8.5124,\n",
      "         12.3920, 18.5528, 14.3255, 15.4649, 13.4639, 14.4357,  9.5067,  9.2264,\n",
      "         11.7835, 12.1974, 12.6683, 10.6131, 10.9769, 11.4575,  9.8634, 12.7658,\n",
      "         11.1634, 11.7452, 11.1083, 12.0585,  8.9197, 10.1788, 11.4629, 10.8758,\n",
      "          9.9870, 16.4350, 19.6005, 12.8009, 13.3678,  7.7241, 11.4306, 16.3079,\n",
      "          7.6141,  8.3945,  7.4063,  7.6537,  7.1852, 10.2202,  9.2706,  8.2999,\n",
      "         11.3577,  9.0099, 12.1534,  8.5657, 14.9766,  9.4590,  9.0299, 11.4326,\n",
      "         13.1228, 10.9962,  9.5912, 13.7741, 17.7735, 11.4662, 14.4239,  9.4401,\n",
      "         10.9488, 11.4428,  8.8774, 11.1708, 13.2077, 10.5293,  3.9493, 13.0342,\n",
      "          7.3280, 14.4241, 11.8984, 13.2981, 14.7093, 12.8032, 14.9474, 13.9381,\n",
      "         17.0005,  7.2609, 10.0826, 10.6985, 10.7088, 11.6680, 12.3991, 12.2286,\n",
      "         14.5661, 11.8390, 14.0223, 10.5858, 10.8458, 10.0482, 11.0914, 15.1448,\n",
      "         12.5781, 14.1366, 14.6803, 10.8724, 14.9431,  8.9011, 15.3028, 10.0230,\n",
      "          9.6057, 14.4658, 10.8494, 12.9907, 12.5650,  8.4417, 11.0759,  9.3253,\n",
      "         12.0588,  6.7109,  9.8564, 12.2743, 14.7969, 10.8146, 15.0206, 11.1448],\n",
      "        [12.7474, 10.4713,  7.8460, 10.8872, 12.3568, 13.2793, 13.6217, 11.8968,\n",
      "         15.0895, 14.9952, 11.9858, 11.1725, 11.9885, 10.6131, 12.1456, 11.7042,\n",
      "         11.9565, 13.7127, 11.0647,  9.1597, 12.2756, 12.6838,  7.0449,  8.9068,\n",
      "         13.1499, 12.0365,  9.3502,  9.5276, 10.4437, 13.5349,  8.7141,  7.0217,\n",
      "         11.7353,  7.4234,  8.4800,  9.8160,  7.5887, 13.0314, 11.5757, 13.6609,\n",
      "         10.4315,  9.7260, 11.0406,  8.0082,  9.5687, 11.8052, 11.0806, 10.3401,\n",
      "          8.7043, 12.4508, 10.3992, 14.7143, 10.9123,  8.2966, 13.6981, 12.8270,\n",
      "          9.9340, 11.0526,  9.9783,  9.7962, 12.5738,  9.7344, 12.5240,  8.9778,\n",
      "          9.3925, 12.2319, 12.3144,  9.5721, 12.6864, 10.4387, 11.8524,  9.0763,\n",
      "         14.0123, 13.5073, 12.2285, 14.0110, 10.5749, 11.5960, 11.2574, 14.0736,\n",
      "          9.5343,  9.6349,  8.5060, 10.6895,  8.6665, 12.9914,  9.2730, 10.0641,\n",
      "         11.2072, 12.8765, 10.2246, 13.2825, 12.0749, 16.2094,  9.4337, 13.6429,\n",
      "         11.8624, 13.4908,  9.3231, 12.0115, 11.8160, 15.4819, 13.4462, 12.0966,\n",
      "         12.5363, 10.7708, 15.6591, 20.4937, 10.2468,  9.4809, 12.8721,  8.4131,\n",
      "          9.5080,  9.8578,  9.6391, 16.5304, 10.4357, 10.0212, 10.2244, 10.1355,\n",
      "         13.5240, 11.6112, 13.4984, 10.4305, 14.1443, 14.4386, 13.6175,  9.5204,\n",
      "         10.2598,  9.7504, 11.2905, 12.8453,  8.8316,  7.2328, 14.5467, 10.4178,\n",
      "         12.1649, 11.9682, 12.8091,  9.3451,  9.8387, 13.0217,  9.5379, 10.5392,\n",
      "         11.6294, 13.3883,  9.0466, 14.4549, 12.3238, 11.7966, 14.2692,  8.4340,\n",
      "         12.2708, 18.3727, 14.1896, 15.3043, 13.3241, 14.2940,  9.4213,  9.1434,\n",
      "         11.6694, 12.0776, 12.5434, 10.5091, 10.8635, 11.3505,  9.7756, 12.6390,\n",
      "         11.0557, 11.6279, 11.0072, 11.9506,  8.8385, 10.0719, 11.3610, 10.7655,\n",
      "          9.8942, 16.2770, 19.4155, 12.6807, 13.2349,  7.6531, 11.3216, 16.1637,\n",
      "          7.5466,  8.3191,  7.3365,  7.5847,  7.1181, 10.1126,  9.1853,  8.2224,\n",
      "         11.2479,  8.9321, 12.0353,  8.4796, 14.8192,  9.3697,  8.9497, 11.3189,\n",
      "         13.0017, 10.8953,  9.5025, 13.6396, 17.6052, 11.3668, 14.2834,  9.3456,\n",
      "         10.8463, 11.3386,  8.7949, 11.0568, 13.0711, 10.4252,  3.9158, 12.9162,\n",
      "          7.2585, 14.2842, 11.7807, 13.1703, 14.5635, 12.6788, 14.7894, 13.8085,\n",
      "         16.8301,  7.1938,  9.9828, 10.5978, 10.6130, 11.5554, 12.2722, 12.1226,\n",
      "         14.4311, 11.7169, 13.8885, 10.4729, 10.7533,  9.9561, 10.9831, 14.9976,\n",
      "         12.4637, 14.0034, 14.5353, 10.7630, 14.8091,  8.8197, 15.1684,  9.9249,\n",
      "          9.5046, 14.3311, 10.7498, 12.8520, 12.4373,  8.3623, 10.9650,  9.2403,\n",
      "         11.9316,  6.6493,  9.7674, 12.1533, 14.6556, 10.7094, 14.8782, 11.0305]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor(0.5858, grad_fn=<MseLossBackward0>)\n",
      "..........................................\n",
      "..........................................\n",
      "------------------------------bwd-1-------------------------------------------------\n",
      "tensor([[11.0275, 10.4368,  5.8126,  ..., 11.5191, 15.2868, 10.4014],\n",
      "        [11.5192, 10.8301,  8.5387,  ..., 11.7374, 15.3666, 10.7942],\n",
      "        [12.5697, 10.2853,  9.2928,  ..., 11.3558, 14.8844, 11.2110],\n",
      "        ...,\n",
      "        [13.5505, 10.3811,  9.5087,  ..., 10.3866, 14.1886, 10.9819],\n",
      "        [13.0194, 10.8077,  6.9815,  ..., 10.3257, 14.3844, 11.0583],\n",
      "        [12.8175, 10.7528,  2.5950,  ..., 11.6229, 15.4788, 11.1348]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[13.8373, 10.4579,  9.3472,  ..., 11.3862, 15.0673, 10.8571],\n",
      "        [11.0275, 10.4368,  5.8126,  ..., 11.5191, 15.2868, 10.4014],\n",
      "        [11.5192, 10.8301,  8.5387,  ..., 11.7374, 15.3666, 10.7942],\n",
      "        ...,\n",
      "        [13.4063, 10.1694,  9.4104,  ..., 10.3181, 14.4028, 11.0296],\n",
      "        [13.5505, 10.3811,  9.5087,  ..., 10.3866, 14.1886, 10.9819],\n",
      "        [13.0194, 10.8077,  6.9815,  ..., 10.3257, 14.3844, 11.0583]])\n",
      "..........................................\n",
      "------------------------------fwd-1-------------------------------------------------\n",
      "..........................................\n",
      "DP25\n",
      "..........................................\n",
      "tensor([[14.2012, 10.4714,  1.5894,  ..., 10.7156, 14.8535, 11.3042],\n",
      "        [11.9520, 10.7644,  6.9420,  ..., 10.8812, 15.2423, 11.6863],\n",
      "        [12.7688, 10.4280,  8.0761,  ..., 10.8963, 15.3917, 11.2177],\n",
      "        ...,\n",
      "        [13.1755, 10.8335,  5.2286,  ..., 11.3597, 15.5292, 11.2652],\n",
      "        [13.1455, 10.4210,  9.3184,  ..., 10.3790, 14.6446, 11.4424],\n",
      "        [12.5261,  9.9668,  8.0178,  ..., 10.7567, 15.0610, 11.4185]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[11.7090, 10.5659,  8.6205,  ..., 11.0181, 15.3969, 11.0751],\n",
      "        [12.6329, 10.3181,  8.5037,  ..., 10.8603, 15.3659, 10.7307],\n",
      "        [11.3390, 10.5974,  5.6269,  ..., 10.8422, 15.2157, 11.1542],\n",
      "        ...,\n",
      "        [12.9100, 10.6148,  7.8511,  ..., 10.7658, 15.4272, 10.9792],\n",
      "        [12.3320, 10.6578,  9.4831,  ..., 10.3737, 15.4607, 11.1412],\n",
      "        [11.1463,  9.3244,  9.2959,  ..., 10.7460, 15.7707,  9.7322]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[12.8214, 10.5270,  7.8896,  ..., 10.7708, 14.9614, 11.0975],\n",
      "        [12.7765, 10.4933,  7.8632,  ..., 10.7336, 14.9110, 11.0569],\n",
      "        [12.7777, 10.4939,  7.8638,  ..., 10.7345, 14.9121, 11.0580],\n",
      "        ...,\n",
      "        [12.7942, 10.5064,  7.8736,  ..., 10.7482, 14.9307, 11.0730],\n",
      "        [12.8177, 10.5243,  7.8875,  ..., 10.7678, 14.9572, 11.0942],\n",
      "        [12.8152, 10.5223,  7.8860,  ..., 10.7657, 14.9544, 11.0919]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor(0.5509, grad_fn=<MseLossBackward0>)\n",
      "..........................................\n",
      "..........................................\n",
      "------------------------------bwd-1-------------------------------------------------\n",
      "tensor([[14.2012, 10.4714,  1.5894,  ..., 10.7156, 14.8535, 11.3042],\n",
      "        [11.9520, 10.7644,  6.9420,  ..., 10.8812, 15.2423, 11.6863],\n",
      "        [12.7688, 10.4280,  8.0761,  ..., 10.8963, 15.3917, 11.2177],\n",
      "        ...,\n",
      "        [11.7716, 10.5197,  8.9436,  ..., 10.7663, 15.2437, 10.9561],\n",
      "        [12.9100, 10.6148,  7.8511,  ..., 10.7658, 15.4272, 10.9792],\n",
      "        [11.1463,  9.3244,  9.2959,  ..., 10.7460, 15.7707,  9.7322]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[12.8729, 10.3357,  7.9372,  ..., 10.6542, 14.9601, 11.3991],\n",
      "        [14.2012, 10.4714,  1.5894,  ..., 10.7156, 14.8535, 11.3042],\n",
      "        [11.9520, 10.7644,  6.9420,  ..., 10.8812, 15.2423, 11.6863],\n",
      "        ...,\n",
      "        [12.7730, 10.1987,  9.2979,  ...,  9.9815, 14.9308, 11.1758],\n",
      "        [11.7716, 10.5197,  8.9436,  ..., 10.7663, 15.2437, 10.9561],\n",
      "        [12.3320, 10.6578,  9.4831,  ..., 10.3737, 15.4607, 11.1412]])\n",
      "..........................................\n",
      "------------------------------fwd-1-------------------------------------------------\n",
      "..........................................\n",
      "DP27\n",
      "..........................................\n",
      "tensor([[12.5259, 10.7749,  9.4218,  ..., 10.5878, 14.9444, 11.0068],\n",
      "        [12.9497, 10.5609,  9.3957,  ..., 10.4140, 14.6376, 11.4257],\n",
      "        [12.7915, 10.4284,  7.1464,  ..., 10.9778, 14.8552, 11.5865],\n",
      "        [11.9754, 10.6559,  5.3779,  ..., 10.2934, 14.8359, 11.4841],\n",
      "        [13.1346, 10.6222,  6.2334,  ..., 10.4771, 14.9373, 11.3749]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[12.6465, 10.2282,  4.7982,  ..., 10.9634, 15.4916, 11.6093],\n",
      "        [10.8018, 10.5082,  3.8491,  ..., 11.1612, 15.5037, 11.1696],\n",
      "        [12.9404, 10.0897,  5.0515,  ..., 11.1785, 15.7229, 11.0660],\n",
      "        [12.6135, 10.2161,  8.0644,  ..., 11.1931, 15.4517, 11.1216],\n",
      "        [11.8674, 10.3775,  6.8104,  ..., 11.0651, 15.2310, 11.2783]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[12.8868, 10.5766,  7.9283,  ..., 10.8253, 15.0350, 11.1566],\n",
      "        [12.8696, 10.5636,  7.9181,  ..., 10.8110, 15.0157, 11.1410],\n",
      "        [12.8469, 10.5463,  7.9047,  ..., 10.7920, 14.9901, 11.1206],\n",
      "        [12.8309, 10.5342,  7.8952,  ..., 10.7787, 14.9720, 11.1060],\n",
      "        [12.8514, 10.5496,  7.9073,  ..., 10.7958, 14.9951, 11.1247]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor(0.4255, grad_fn=<MseLossBackward0>)\n",
      "..........................................\n",
      "..........................................\n",
      "------------------------------bwd-1-------------------------------------------------\n",
      "tensor([[12.9497, 10.5609,  9.3957,  ..., 10.4140, 14.6376, 11.4257],\n",
      "        [12.7915, 10.4284,  7.1464,  ..., 10.9778, 14.8552, 11.5865],\n",
      "        [11.9754, 10.6559,  5.3779,  ..., 10.2934, 14.8359, 11.4841],\n",
      "        ...,\n",
      "        [11.8674, 10.3775,  6.8104,  ..., 11.0651, 15.2310, 11.2783],\n",
      "        [12.1701, 10.1190,  5.5042,  ..., 10.9571, 15.3179, 11.4451],\n",
      "        [12.2547, 10.0446,  8.4060,  ..., 10.7133, 15.0798, 11.2709]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[12.5259, 10.7749,  9.4218,  ..., 10.5878, 14.9444, 11.0068],\n",
      "        [12.9497, 10.5609,  9.3957,  ..., 10.4140, 14.6376, 11.4257],\n",
      "        [12.7915, 10.4284,  7.1464,  ..., 10.9778, 14.8552, 11.5865],\n",
      "        ...,\n",
      "        [12.6135, 10.2161,  8.0644,  ..., 11.1931, 15.4517, 11.1216],\n",
      "        [11.8674, 10.3775,  6.8104,  ..., 11.0651, 15.2310, 11.2783],\n",
      "        [12.1701, 10.1190,  5.5042,  ..., 10.9571, 15.3179, 11.4451]])\n",
      "..........................................\n",
      "------------------------------fwd-1-------------------------------------------------\n",
      "..........................................\n",
      "DP17\n",
      "..........................................\n",
      "tensor([[12.2296, 11.1922,  9.2751,  ..., 11.5183, 15.1146, 11.4945],\n",
      "        [12.0775, 10.9549,  9.0829,  ..., 11.4660, 15.1572, 11.4346],\n",
      "        [12.6696, 11.2246,  8.3345,  ..., 11.5573, 14.2902, 11.2241],\n",
      "        ...,\n",
      "        [12.8682, 10.7235,  8.3535,  ..., 11.0379, 14.7791, 11.0302],\n",
      "        [13.0890, 11.0414,  7.7803,  ..., 10.8051, 14.5250, 10.9366],\n",
      "        [13.4808, 11.3006,  9.3239,  ..., 10.5519, 14.2633, 11.1922]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[12.8931, 10.6979,  9.4071,  ..., 11.0404, 15.1129, 10.9977],\n",
      "        [12.4956, 10.1153,  9.3281,  ..., 11.2039, 15.1868, 11.1539],\n",
      "        [12.5172, 10.2167,  9.3340,  ..., 11.1267, 15.1853, 11.3426],\n",
      "        ...,\n",
      "        [12.2723, 10.2519,  8.7063,  ..., 11.3268, 15.5833, 11.3567],\n",
      "        [11.2914, 10.4476,  4.0042,  ..., 10.9872, 15.1460, 11.2837],\n",
      "        [12.3096, 10.6115,  5.0568,  ..., 11.3866, 15.0250, 11.2073]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[12.8828, 10.5736,  7.9259,  ..., 10.8219, 15.0305, 11.1529],\n",
      "        [12.8860, 10.5761,  7.9278,  ..., 10.8247, 15.0342, 11.1558],\n",
      "        [12.8616, 10.5576,  7.9134,  ..., 10.8043, 15.0067, 11.1337],\n",
      "        ...,\n",
      "        [12.8277, 10.5320,  7.8934,  ..., 10.7762, 14.9686, 11.1031],\n",
      "        [12.8509, 10.5496,  7.9072,  ..., 10.7955, 14.9948, 11.1241],\n",
      "        [12.8944, 10.5826,  7.9328,  ..., 10.8317, 15.0438, 11.1633]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor(0.5600, grad_fn=<MseLossBackward0>)\n",
      "..........................................\n",
      "..........................................\n",
      "------------------------------bwd-1-------------------------------------------------\n",
      "tensor([[12.0775, 10.9549,  9.0829,  ..., 11.4660, 15.1572, 11.4346],\n",
      "        [12.6696, 11.2246,  8.3345,  ..., 11.5573, 14.2902, 11.2241],\n",
      "        [12.9550, 10.4760,  9.3551,  ..., 11.0933, 15.0827, 11.1564],\n",
      "        ...,\n",
      "        [12.2723, 10.2519,  8.7063,  ..., 11.3268, 15.5833, 11.3567],\n",
      "        [11.2914, 10.4476,  4.0042,  ..., 10.9872, 15.1460, 11.2837],\n",
      "        [12.3096, 10.6115,  5.0568,  ..., 11.3866, 15.0250, 11.2073]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[12.2296, 11.1922,  9.2751,  ..., 11.5183, 15.1146, 11.4945],\n",
      "        [12.0775, 10.9549,  9.0829,  ..., 11.4660, 15.1572, 11.4346],\n",
      "        [12.6696, 11.2246,  8.3345,  ..., 11.5573, 14.2902, 11.2241],\n",
      "        ...,\n",
      "        [13.8316, 10.4280,  9.3238,  ..., 11.1550, 15.1054, 10.9888],\n",
      "        [12.2723, 10.2519,  8.7063,  ..., 11.3268, 15.5833, 11.3567],\n",
      "        [11.2914, 10.4476,  4.0042,  ..., 10.9872, 15.1460, 11.2837]])\n",
      "..........................................\n",
      "------------------------------fwd-1-------------------------------------------------\n",
      "..........................................\n",
      "DP08\n",
      "..........................................\n",
      "tensor([[14.3244, 10.6757,  8.3800,  ...,  9.9581, 14.3711, 10.8823],\n",
      "        [13.2055, 10.7469,  6.9717,  ..., 10.4274, 14.7689, 10.9133],\n",
      "        [12.7772, 10.6996,  8.3690,  ..., 10.7999, 15.0753, 10.9116],\n",
      "        ...,\n",
      "        [13.9125, 10.5258,  4.5179,  ...,  9.8736, 14.3713, 10.5925],\n",
      "        [12.8827, 10.4929,  5.9931,  ..., 10.5885, 14.8660, 10.9054],\n",
      "        [13.0911, 10.7676,  8.6265,  ..., 10.6164, 15.0759, 11.1004]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[13.0376, 10.5025,  8.1121,  ..., 10.2958, 14.7770, 11.4057],\n",
      "        [12.3527, 10.6499,  9.2517,  ..., 10.5707, 14.9038, 10.8201],\n",
      "        [11.8935, 10.3180,  4.2700,  ...,  9.6126, 14.8748, 10.9619],\n",
      "        ...,\n",
      "        [12.8217, 10.1198,  8.0957,  ..., 10.8881, 15.2801, 11.2058],\n",
      "        [13.1224, 10.4685,  9.3390,  ..., 10.6392, 15.0651, 11.1486],\n",
      "        [13.1968, 10.4331,  8.5728,  ..., 10.7454, 15.1403, 10.9935]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[12.8677, 10.5623,  7.9170,  ..., 10.8095, 15.0137, 11.1393],\n",
      "        [12.8639, 10.5595,  7.9148,  ..., 10.8063, 15.0094, 11.1358],\n",
      "        [12.8360, 10.5384,  7.8983,  ..., 10.7831, 14.9780, 11.1106],\n",
      "        ...,\n",
      "        [12.8324, 10.5355,  7.8962,  ..., 10.7801, 14.9739, 11.1074],\n",
      "        [12.8182, 10.5247,  7.8878,  ..., 10.7682, 14.9578, 11.0945],\n",
      "        [12.8635, 10.5591,  7.9145,  ..., 10.8059, 15.0089, 11.1354]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor(0.3486, grad_fn=<MseLossBackward0>)\n",
      "..........................................\n",
      "..........................................\n",
      "------------------------------bwd-1-------------------------------------------------\n",
      "tensor([[12.5069, 11.1168,  9.3540,  ...,  9.8396, 14.6386, 10.9830],\n",
      "        [12.5810, 10.6850,  9.3418,  ..., 10.4834, 14.9181, 10.8806],\n",
      "        [12.5487, 10.5647,  7.0602,  ..., 10.8816, 14.9374, 10.8664],\n",
      "        ...,\n",
      "        [12.8217, 10.1198,  8.0957,  ..., 10.8881, 15.2801, 11.2058],\n",
      "        [13.1224, 10.4685,  9.3390,  ..., 10.6392, 15.0651, 11.1486],\n",
      "        [13.1968, 10.4331,  8.5728,  ..., 10.7454, 15.1403, 10.9935]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[12.7345, 10.5938,  9.3461,  ..., 10.1987, 14.8202, 11.0630],\n",
      "        [12.5069, 11.1168,  9.3540,  ...,  9.8396, 14.6386, 10.9830],\n",
      "        [12.5810, 10.6850,  9.3418,  ..., 10.4834, 14.9181, 10.8806],\n",
      "        ...,\n",
      "        [13.4870, 10.2000,  5.3035,  ..., 10.9966, 15.3932, 11.2997],\n",
      "        [12.8217, 10.1198,  8.0957,  ..., 10.8881, 15.2801, 11.2058],\n",
      "        [13.1224, 10.4685,  9.3390,  ..., 10.6392, 15.0651, 11.1486]])\n",
      "..........................................\n",
      "------------------------------fwd-1-------------------------------------------------\n",
      "..........................................\n",
      "DP04\n",
      "..........................................\n",
      "tensor([[13.1175, 11.6782,  9.3753,  ..., 10.9211, 14.9894, 10.7498],\n",
      "        [12.5627, 11.7049,  9.4379,  ..., 10.9256, 15.1229, 11.2062],\n",
      "        [13.7955, 11.9714,  9.3746,  ..., 11.0650, 14.7634, 11.1094],\n",
      "        ...,\n",
      "        [12.5305, 11.6333,  6.4926,  ..., 11.1398, 15.2844, 11.1633],\n",
      "        [13.0425, 10.3296,  9.3798,  ..., 10.7429, 14.9375, 11.3456],\n",
      "        [14.5591, 11.5893,  7.1946,  ..., 10.9012, 14.9266, 10.7247]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[13.1756, 12.2755,  7.4962,  ..., 10.4541, 14.8253, 11.0784],\n",
      "        [14.1937, 12.3853,  8.6343,  ..., 10.8730, 14.9056, 10.9327],\n",
      "        [13.6931, 12.4450,  7.9038,  ..., 11.0708, 15.0904, 11.0713],\n",
      "        ...,\n",
      "        [13.3302, 12.3060,  6.1358,  ..., 10.9076, 14.7728, 10.9550],\n",
      "        [13.5716, 12.3362,  9.4531,  ..., 10.6438, 14.5846, 10.7489],\n",
      "        [13.6461, 12.1902,  9.4105,  ..., 10.5526, 14.7192, 11.0933]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[12.8455, 10.5456,  7.9040,  ..., 10.7910, 14.9887, 11.1192],\n",
      "        [12.8189, 10.5253,  7.8882,  ..., 10.7688, 14.9587, 11.0952],\n",
      "        [12.8398, 10.5412,  7.9006,  ..., 10.7863, 14.9822, 11.1140],\n",
      "        ...,\n",
      "        [12.7967, 10.5085,  7.8751,  ..., 10.7504, 14.9337, 11.0751],\n",
      "        [12.8297, 10.5336,  7.8946,  ..., 10.7779, 14.9709, 11.1049],\n",
      "        [12.7431, 10.4675,  7.8434,  ..., 10.7056, 14.8731, 11.0269]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor(0.3809, grad_fn=<MseLossBackward0>)\n",
      "..........................................\n",
      "..........................................\n",
      "------------------------------bwd-1-------------------------------------------------\n",
      "tensor([[12.5627, 11.7049,  9.4379,  ..., 10.9256, 15.1229, 11.2062],\n",
      "        [13.7955, 11.9714,  9.3746,  ..., 11.0650, 14.7634, 11.1094],\n",
      "        [13.9835, 11.9322,  9.3678,  ..., 11.1461, 14.9480, 11.3309],\n",
      "        ...,\n",
      "        [11.2888, 11.9361,  8.1820,  ..., 11.1903, 15.1497, 11.1394],\n",
      "        [13.5716, 12.3362,  9.4531,  ..., 10.6438, 14.5846, 10.7489],\n",
      "        [13.6461, 12.1902,  9.4105,  ..., 10.5526, 14.7192, 11.0933]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[13.1175, 11.6782,  9.3753,  ..., 10.9211, 14.9894, 10.7498],\n",
      "        [12.5627, 11.7049,  9.4379,  ..., 10.9256, 15.1229, 11.2062],\n",
      "        [13.7955, 11.9714,  9.3746,  ..., 11.0650, 14.7634, 11.1094],\n",
      "        ...,\n",
      "        [12.9874, 12.3331,  9.2798,  ..., 11.0112, 14.7328, 11.0332],\n",
      "        [13.3302, 12.3060,  6.1358,  ..., 10.9076, 14.7728, 10.9550],\n",
      "        [13.5716, 12.3362,  9.4531,  ..., 10.6438, 14.5846, 10.7489]])\n",
      "..........................................\n",
      "------------------------------fwd-1-------------------------------------------------\n",
      "..........................................\n",
      "DP15\n",
      "..........................................\n",
      "tensor([[11.0200, 10.9666,  9.3050,  ..., 11.7241, 15.4042, 11.6388],\n",
      "        [12.5657, 10.8886,  9.3481,  ..., 11.7229, 15.4046, 11.5374],\n",
      "        [13.3016, 10.9788,  9.3040,  ..., 11.6235, 15.0186, 11.0825],\n",
      "        [12.1830, 10.9308,  8.1221,  ..., 11.5471, 15.1533, 11.3403]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[12.6961, 11.2276,  8.3552,  ..., 11.5893, 15.2622, 11.5044],\n",
      "        [11.7869, 11.2056,  3.3632,  ..., 11.3504, 15.1405, 10.9418],\n",
      "        [11.6178, 10.8852,  6.9783,  ..., 11.0186, 14.9528, 11.2286],\n",
      "        [12.9329, 10.8056,  7.5574,  ..., 11.4453, 15.1752, 11.4885]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[12.9109, 10.5951,  7.9426,  ..., 10.8455, 15.0624, 11.1782],\n",
      "        [12.8736, 10.5666,  7.9205,  ..., 10.8143, 15.0202, 11.1446],\n",
      "        [12.8662, 10.5609,  7.9161,  ..., 10.8081, 15.0118, 11.1380],\n",
      "        [12.8786, 10.5704,  7.9234,  ..., 10.8184, 15.0258, 11.1491]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor(0.4899, grad_fn=<MseLossBackward0>)\n",
      "..........................................\n",
      "..........................................\n",
      "------------------------------bwd-1-------------------------------------------------\n",
      "tensor([[12.5657, 10.8886,  9.3481,  ..., 11.7229, 15.4046, 11.5374],\n",
      "        [13.3016, 10.9788,  9.3040,  ..., 11.6235, 15.0186, 11.0825],\n",
      "        [12.1830, 10.9308,  8.1221,  ..., 11.5471, 15.1533, 11.3403],\n",
      "        ...,\n",
      "        [11.7869, 11.2056,  3.3632,  ..., 11.3504, 15.1405, 10.9418],\n",
      "        [11.6178, 10.8852,  6.9783,  ..., 11.0186, 14.9528, 11.2286],\n",
      "        [12.9329, 10.8056,  7.5574,  ..., 11.4453, 15.1752, 11.4885]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[11.0200, 10.9666,  9.3050,  ..., 11.7241, 15.4042, 11.6388],\n",
      "        [12.5657, 10.8886,  9.3481,  ..., 11.7229, 15.4046, 11.5374],\n",
      "        [13.3016, 10.9788,  9.3040,  ..., 11.6235, 15.0186, 11.0825],\n",
      "        ...,\n",
      "        [12.6961, 11.2276,  8.3552,  ..., 11.5893, 15.2622, 11.5044],\n",
      "        [11.7869, 11.2056,  3.3632,  ..., 11.3504, 15.1405, 10.9418],\n",
      "        [11.6178, 10.8852,  6.9783,  ..., 11.0186, 14.9528, 11.2286]])\n",
      "..........................................\n",
      "------------------------------fwd-1-------------------------------------------------\n",
      "..........................................\n",
      "DP12\n",
      "..........................................\n",
      "tensor([[13.5040, 10.0282,  7.7011,  ..., 10.4387, 14.9915, 11.3239],\n",
      "        [12.8491, 10.6833,  7.0299,  ..., 11.5325, 15.3381, 10.8623],\n",
      "        [12.9665, 10.2930,  9.0163,  ..., 11.0411, 15.2409, 10.8485],\n",
      "        [13.5989, 10.0937,  6.5610,  ..., 11.1122, 15.1441, 11.0510],\n",
      "        [13.4209, 10.3531,  9.2291,  ..., 10.8507, 14.9389, 10.7886]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[11.9260, 10.9802,  6.7734,  ..., 11.4688, 15.3315, 10.9773],\n",
      "        [11.8029, 10.5065,  4.0661,  ..., 11.1269, 15.4660, 10.9528],\n",
      "        [11.9111, 10.3760,  9.3303,  ..., 11.0867, 14.9746, 11.1236],\n",
      "        [13.0407, 10.0516,  9.3957,  ..., 10.6512, 15.2894, 11.3156],\n",
      "        [13.2118, 10.2298,  8.6522,  ..., 10.9495, 15.3664, 11.0274]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[12.8122, 10.5199,  7.8841,  ..., 10.7631, 14.9509, 11.0892],\n",
      "        [12.8318, 10.5348,  7.8957,  ..., 10.7794, 14.9730, 11.1069],\n",
      "        [12.8675, 10.5621,  7.9169,  ..., 10.8093, 15.0134, 11.1391],\n",
      "        [12.8764, 10.5686,  7.9221,  ..., 10.8166, 15.0233, 11.1472],\n",
      "        [12.8541, 10.5518,  7.9089,  ..., 10.7981, 14.9982, 11.1271]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor(0.6765, grad_fn=<MseLossBackward0>)\n",
      "..........................................\n",
      "..........................................\n",
      "------------------------------bwd-1-------------------------------------------------\n",
      "tensor([[12.8491, 10.6833,  7.0299,  ..., 11.5325, 15.3381, 10.8623],\n",
      "        [12.9665, 10.2930,  9.0163,  ..., 11.0411, 15.2409, 10.8485],\n",
      "        [13.5989, 10.0937,  6.5610,  ..., 11.1122, 15.1441, 11.0510],\n",
      "        ...,\n",
      "        [13.3785, 10.0848,  9.3903,  ..., 10.6442, 15.2033, 10.9425],\n",
      "        [13.2566, 10.8380,  7.4013,  ..., 10.7594, 14.9645, 11.0001],\n",
      "        [13.1875, 10.7263,  7.2419,  ..., 11.0029, 15.2338, 11.0410]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[13.5040, 10.0282,  7.7011,  ..., 10.4387, 14.9915, 11.3239],\n",
      "        [12.8491, 10.6833,  7.0299,  ..., 11.5325, 15.3381, 10.8623],\n",
      "        [12.9665, 10.2930,  9.0163,  ..., 11.0411, 15.2409, 10.8485],\n",
      "        ...,\n",
      "        [14.1441,  9.9061,  6.9579,  ..., 10.9197, 15.0871, 10.4507],\n",
      "        [13.3785, 10.0848,  9.3903,  ..., 10.6442, 15.2033, 10.9425],\n",
      "        [13.2566, 10.8380,  7.4013,  ..., 10.7594, 14.9645, 11.0001]])\n",
      "..........................................\n",
      "------------------------------fwd-1-------------------------------------------------\n",
      "------------------------------bwd-1-------------------------------------------------\n",
      "tensor([[12.2188, 10.6867,  9.2221,  ..., 11.6866, 15.6232, 11.4978],\n",
      "        [12.2536, 10.7269,  9.3445,  ..., 11.3575, 15.3160, 11.3222],\n",
      "        [13.8551, 10.2111,  9.4264,  ..., 11.1201, 14.6990, 11.0405],\n",
      "        ...,\n",
      "        [13.1981, 10.3945,  9.4150,  ..., 10.5647, 14.4630, 11.4176],\n",
      "        [13.1213, 10.7974,  8.2288,  ..., 10.9996, 14.8914, 11.0878],\n",
      "        [13.4485, 10.5149,  7.7922,  ..., 11.1199, 14.8397, 11.1489]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[13.0540, 10.4288,  8.4170,  ..., 11.4229, 15.3243, 11.4153],\n",
      "        [12.2188, 10.6867,  9.2221,  ..., 11.6866, 15.6232, 11.4978],\n",
      "        [12.2536, 10.7269,  9.3445,  ..., 11.3575, 15.3160, 11.3222],\n",
      "        ...,\n",
      "        [12.8160, 10.2657,  9.1820,  ..., 10.5175, 14.7099, 11.0981],\n",
      "        [13.1981, 10.3945,  9.4150,  ..., 10.5647, 14.4630, 11.4176],\n",
      "        [13.1213, 10.7974,  8.2288,  ..., 10.9996, 14.8914, 11.0878]])\n",
      "..........................................\n",
      "------------------------------fwd-1-------------------------------------------------\n",
      "------------------------------bwd-1-------------------------------------------------\n",
      "tensor([[13.6829, 10.8895,  9.4438,  ..., 10.8457, 14.7022, 11.4988],\n",
      "        [14.0289, 10.7303,  8.5845,  ..., 10.1384, 14.0577, 11.6051],\n",
      "        [13.1310, 11.0404,  7.9971,  ...,  9.9803, 13.9429, 11.5332],\n",
      "        ...,\n",
      "        [12.5687, 10.6631,  8.1078,  ..., 10.3081, 14.0110, 11.0805],\n",
      "        [12.0994, 10.6436,  8.2567,  ..., 10.3936, 13.9623, 10.9732],\n",
      "        [12.5357, 10.3427,  9.3201,  ..., 10.5930, 14.4335, 11.0320]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[12.8451, 11.0070,  9.3787,  ...,  8.9188, 13.5992, 10.9288],\n",
      "        [13.6829, 10.8895,  9.4438,  ..., 10.8457, 14.7022, 11.4988],\n",
      "        [14.0289, 10.7303,  8.5845,  ..., 10.1384, 14.0577, 11.6051],\n",
      "        ...,\n",
      "        [12.9128, 10.5920,  8.4763,  ..., 10.6788, 14.6122, 11.2690],\n",
      "        [12.5687, 10.6631,  8.1078,  ..., 10.3081, 14.0110, 11.0805],\n",
      "        [12.0994, 10.6436,  8.2567,  ..., 10.3936, 13.9623, 10.9732]])\n",
      "..........................................\n",
      "------------------------------fwd-1-------------------------------------------------\n",
      "..........................................\n",
      "DP01\n",
      "..........................................\n",
      "tensor([[12.1872, 10.0754,  9.3155,  ..., 10.6352, 14.2632, 11.1500],\n",
      "        [12.6439,  9.8714,  7.5239,  ..., 10.6109, 14.0019, 10.9247],\n",
      "        [13.6329, 10.0258,  4.4312,  ..., 10.8480, 14.1785, 10.7053],\n",
      "        ...,\n",
      "        [11.4168, 10.2999,  9.2688,  ..., 10.9211, 14.6811, 11.1139],\n",
      "        [12.0527, 10.4883,  9.2734,  ..., 10.9450, 14.6648, 11.0817],\n",
      "        [12.0232, 10.5771,  7.0086,  ..., 10.9853, 15.0267, 11.0356]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[13.6524,  9.9955,  9.2899,  ..., 10.9696, 14.9112, 11.0957],\n",
      "        [13.3442,  9.9307,  9.2716,  ..., 10.9977, 15.1133, 11.2733],\n",
      "        [11.9520, 10.2932,  7.9940,  ..., 11.3780, 14.7866, 11.0890],\n",
      "        ...,\n",
      "        [13.0165, 10.5462,  9.3245,  ..., 11.2547, 14.7118, 10.8268],\n",
      "        [13.9594, 10.2975,  6.2032,  ..., 10.9504, 14.6609, 10.9312],\n",
      "        [13.7202, 10.1946,  7.2218,  ..., 10.9651, 14.5020, 10.9481]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[12.8736, 10.5663,  7.9204,  ..., 10.8141, 15.0200, 11.1447],\n",
      "        [12.8410, 10.5416,  7.9011,  ..., 10.7870, 14.9833, 11.1153],\n",
      "        [12.7980, 10.5092,  7.8758,  ..., 10.7513, 14.9350, 11.0764],\n",
      "        ...,\n",
      "        [12.8038, 10.5137,  7.8793,  ..., 10.7562, 14.9416, 11.0816],\n",
      "        [12.8121, 10.5198,  7.8841,  ..., 10.7630, 14.9508, 11.0891],\n",
      "        [12.7527, 10.4753,  7.8492,  ..., 10.7139, 14.8842, 11.0354]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor(0.3384, grad_fn=<MseLossBackward0>)\n",
      "..........................................\n",
      "..........................................\n",
      "------------------------------bwd-1-------------------------------------------------\n",
      "tensor([[12.6439,  9.8714,  7.5239,  ..., 10.6109, 14.0019, 10.9247],\n",
      "        [13.6329, 10.0258,  4.4312,  ..., 10.8480, 14.1785, 10.7053],\n",
      "        [12.1259, 10.2535,  5.2508,  ..., 10.5715, 14.0422, 11.1556],\n",
      "        ...,\n",
      "        [13.0165, 10.5462,  9.3245,  ..., 11.2547, 14.7118, 10.8268],\n",
      "        [13.9594, 10.2975,  6.2032,  ..., 10.9504, 14.6609, 10.9312],\n",
      "        [13.7202, 10.1946,  7.2218,  ..., 10.9651, 14.5020, 10.9481]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[12.1872, 10.0754,  9.3155,  ..., 10.6352, 14.2632, 11.1500],\n",
      "        [12.6439,  9.8714,  7.5239,  ..., 10.6109, 14.0019, 10.9247],\n",
      "        [13.6329, 10.0258,  4.4312,  ..., 10.8480, 14.1785, 10.7053],\n",
      "        ...,\n",
      "        [12.4435, 10.4521,  9.3436,  ..., 11.2826, 14.7172, 11.0945],\n",
      "        [13.0165, 10.5462,  9.3245,  ..., 11.2547, 14.7118, 10.8268],\n",
      "        [13.9594, 10.2975,  6.2032,  ..., 10.9504, 14.6609, 10.9312]])\n",
      "..........................................\n",
      "------------------------------fwd-1-------------------------------------------------\n",
      "..........................................\n",
      "DP26\n",
      "..........................................\n",
      "tensor([[12.9175, 10.5043,  4.3111,  ...,  9.6683, 13.8857, 11.2675],\n",
      "        [13.2028, 10.3769,  5.9704,  ..., 10.3898, 14.4059, 11.0792],\n",
      "        [13.1255, 10.5638,  7.0701,  ..., 10.7253, 14.6276, 11.0828],\n",
      "        [14.3563, 10.5679,  7.8777,  ..., 10.0684, 14.0232, 11.1250],\n",
      "        [13.9615, 10.7091,  9.3225,  ..., 10.8333, 14.8265, 10.7974]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[13.2443, 10.6146,  5.1295,  ..., 10.7393, 15.5495, 10.5221],\n",
      "        [15.0052, 10.5296,  7.4992,  ..., 10.6462, 15.5867,  9.4680],\n",
      "        [11.5491, 10.7051,  5.1357,  ..., 10.6578, 15.3956, 11.2025],\n",
      "        [12.4446, 10.9196,  8.6488,  ..., 11.2642, 15.7411, 10.9127],\n",
      "        [13.4261, 10.4748,  9.0587,  ..., 10.9032, 15.2676, 10.8164]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[12.8498, 10.5490,  7.9066,  ..., 10.7947, 14.9937, 11.1230],\n",
      "        [12.8813, 10.5727,  7.9251,  ..., 10.8208, 15.0290, 11.1515],\n",
      "        [12.8717, 10.5653,  7.9194,  ..., 10.8128, 15.0181, 11.1429],\n",
      "        [12.8371, 10.5390,  7.8989,  ..., 10.7840, 14.9791, 11.1117],\n",
      "        [12.8500, 10.5489,  7.9066,  ..., 10.7948, 14.9937, 11.1233]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor(0.5092, grad_fn=<MseLossBackward0>)\n",
      "..........................................\n",
      "..........................................\n",
      "------------------------------bwd-1-------------------------------------------------\n",
      "tensor([[13.2028, 10.3769,  5.9704,  ..., 10.3898, 14.4059, 11.0792],\n",
      "        [13.1255, 10.5638,  7.0701,  ..., 10.7253, 14.6276, 11.0828],\n",
      "        [13.9615, 10.7091,  9.3225,  ..., 10.8333, 14.8265, 10.7974],\n",
      "        ...,\n",
      "        [12.9693, 10.4763,  8.7876,  ..., 10.1192, 14.9480, 11.5080],\n",
      "        [12.4446, 10.9196,  8.6488,  ..., 11.2642, 15.7411, 10.9127],\n",
      "        [13.4261, 10.4748,  9.0587,  ..., 10.9032, 15.2676, 10.8164]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[12.9175, 10.5043,  4.3111,  ...,  9.6683, 13.8857, 11.2675],\n",
      "        [13.2028, 10.3769,  5.9704,  ..., 10.3898, 14.4059, 11.0792],\n",
      "        [14.3563, 10.5679,  7.8777,  ..., 10.0684, 14.0232, 11.1250],\n",
      "        ...,\n",
      "        [11.5491, 10.7051,  5.1357,  ..., 10.6578, 15.3956, 11.2025],\n",
      "        [12.9693, 10.4763,  8.7876,  ..., 10.1192, 14.9480, 11.5080],\n",
      "        [12.4446, 10.9196,  8.6488,  ..., 11.2642, 15.7411, 10.9127]])\n",
      "..........................................\n",
      "------------------------------fwd-1-------------------------------------------------\n",
      "..........................................\n",
      "DP24\n",
      "..........................................\n",
      "tensor([[12.7060, 10.6568,  3.8799,  ..., 10.7987, 14.8348, 10.4446],\n",
      "        [12.6921, 11.2039,  6.1744,  ..., 11.3160, 15.0523, 11.2579],\n",
      "        [11.8525, 10.6833,  3.7928,  ..., 10.9345, 14.9403, 11.5350],\n",
      "        ...,\n",
      "        [12.6508, 10.4176,  9.3279,  ..., 10.4885, 13.9564, 11.6463],\n",
      "        [11.9134, 10.8638,  3.8821,  ..., 10.4950, 14.4562, 11.2409],\n",
      "        [13.5484, 10.4151,  6.2130,  ..., 10.5294, 14.4985, 11.5807]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[12.4908, 10.8465,  8.8995,  ..., 10.9449, 14.8107, 11.3185],\n",
      "        [13.2676, 10.5263,  3.0148,  ..., 10.8359, 14.1706, 11.1250],\n",
      "        [11.6993, 10.6271,  9.3059,  ..., 10.7473, 14.5172, 11.4676],\n",
      "        ...,\n",
      "        [12.0893, 10.7194,  7.0179,  ..., 10.8892, 14.8987, 11.2650],\n",
      "        [14.7529, 10.4199,  4.8835,  ..., 10.3822, 13.8857, 11.0852],\n",
      "        [13.8523, 10.3885,  9.3643,  ..., 11.1756, 15.0450, 11.4206]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[12.8129, 10.5201,  7.8845,  ..., 10.7635, 14.9515, 11.0901],\n",
      "        [12.8664, 10.5610,  7.9162,  ..., 10.8082, 15.0120, 11.1382],\n",
      "        [12.7724, 10.4896,  7.8606,  ..., 10.7299, 14.9060, 11.0534],\n",
      "        ...,\n",
      "        [12.8054, 10.5150,  7.8802,  ..., 10.7576, 14.9434, 11.0830],\n",
      "        [12.8675, 10.5620,  7.9168,  ..., 10.8092, 15.0133, 11.1391],\n",
      "        [12.8505, 10.5492,  7.9069,  ..., 10.7951, 14.9942, 11.1238]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor(0.4984, grad_fn=<MseLossBackward0>)\n",
      "..........................................\n",
      "..........................................\n",
      "------------------------------bwd-1-------------------------------------------------\n",
      "tensor([[12.6921, 11.2039,  6.1744,  ..., 11.3160, 15.0523, 11.2579],\n",
      "        [11.8525, 10.6833,  3.7928,  ..., 10.9345, 14.9403, 11.5350],\n",
      "        [11.2121, 11.4433,  7.5675,  ..., 11.2451, 14.6255, 11.2611],\n",
      "        ...,\n",
      "        [12.0893, 10.7194,  7.0179,  ..., 10.8892, 14.8987, 11.2650],\n",
      "        [14.7529, 10.4199,  4.8835,  ..., 10.3822, 13.8857, 11.0852],\n",
      "        [13.8523, 10.3885,  9.3643,  ..., 11.1756, 15.0450, 11.4206]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[12.7060, 10.6568,  3.8799,  ..., 10.7987, 14.8348, 10.4446],\n",
      "        [12.6921, 11.2039,  6.1744,  ..., 11.3160, 15.0523, 11.2579],\n",
      "        [11.8525, 10.6833,  3.7928,  ..., 10.9345, 14.9403, 11.5350],\n",
      "        ...,\n",
      "        [12.9487, 10.3940,  7.8414,  ..., 10.6304, 14.1996, 11.2529],\n",
      "        [12.0893, 10.7194,  7.0179,  ..., 10.8892, 14.8987, 11.2650],\n",
      "        [14.7529, 10.4199,  4.8835,  ..., 10.3822, 13.8857, 11.0852]])\n",
      "..........................................\n",
      "------------------------------fwd-1-------------------------------------------------\n",
      "..........................................\n",
      "DP03\n",
      "..........................................\n",
      "tensor([[11.5843,  9.7151,  9.2488,  ..., 11.2651, 14.8906, 10.9321],\n",
      "        [11.8337, 10.5256,  9.2356,  ..., 11.1712, 14.8084, 11.3213],\n",
      "        [13.0301,  9.4959,  9.2850,  ..., 11.1300, 14.8476, 10.9778],\n",
      "        ...,\n",
      "        [13.2162,  9.7016,  9.2580,  ..., 11.5564, 15.5385, 11.0703],\n",
      "        [13.0244,  9.7967,  9.3379,  ..., 11.3363, 15.5453, 11.2620],\n",
      "        [12.4813,  9.7647,  9.4115,  ..., 11.5498, 15.7364, 11.3499]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[12.7007,  9.5920,  9.3485,  ..., 11.3790, 15.2239, 11.0304],\n",
      "        [13.8006, 11.8828,  9.2783,  ..., 11.6580, 15.3869, 11.1928],\n",
      "        [12.6349, 10.7402,  9.3095,  ..., 11.4296, 15.5745, 11.3772],\n",
      "        ...,\n",
      "        [12.1063,  9.9985,  9.1368,  ..., 11.5436, 15.7111, 11.2806],\n",
      "        [12.1742,  9.8117,  9.4542,  ..., 11.4778, 15.5943, 11.4804],\n",
      "        [12.2909,  9.9130,  7.2522,  ..., 11.4746, 15.5539, 11.2898]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[12.7477, 10.4713,  7.8462,  ..., 10.7096, 14.8784, 11.0309],\n",
      "        [12.8228, 10.5282,  7.8905,  ..., 10.7720, 14.9630, 11.0987],\n",
      "        [12.8035, 10.5135,  7.8791,  ..., 10.7560, 14.9412, 11.0813],\n",
      "        ...,\n",
      "        [12.7757, 10.4926,  7.8627,  ..., 10.7329, 14.9100, 11.0562],\n",
      "        [12.8082, 10.5172,  7.8819,  ..., 10.7599, 14.9466, 11.0855],\n",
      "        [12.8325, 10.5355,  7.8962,  ..., 10.7801, 14.9739, 11.1074]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor(0.5535, grad_fn=<MseLossBackward0>)\n",
      "..........................................\n",
      "..........................................\n",
      "------------------------------bwd-1-------------------------------------------------\n",
      "tensor([[13.0301,  9.4959,  9.2850,  ..., 11.1300, 14.8476, 10.9778],\n",
      "        [ 9.5683,  9.7467,  9.3007,  ..., 11.4421, 15.0671, 11.4888],\n",
      "        [11.5942,  9.7597,  9.2875,  ..., 11.5877, 15.4326, 11.4224],\n",
      "        ...,\n",
      "        [12.1063,  9.9985,  9.1368,  ..., 11.5436, 15.7111, 11.2806],\n",
      "        [12.1742,  9.8117,  9.4542,  ..., 11.4778, 15.5943, 11.4804],\n",
      "        [12.2909,  9.9130,  7.2522,  ..., 11.4746, 15.5539, 11.2898]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[11.8337, 10.5256,  9.2356,  ..., 11.1712, 14.8084, 11.3213],\n",
      "        [13.0301,  9.4959,  9.2850,  ..., 11.1300, 14.8476, 10.9778],\n",
      "        [ 9.5683,  9.7467,  9.3007,  ..., 11.4421, 15.0671, 11.4888],\n",
      "        ...,\n",
      "        [11.5303, 10.0547,  7.5466,  ..., 11.4870, 15.7548, 11.6232],\n",
      "        [12.1063,  9.9985,  9.1368,  ..., 11.5436, 15.7111, 11.2806],\n",
      "        [12.1742,  9.8117,  9.4542,  ..., 11.4778, 15.5943, 11.4804]])\n",
      "..........................................\n",
      "------------------------------fwd-1-------------------------------------------------\n",
      "..........................................\n",
      "DP05\n",
      "..........................................\n",
      "tensor([[11.7257, 10.6205,  9.3448,  ...,  8.9484, 14.0264, 11.0433],\n",
      "        [11.4969, 10.1749,  6.5644,  ..., 10.1094, 14.5023, 10.9351],\n",
      "        [11.7905, 10.1228,  7.7901,  ..., 10.0822, 14.6124, 11.0985],\n",
      "        ...,\n",
      "        [13.4304, 10.4668,  9.3048,  ...,  9.7255, 14.4404, 11.0096],\n",
      "        [13.4231, 10.5670,  7.4092,  ..., 10.2770, 14.6353, 11.0278],\n",
      "        [12.4143, 10.4221,  9.3384,  ..., 10.0313, 14.5299, 10.7569]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[13.5884, 10.5649,  9.4120,  ...,  9.7042, 14.7014, 11.2221],\n",
      "        [13.2964, 10.3342,  9.3020,  ...,  9.8342, 14.1961, 11.2919],\n",
      "        [13.4772,  9.8792,  4.2824,  ...,  9.6340, 14.5651, 11.0591],\n",
      "        ...,\n",
      "        [12.8732, 10.9362,  4.8888,  ..., 10.6372, 14.3444, 11.1701],\n",
      "        [13.5696,  9.9946,  6.5958,  ..., 10.0642, 14.1148, 11.3068],\n",
      "        [11.8543, 10.4707,  9.3676,  ..., 10.1530, 14.9569, 11.0474]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[12.8729, 10.5660,  7.9200,  ..., 10.8137, 15.0193, 11.1441],\n",
      "        [12.8135, 10.5211,  7.8850,  ..., 10.7643, 14.9525, 11.0904],\n",
      "        [12.8283, 10.5324,  7.8938,  ..., 10.7767, 14.9692, 11.1036],\n",
      "        ...,\n",
      "        [12.8458, 10.5457,  7.9041,  ..., 10.7912, 14.9890, 11.1194],\n",
      "        [12.8291, 10.5330,  7.8943,  ..., 10.7774, 14.9702, 11.1044],\n",
      "        [12.8503, 10.5491,  7.9067,  ..., 10.7950, 14.9940, 11.1235]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor(0.6806, grad_fn=<MseLossBackward0>)\n",
      "..........................................\n",
      "..........................................\n",
      "------------------------------bwd-1-------------------------------------------------\n",
      "tensor([[11.4969, 10.1749,  6.5644,  ..., 10.1094, 14.5023, 10.9351],\n",
      "        [11.7905, 10.1228,  7.7901,  ..., 10.0822, 14.6124, 11.0985],\n",
      "        [11.8552, 10.2367,  9.3299,  ..., 10.2876, 14.5071, 11.3246],\n",
      "        ...,\n",
      "        [12.8732, 10.9362,  4.8888,  ..., 10.6372, 14.3444, 11.1701],\n",
      "        [13.5696,  9.9946,  6.5958,  ..., 10.0642, 14.1148, 11.3068],\n",
      "        [11.8543, 10.4707,  9.3676,  ..., 10.1530, 14.9569, 11.0474]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[11.7257, 10.6205,  9.3448,  ...,  8.9484, 14.0264, 11.0433],\n",
      "        [11.4969, 10.1749,  6.5644,  ..., 10.1094, 14.5023, 10.9351],\n",
      "        [13.1054,  9.9187,  9.4108,  ..., 10.7127, 15.2963, 10.8783],\n",
      "        ...,\n",
      "        [13.4615, 10.4465,  4.6816,  ..., 10.0494, 14.1796, 11.5570],\n",
      "        [12.8732, 10.9362,  4.8888,  ..., 10.6372, 14.3444, 11.1701],\n",
      "        [13.5696,  9.9946,  6.5958,  ..., 10.0642, 14.1148, 11.3068]])\n",
      "..........................................\n",
      "------------------------------fwd-1-------------------------------------------------\n",
      "..........................................\n",
      "DP29\n",
      "..........................................\n",
      "tensor([[13.9230, 10.4369,  7.0732,  ...,  9.3902, 13.7056, 11.6806],\n",
      "        [11.8308, 10.5532,  7.0860,  ..., 10.3905, 14.4454, 11.5946],\n",
      "        [12.2325, 10.3899,  7.9130,  ..., 10.3504, 14.7519, 11.4311],\n",
      "        [12.0195, 10.4186,  8.0452,  ..., 10.4530, 14.7261, 11.7266],\n",
      "        [12.5411, 10.4332,  9.0404,  ..., 10.6302, 14.7261, 11.2640]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[13.9356, 10.4057,  7.4867,  ..., 10.8016, 14.4949, 10.7094],\n",
      "        [12.7803, 10.1177,  7.9848,  ..., 10.3812, 14.8443, 11.0668],\n",
      "        [12.8872, 10.6883,  7.9749,  ..., 10.4049, 14.9082, 11.0207],\n",
      "        [12.8547, 10.3859,  5.3638,  ..., 11.0499, 15.0197, 10.9356],\n",
      "        [13.0846, 10.4307,  3.1784,  ...,  9.6947, 14.0399, 10.9959]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[12.8528, 10.5505,  7.9081,  ..., 10.7968, 14.9965, 11.1260],\n",
      "        [12.8796, 10.5712,  7.9240,  ..., 10.8193, 15.0269, 11.1500],\n",
      "        [12.8502, 10.5488,  7.9066,  ..., 10.7948, 14.9938, 11.1236],\n",
      "        [12.8245, 10.5294,  7.8915,  ..., 10.7734, 14.9649, 11.1003],\n",
      "        [12.7901, 10.5033,  7.8711,  ..., 10.7448, 14.9261, 11.0692]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor(0.5739, grad_fn=<MseLossBackward0>)\n",
      "..........................................\n",
      "..........................................\n",
      "------------------------------bwd-1-------------------------------------------------\n",
      "tensor([[11.8308, 10.5532,  7.0860,  ..., 10.3905, 14.4454, 11.5946],\n",
      "        [12.0195, 10.4186,  8.0452,  ..., 10.4530, 14.7261, 11.7266],\n",
      "        [12.5411, 10.4332,  9.0404,  ..., 10.6302, 14.7261, 11.2640],\n",
      "        ...,\n",
      "        [12.8872, 10.6883,  7.9749,  ..., 10.4049, 14.9082, 11.0207],\n",
      "        [12.8547, 10.3859,  5.3638,  ..., 11.0499, 15.0197, 10.9356],\n",
      "        [13.0846, 10.4307,  3.1784,  ...,  9.6947, 14.0399, 10.9959]])\n",
      "..........................................\n",
      "..........................................\n",
      "tensor([[13.9230, 10.4369,  7.0732,  ...,  9.3902, 13.7056, 11.6806],\n",
      "        [12.2325, 10.3899,  7.9130,  ..., 10.3504, 14.7519, 11.4311],\n",
      "        [12.0195, 10.4186,  8.0452,  ..., 10.4530, 14.7261, 11.7266],\n",
      "        ...,\n",
      "        [12.6687, 10.4122,  7.8580,  ..., 10.6507, 14.9838, 11.0196],\n",
      "        [12.8872, 10.6883,  7.9749,  ..., 10.4049, 14.9082, 11.0207],\n",
      "        [12.8547, 10.3859,  5.3638,  ..., 11.0499, 15.0197, 10.9356]])\n",
      "..........................................\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "max_step = 1\n",
    "sample_id = 'Subject ID'\n",
    "time_id = 'Gestational age (GA)/weeks'\n",
    "\n",
    "loss_fwd = 0\n",
    "loss_bwd = 0\n",
    "loss_rev_cons = 0\n",
    "loss_tem_cons = 0\n",
    "total_loss = 0\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "for i, batch in enumerate(dataloader):\n",
    "    inputs = batch['input_data']  \n",
    "    subject_ids = batch['sample_id'] \n",
    "    row_idxs = batch['row_ids']\n",
    "    time_ids = batch['time_ids']\n",
    "\n",
    "    \n",
    "    for i, sample_input in enumerate(inputs):\n",
    "\n",
    "\n",
    "        for step in list(range(1,max_step+1,1)):\n",
    "    \n",
    "            # ----------- Train Forward Prediction\n",
    "\n",
    "            target_rows, target_indices, target_time_indices, comparable_booleans = get_dynamic_targets(pregnancy_df, feature_list, sample_id, [subject_ids[i]], time_id, [time_ids[i]], fwd=20)\n",
    "            print(f'------------------------------fwd-{step}-------------------------------------------------')\n",
    "\n",
    "            filtered_sample_inputs = sample_input[comparable_booleans]\n",
    "            \n",
    "            if len(filtered_sample_inputs) > 0:\n",
    "                print('..........................................')\n",
    "                print(subject_ids[i])\n",
    "                print('..........................................')\n",
    "                print(filtered_sample_inputs)\n",
    "                \n",
    "                print('..........................................')\n",
    "                sample_output = [tensor.clone() for tensor in filtered_sample_inputs]\n",
    "                print('..........................................')\n",
    "                target = target_rows[0][comparable_booleans]\n",
    "                print(target_rows[0][comparable_booleans])\n",
    "                print('..........................................')\n",
    "                print('..........................................')\n",
    "                bwd_o, fwd_o = LinKoopAE_model.predict(filtered_sample_inputs, fwd=20)\n",
    "                print(fwd_o[0])\n",
    "                print(criterion(fwd_o[0], target))\n",
    "                print('..........................................')\n",
    "                print('..........................................')\n",
    "\n",
    "            else: \n",
    "                break\n",
    "                \n",
    "        for step in list(range(1,max_step+1,1)):\n",
    "            # ----------- Train Backward Prediction\n",
    "            target_rows, target_indices, target_time_indices, comparable_booleans = get_dynamic_targets(pregnancy_df, feature_list, sample_id, [subject_ids[i]], time_id, [time_ids[i]], bwd=step)\n",
    "\n",
    "            print(f'------------------------------bwd-{step}-------------------------------------------------')\n",
    "\n",
    "\n",
    "            filtered_sample_inputs = sample_input[comparable_booleans]\n",
    "\n",
    "            if len(filtered_sample_inputs) > 0:\n",
    "            \n",
    "                print(filtered_sample_inputs)\n",
    "                print('..........................................')\n",
    "                sample_output = [tensor.clone() for tensor in filtered_sample_inputs]\n",
    "                print('..........................................')\n",
    "                print(target_rows[0][comparable_booleans])\n",
    "                print('..........................................')\n",
    "\n",
    "            else: \n",
    "                break\n",
    "\n",
    "             \n",
    "            \n",
    "    \n",
    "\n",
    "    if i == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "50ab8e72-3a81-44e9-9a95-9cfaf75e004a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "\n",
    "def lr_scheduler(optimizer, epoch, lr_decay_rate=0.8, decayEpochs=[]):\n",
    "        \"\"\"Decay learning rate by a factor of lr_decay_rate every lr_decay_epoch epochs\"\"\"\n",
    "        if epoch in decayEpochs:\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] *= lr_decay_rate\n",
    "            return optimizer\n",
    "        else:\n",
    "            return optimizer\n",
    "\n",
    "# Example training loop with the Identity model\n",
    "def train(model, dataloader, lr, learning_rate_change, decayEpochs=[40, 80, 120, 160], num_epochs=10,  max_Kstep=2, weight_decay=0.01):\n",
    "    model.train()\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    criterion = nn.MSELoss()\n",
    "             \n",
    "\n",
    "    for epoch in range(num_epochs+1):\n",
    "        print(f'----------Training epoch--------')\n",
    "        print(f'----------------{epoch}---------------')\n",
    "        print('')\n",
    "    \n",
    "        for i, batch in enumerate(dataloader):\n",
    "            inputs = batch['input_data']  \n",
    "            subject_ids = batch['sample_id'] \n",
    "            row_idxs = batch['row_ids']\n",
    "            time_ids = batch['time_ids']\n",
    "            \n",
    "            loss_fwd_total = 0\n",
    "            loss_bwd_total = 0\n",
    "            \n",
    "            loss_inv_cons_total = 0\n",
    "            \n",
    "            loss_temp_cons_total = 0\n",
    "            \n",
    "            # Loop over each sample in the batch\n",
    "            for i, sample_input in enumerate(inputs):\n",
    "                \n",
    "                # ------------------- Forward prediction ------------------\n",
    "    \n",
    "                for step in range(1, max_Kstep+1):\n",
    "                    \n",
    "                    # Get dynamic forward targets\n",
    "                    target_rows, target_indices, target_time_indices, comparable_booleans = get_dynamic_targets(\n",
    "                        pregnancy_df, feature_list, sample_id, [subject_ids[i]], time_id, [time_ids[i]], fwd=max_Kstep\n",
    "                    )\n",
    "                    \n",
    "                    filtered_sample_inputs = sample_input[comparable_booleans]\n",
    "                    \n",
    "                    if len(filtered_sample_inputs) > 0:\n",
    "                        bwd_output, fwd_output = model(filtered_sample_inputs, fwd=step) \n",
    "                        \n",
    "\n",
    "                        target = target_rows[0][comparable_booleans]\n",
    "                        \n",
    "                        \n",
    "                        # Compute loss\n",
    "                        loss_fwd = criterion(fwd_output[step-1], target)\n",
    "                        loss_fwd_total += loss_fwd\n",
    "                        \n",
    "                        if step > 1: # Calculate fwd temporary consistency loss\n",
    "                            target_id_tensor = torch.tensor(target_indices)\n",
    "                            past_mask = torch.isin(past_fwd_prediction_ids, target_id_tensor).squeeze()\n",
    "                            current_mask = torch.isin(target_id_tensor, past_fwd_prediction_ids).squeeze()\n",
    "                            \n",
    "                            loss_fwd_temp_cons = criterion(fwd_output[step-1][current_mask], past_fwd_prediction[past_mask])\n",
    "                            loss_temp_cons_total += loss_fwd_temp_cons\n",
    "                        \n",
    "                        past_fwd_prediction = fwd_output[step-1]\n",
    "                        past_fwd_prediction_ids = torch.tensor(target_indices)\n",
    "                    else:\n",
    "                        break\n",
    "    \n",
    "                        \n",
    "                # ------------------- Backward prediction ------------------\n",
    "                for step in range(1, max_Kstep+1):\n",
    "                    \n",
    "                    # Get dynamic forward targets\n",
    "                    target_rows, target_indices, target_time_indices, comparable_booleans = get_dynamic_targets(\n",
    "                        pregnancy_df, feature_list, sample_id, [subject_ids[i]], time_id, [time_ids[i]], bwd=max_Kstep\n",
    "                    )\n",
    "                    \n",
    "                    filtered_sample_inputs = sample_input[comparable_booleans]\n",
    "                    \n",
    "                    if len(filtered_sample_inputs) > 0:\n",
    "                        bwd_output, fwd_output = model(filtered_sample_inputs, bwd=step)  # Model returns input\n",
    "                        target = target_rows[0][comparable_booleans]\n",
    "                        \n",
    "                        # Compute loss\n",
    "                        loss_bwd = criterion(bwd_output[step-1], target)\n",
    "                        loss_bwd_total += loss_bwd\n",
    "    \n",
    "                        \n",
    "                        if step > 1: # Calculate bwd temporary consistency loss\n",
    "                            target_id_tensor = torch.tensor(target_indices)\n",
    "                            past_mask = torch.isin(past_bwd_prediction_ids, target_id_tensor).squeeze()\n",
    "                            current_mask = torch.isin(target_id_tensor, past_bwd_prediction_ids).squeeze()\n",
    "                            \n",
    "                            \n",
    "                            loss_bwd_temp_cons = criterion(bwd_output[step-1][current_mask], past_bwd_prediction[past_mask])\n",
    "                            loss_temp_cons_total += loss_bwd_temp_cons\n",
    "    \n",
    "                        past_bwd_prediction = bwd_output[step-1]\n",
    "                        past_bwd_prediction_ids = torch.tensor(target_indices)\n",
    "                    else:\n",
    "                        break\n",
    "                        \n",
    "                # ------------------- Inverse Consistency Calculation ------------------\n",
    "                    B, F = model.Kmatrix()\n",
    "\n",
    "                    K = F.shape[-1]\n",
    "    \n",
    "                    for k in range(1,K+1):\n",
    "                        Fs1 = F[:,:k]\n",
    "                        Bs1 = B[:k,:]\n",
    "                        Fs2 = F[:k,:]\n",
    "                        Bs2 = B[:,:k]\n",
    "    \n",
    "                        Ik = torch.eye(k).float()#.to(device)\n",
    "    \n",
    "                        loss_inv_cons = (torch.sum((torch.matmul(Bs1, Fs1) - Ik)**2) + \\\n",
    "                                             torch.sum((torch.matmul(Fs2, Bs2) - Ik)**2) ) / (2.0*k)\n",
    "                        loss_inv_cons_total += loss_inv_cons\n",
    "    \n",
    "\n",
    "            # ------------------ TOTAL Batch Loss Calculation ---------------------\n",
    "            loss_fwd_total_avg = loss_fwd_total/len(inputs)\n",
    "            loss_bwd_total_avg = loss_bwd_total/len(inputs)\n",
    "            \n",
    "            loss_inv_cons_total_avg = loss_inv_cons_total/len(inputs)\n",
    "            \n",
    "            loss_temp_cons_total_avg = loss_temp_cons_total/len(inputs)\n",
    "        \n",
    "            loss_total = loss_fwd_total_avg + loss_bwd_total_avg + loss_inv_cons_total_avg + loss_temp_cons_total_avg\n",
    "            print(f'Total Loss: {loss_total}')\n",
    "            print('')\n",
    "            print(f'Total Fwd Loss: {loss_fwd_total_avg}')\n",
    "            print(f'Total Bwd Loss: {loss_bwd_total_avg}')\n",
    "            print(f'Total Inv Consistency Loss: {loss_inv_cons_total_avg}')\n",
    "            print(f'Total Temp Consistency Loss: {loss_temp_cons_total_avg}')\n",
    "        \n",
    "            # ================ Backward Propagation =================================\n",
    "            optimizer.zero_grad()\n",
    "            loss_total.backward()\n",
    "            #torch.nn.utils.clip_grad_norm_(model.parameters(), gradclip) # gradient clip\n",
    "            optimizer.step()\n",
    "            \n",
    "        # schedule learning rate decay    \n",
    "        optimizer = lr_scheduler(optimizer, epoch, lr_decay_rate=learning_rate_change, decayEpochs=decayEpochs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "47e6857c-dd7c-4571-ac7a-1087720c28c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'koopomics.model.koopmanANN' from '/Users/daviddornig/Documents/Master_Thesis/Bioinf/Code/philipp-trinh/KOOPOMICS/koopomics/model/koopmanANN.py'>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(ko)\n",
    "importlib.reload(em)\n",
    "importlib.reload(op)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b6a8fdf-5ece-4864-a754-fb82e4d4e22c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12.6370, 10.2126,  8.5770,  ..., 11.0107, 14.6941, 11.2297],\n",
       "        [16.6716,  9.8858,  4.1114,  ..., 10.6120, 14.2646, 10.9489],\n",
       "        [13.1236, 10.4111,  6.3277,  ..., 10.6507, 14.5418, 10.9686],\n",
       "        ...,\n",
       "        [14.0273, 10.2935,  7.1470,  ..., 10.4400, 14.3452, 10.8084],\n",
       "        [12.5750, 10.0688,  9.2009,  ..., 10.2089, 13.9985, 11.1384],\n",
       "        [13.4534, 10.1143,  8.7439,  ..., 10.6204, 14.5793, 11.3174]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_sample_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8e4ad8fe-b720-4430-801f-fc591c87f3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6342, 0.6485, 0.3385,  ..., 0.6440, 0.9955, 0.2985],\n",
      "        [0.2639, 0.3725, 0.5048,  ..., 0.3336, 0.0653, 0.2688],\n",
      "        [0.5126, 0.9072, 0.4557,  ..., 0.5046, 0.1143, 0.8962],\n",
      "        ...,\n",
      "        [0.5570, 0.7552, 0.7077,  ..., 0.9012, 0.8539, 0.8481],\n",
      "        [0.2826, 0.8940, 0.0260,  ..., 0.7536, 0.1571, 0.1324],\n",
      "        [0.6548, 0.5269, 0.4565,  ..., 0.8929, 0.9102, 0.1670]])\n"
     ]
    }
   ],
   "source": [
    "bwdm, fwdm = LinKoopAE_model.Kmatrix()\n",
    "print(fwdm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "da178666-ad7e-4f1b-816a-ba82eb3716e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([],\n",
       " [tensor([[ 0.0218,  0.5354, -0.0854,  ...,  0.5530,  0.6673,  0.3704],\n",
       "          [ 0.0221,  0.5422, -0.0863,  ...,  0.5583,  0.6739,  0.3759],\n",
       "          [ 0.0217,  0.5369, -0.0857,  ...,  0.5544,  0.6694,  0.3717],\n",
       "          ...,\n",
       "          [ 0.0217,  0.5342, -0.0854,  ...,  0.5523,  0.6669,  0.3702],\n",
       "          [ 0.0223,  0.5243, -0.0844,  ...,  0.5438,  0.6570,  0.3626],\n",
       "          [ 0.0219,  0.5398, -0.0856,  ...,  0.5561,  0.6723,  0.3737]],\n",
       "         grad_fn=<AddmmBackward0>)])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LinKoopAE_model.predict(filtered_sample_inputs, fwd=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a154a3f9-f309-4ab2-b141-855a6e14db35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FF_AE\n",
      "----------Training epoch--------\n",
      "----------------0---------------\n",
      "\n",
      "Total Loss: 1297991.75\n",
      "\n",
      "Total Fwd Loss: 2083.7421875\n",
      "Total Bwd Loss: 2074.73681640625\n",
      "Total Inv Consistency Loss: 1293833.25\n",
      "Total Temp Consistency Loss: 0.0005003109108656645\n",
      "Total Loss: 212308967424.0\n",
      "\n",
      "Total Fwd Loss: 159801917440.0\n",
      "Total Bwd Loss: 3532527616.0\n",
      "Total Inv Consistency Loss: 1300954.75\n",
      "Total Temp Consistency Loss: 48973238272.0\n",
      "Total Loss: 1282637.0\n",
      "\n",
      "Total Fwd Loss: 2061.059326171875\n",
      "Total Bwd Loss: 2078.19580078125\n",
      "Total Inv Consistency Loss: 1278497.75\n",
      "Total Temp Consistency Loss: 0.022574659436941147\n",
      "Total Loss: 1265576.25\n",
      "\n",
      "Total Fwd Loss: 2064.48046875\n",
      "Total Bwd Loss: 2076.25537109375\n",
      "Total Inv Consistency Loss: 1261435.5\n",
      "Total Temp Consistency Loss: 0.002978746546432376\n",
      "Total Loss: 1251786.625\n",
      "\n",
      "Total Fwd Loss: 2073.986572265625\n",
      "Total Bwd Loss: 2082.56298828125\n",
      "Total Inv Consistency Loss: 1247630.125\n",
      "Total Temp Consistency Loss: 0.03756790608167648\n",
      "Total Loss: 1240195.5\n",
      "\n",
      "Total Fwd Loss: 2068.69970703125\n",
      "Total Bwd Loss: 2057.49853515625\n",
      "Total Inv Consistency Loss: 1236068.625\n",
      "Total Temp Consistency Loss: 0.56737220287323\n",
      "----------Training epoch--------\n",
      "----------------1---------------\n",
      "\n",
      "Total Loss: 1230220.875\n",
      "\n",
      "Total Fwd Loss: 2012.98046875\n",
      "Total Bwd Loss: 2017.4456787109375\n",
      "Total Inv Consistency Loss: 1226185.875\n",
      "Total Temp Consistency Loss: 4.566912651062012\n",
      "Total Loss: 1221284.375\n",
      "\n",
      "Total Fwd Loss: 1813.8294677734375\n",
      "Total Bwd Loss: 1868.251708984375\n",
      "Total Inv Consistency Loss: 1217586.25\n",
      "Total Temp Consistency Loss: 15.939153671264648\n",
      "Total Loss: 1211637.5\n",
      "\n",
      "Total Fwd Loss: 527.157958984375\n",
      "Total Bwd Loss: 1088.056640625\n",
      "Total Inv Consistency Loss: 1210017.125\n",
      "Total Temp Consistency Loss: 5.081994533538818\n",
      "Total Loss: 1206638.875\n",
      "\n",
      "Total Fwd Loss: 1639.6138916015625\n",
      "Total Bwd Loss: 1624.6025390625\n",
      "Total Inv Consistency Loss: 1203321.75\n",
      "Total Temp Consistency Loss: 52.893165588378906\n",
      "Total Loss: 1201049.25\n",
      "\n",
      "Total Fwd Loss: 1830.4759521484375\n",
      "Total Bwd Loss: 1819.9515380859375\n",
      "Total Inv Consistency Loss: 1197331.75\n",
      "Total Temp Consistency Loss: 67.15187072753906\n",
      "Total Loss: 1195758.0\n",
      "\n",
      "Total Fwd Loss: 1885.9583740234375\n",
      "Total Bwd Loss: 1851.093994140625\n",
      "Total Inv Consistency Loss: 1191962.25\n",
      "Total Temp Consistency Loss: 58.718284606933594\n",
      "----------Training epoch--------\n",
      "----------------2---------------\n",
      "\n",
      "Total Loss: 1190850.375\n",
      "\n",
      "Total Fwd Loss: 1864.4215087890625\n",
      "Total Bwd Loss: 1838.8499755859375\n",
      "Total Inv Consistency Loss: 1187111.875\n",
      "Total Temp Consistency Loss: 35.20701217651367\n",
      "Total Loss: 1186252.625\n",
      "\n",
      "Total Fwd Loss: 1776.968994140625\n",
      "Total Bwd Loss: 1731.301513671875\n",
      "Total Inv Consistency Loss: 1182728.375\n",
      "Total Temp Consistency Loss: 15.958602905273438\n",
      "Total Loss: 1180666.0\n",
      "\n",
      "Total Fwd Loss: 807.0711669921875\n",
      "Total Bwd Loss: 1119.1673583984375\n",
      "Total Inv Consistency Loss: 1178731.0\n",
      "Total Temp Consistency Loss: 8.799341201782227\n",
      "Total Loss: 2413796.5\n",
      "\n",
      "Total Fwd Loss: 1037391.125\n",
      "Total Bwd Loss: 105048.7890625\n",
      "Total Inv Consistency Loss: 1175091.0\n",
      "Total Temp Consistency Loss: 96265.625\n",
      "Total Loss: 1174228.625\n",
      "\n",
      "Total Fwd Loss: 1174.656982421875\n",
      "Total Bwd Loss: 1283.8533935546875\n",
      "Total Inv Consistency Loss: 1171741.75\n",
      "Total Temp Consistency Loss: 28.42317771911621\n",
      "Total Loss: 1172603.125\n",
      "\n",
      "Total Fwd Loss: 1988.270263671875\n",
      "Total Bwd Loss: 1920.397216796875\n",
      "Total Inv Consistency Loss: 1168694.0\n",
      "Total Temp Consistency Loss: 0.5394156575202942\n",
      "----------Training epoch--------\n",
      "----------------3---------------\n",
      "\n",
      "Total Loss: 1169820.375\n",
      "\n",
      "Total Fwd Loss: 1964.726806640625\n",
      "Total Bwd Loss: 1976.108642578125\n",
      "Total Inv Consistency Loss: 1165875.125\n",
      "Total Temp Consistency Loss: 4.379538536071777\n",
      "Total Loss: 1167176.5\n",
      "\n",
      "Total Fwd Loss: 1935.716796875\n",
      "Total Bwd Loss: 1948.6048583984375\n",
      "Total Inv Consistency Loss: 1163286.125\n",
      "Total Temp Consistency Loss: 6.0046868324279785\n",
      "Total Loss: 1164604.125\n",
      "\n",
      "Total Fwd Loss: 1840.399658203125\n",
      "Total Bwd Loss: 1858.4505615234375\n",
      "Total Inv Consistency Loss: 1160901.875\n",
      "Total Temp Consistency Loss: 3.3508403301239014\n",
      "Total Loss: 1161242.625\n",
      "\n",
      "Total Fwd Loss: 1258.5753173828125\n",
      "Total Bwd Loss: 1293.06005859375\n",
      "Total Inv Consistency Loss: 1158690.75\n",
      "Total Temp Consistency Loss: 0.2016722410917282\n",
      "Total Loss: 1166474.25\n",
      "\n",
      "Total Fwd Loss: 4880.43994140625\n",
      "Total Bwd Loss: 4425.9443359375\n",
      "Total Inv Consistency Loss: 1156641.25\n",
      "Total Temp Consistency Loss: 526.5631103515625\n",
      "Total Loss: 1230317.0\n",
      "\n",
      "Total Fwd Loss: 35249.15625\n",
      "Total Bwd Loss: 35262.25\n",
      "Total Inv Consistency Loss: 1154731.75\n",
      "Total Temp Consistency Loss: 5073.853515625\n",
      "----------Training epoch--------\n",
      "----------------4---------------\n",
      "\n",
      "Total Loss: 1155042.5\n",
      "\n",
      "Total Fwd Loss: 883.2150268554688\n",
      "Total Bwd Loss: 1136.71240234375\n",
      "Total Inv Consistency Loss: 1152969.0\n",
      "Total Temp Consistency Loss: 53.66569900512695\n",
      "Total Loss: 1154396.5\n",
      "\n",
      "Total Fwd Loss: 1503.716552734375\n",
      "Total Bwd Loss: 1578.4158935546875\n",
      "Total Inv Consistency Loss: 1151314.0\n",
      "Total Temp Consistency Loss: 0.41051769256591797\n",
      "Total Loss: 1153538.375\n",
      "\n",
      "Total Fwd Loss: 1873.48828125\n",
      "Total Bwd Loss: 1897.378173828125\n",
      "Total Inv Consistency Loss: 1149767.5\n",
      "Total Temp Consistency Loss: 0.036598894745111465\n",
      "Total Loss: 1152383.125\n",
      "\n",
      "Total Fwd Loss: 2020.846435546875\n",
      "Total Bwd Loss: 2036.013671875\n",
      "Total Inv Consistency Loss: 1148326.125\n",
      "Total Temp Consistency Loss: 0.065719835460186\n",
      "Total Loss: 1151071.875\n",
      "\n",
      "Total Fwd Loss: 2051.58740234375\n",
      "Total Bwd Loss: 2052.77685546875\n",
      "Total Inv Consistency Loss: 1146967.375\n",
      "Total Temp Consistency Loss: 0.06927202641963959\n",
      "Total Loss: 1149852.25\n",
      "\n",
      "Total Fwd Loss: 2078.423828125\n",
      "Total Bwd Loss: 2082.398681640625\n",
      "Total Inv Consistency Loss: 1145691.25\n",
      "Total Temp Consistency Loss: 0.07684780657291412\n",
      "----------Training epoch--------\n",
      "----------------5---------------\n",
      "\n",
      "Total Loss: 1148713.875\n",
      "\n",
      "Total Fwd Loss: 2109.584228515625\n",
      "Total Bwd Loss: 2108.91748046875\n",
      "Total Inv Consistency Loss: 1144495.25\n",
      "Total Temp Consistency Loss: 0.08991806209087372\n",
      "Total Loss: 1147557.625\n",
      "\n",
      "Total Fwd Loss: 2099.974853515625\n",
      "Total Bwd Loss: 2099.801513671875\n",
      "Total Inv Consistency Loss: 1143357.75\n",
      "Total Temp Consistency Loss: 0.10426972806453705\n",
      "Total Loss: 1146478.75\n",
      "\n",
      "Total Fwd Loss: 2104.254638671875\n",
      "Total Bwd Loss: 2088.00927734375\n",
      "Total Inv Consistency Loss: 1142286.375\n",
      "Total Temp Consistency Loss: 0.11800005286931992\n",
      "Total Loss: 1145470.125\n",
      "\n",
      "Total Fwd Loss: 2089.88623046875\n",
      "Total Bwd Loss: 2113.19189453125\n",
      "Total Inv Consistency Loss: 1141266.875\n",
      "Total Temp Consistency Loss: 0.1304127424955368\n",
      "Total Loss: 1144484.25\n",
      "\n",
      "Total Fwd Loss: 2090.321044921875\n",
      "Total Bwd Loss: 2088.91259765625\n",
      "Total Inv Consistency Loss: 1140304.875\n",
      "Total Temp Consistency Loss: 0.13633635640144348\n",
      "Total Loss: 1143574.5\n",
      "\n",
      "Total Fwd Loss: 2083.65283203125\n",
      "Total Bwd Loss: 2101.684326171875\n",
      "Total Inv Consistency Loss: 1139389.0\n",
      "Total Temp Consistency Loss: 0.13867494463920593\n",
      "----------Training epoch--------\n",
      "----------------6---------------\n",
      "\n",
      "Total Loss: 1142711.625\n",
      "\n",
      "Total Fwd Loss: 2105.413818359375\n",
      "Total Bwd Loss: 2102.311767578125\n",
      "Total Inv Consistency Loss: 1138503.75\n",
      "Total Temp Consistency Loss: 0.13246779143810272\n",
      "Total Loss: 1141855.875\n",
      "\n",
      "Total Fwd Loss: 2103.810791015625\n",
      "Total Bwd Loss: 2077.99755859375\n",
      "Total Inv Consistency Loss: 1137674.0\n",
      "Total Temp Consistency Loss: 0.12012815475463867\n",
      "Total Loss: 1141028.25\n",
      "\n",
      "Total Fwd Loss: 2082.10888671875\n",
      "Total Bwd Loss: 2081.222412109375\n",
      "Total Inv Consistency Loss: 1136864.75\n",
      "Total Temp Consistency Loss: 0.1053779125213623\n",
      "Total Loss: 1140271.875\n",
      "\n",
      "Total Fwd Loss: 2075.73876953125\n",
      "Total Bwd Loss: 2099.80712890625\n",
      "Total Inv Consistency Loss: 1136096.25\n",
      "Total Temp Consistency Loss: 0.09367746859788895\n",
      "Total Loss: 1139498.625\n",
      "\n",
      "Total Fwd Loss: 2064.296875\n",
      "Total Bwd Loss: 2088.724365234375\n",
      "Total Inv Consistency Loss: 1135345.5\n",
      "Total Temp Consistency Loss: 0.09179852157831192\n",
      "Total Loss: 1138775.5\n",
      "\n",
      "Total Fwd Loss: 2072.361572265625\n",
      "Total Bwd Loss: 2071.901611328125\n",
      "Total Inv Consistency Loss: 1134631.125\n",
      "Total Temp Consistency Loss: 0.10178355127573013\n",
      "----------Training epoch--------\n",
      "----------------7---------------\n",
      "\n",
      "Total Loss: 1138105.75\n",
      "\n",
      "Total Fwd Loss: 2091.88232421875\n",
      "Total Bwd Loss: 2081.14404296875\n",
      "Total Inv Consistency Loss: 1133932.625\n",
      "Total Temp Consistency Loss: 0.1489635407924652\n",
      "Total Loss: 1137387.375\n",
      "\n",
      "Total Fwd Loss: 2047.3382568359375\n",
      "Total Bwd Loss: 2073.853271484375\n",
      "Total Inv Consistency Loss: 1133265.875\n",
      "Total Temp Consistency Loss: 0.26038917899131775\n",
      "Total Loss: 1136689.875\n",
      "\n",
      "Total Fwd Loss: 2051.97802734375\n",
      "Total Bwd Loss: 2023.222412109375\n",
      "Total Inv Consistency Loss: 1132614.125\n",
      "Total Temp Consistency Loss: 0.5173190832138062\n",
      "Total Loss: 1136066.75\n",
      "\n",
      "Total Fwd Loss: 2030.9730224609375\n",
      "Total Bwd Loss: 2051.928466796875\n",
      "Total Inv Consistency Loss: 1131982.875\n",
      "Total Temp Consistency Loss: 0.9749987721443176\n",
      "Total Loss: 1135424.0\n",
      "\n",
      "Total Fwd Loss: 2025.884765625\n",
      "Total Bwd Loss: 2037.6353759765625\n",
      "Total Inv Consistency Loss: 1131358.625\n",
      "Total Temp Consistency Loss: 1.8809982538223267\n",
      "Total Loss: 1134757.125\n",
      "\n",
      "Total Fwd Loss: 1976.9921875\n",
      "Total Bwd Loss: 2017.2738037109375\n",
      "Total Inv Consistency Loss: 1130759.25\n",
      "Total Temp Consistency Loss: 3.597916841506958\n",
      "----------Training epoch--------\n",
      "----------------8---------------\n",
      "\n",
      "Total Loss: 1134112.75\n",
      "\n",
      "Total Fwd Loss: 1948.611328125\n",
      "Total Bwd Loss: 1978.067626953125\n",
      "Total Inv Consistency Loss: 1130179.25\n",
      "Total Temp Consistency Loss: 6.890296936035156\n",
      "Total Loss: 1133460.75\n",
      "\n",
      "Total Fwd Loss: 1910.304443359375\n",
      "Total Bwd Loss: 1940.205322265625\n",
      "Total Inv Consistency Loss: 1129597.0\n",
      "Total Temp Consistency Loss: 13.228177070617676\n",
      "Total Loss: 1132818.125\n",
      "\n",
      "Total Fwd Loss: 1853.126220703125\n",
      "Total Bwd Loss: 1906.0982666015625\n",
      "Total Inv Consistency Loss: 1129033.75\n",
      "Total Temp Consistency Loss: 25.156612396240234\n",
      "Total Loss: 1132164.375\n",
      "\n",
      "Total Fwd Loss: 1793.4056396484375\n",
      "Total Bwd Loss: 1848.020263671875\n",
      "Total Inv Consistency Loss: 1128475.75\n",
      "Total Temp Consistency Loss: 47.280364990234375\n",
      "Total Loss: 1131556.5\n",
      "\n",
      "Total Fwd Loss: 1732.2808837890625\n",
      "Total Bwd Loss: 1798.6259765625\n",
      "Total Inv Consistency Loss: 1127936.5\n",
      "Total Temp Consistency Loss: 89.09380340576172\n",
      "Total Loss: 1130921.75\n",
      "\n",
      "Total Fwd Loss: 1654.195556640625\n",
      "Total Bwd Loss: 1716.439697265625\n",
      "Total Inv Consistency Loss: 1127393.25\n",
      "Total Temp Consistency Loss: 157.90536499023438\n",
      "----------Training epoch--------\n",
      "----------------9---------------\n",
      "\n",
      "Total Loss: 1130445.25\n",
      "\n",
      "Total Fwd Loss: 1630.2900390625\n",
      "Total Bwd Loss: 1716.504638671875\n",
      "Total Inv Consistency Loss: 1126868.875\n",
      "Total Temp Consistency Loss: 229.5946807861328\n",
      "Total Loss: 1129891.5\n",
      "\n",
      "Total Fwd Loss: 1606.6295166015625\n",
      "Total Bwd Loss: 1682.7255859375\n",
      "Total Inv Consistency Loss: 1126352.875\n",
      "Total Temp Consistency Loss: 249.19265747070312\n",
      "Total Loss: 1129326.5\n",
      "\n",
      "Total Fwd Loss: 1577.122314453125\n",
      "Total Bwd Loss: 1674.841064453125\n",
      "Total Inv Consistency Loss: 1125841.0\n",
      "Total Temp Consistency Loss: 233.47067260742188\n",
      "Total Loss: 1128720.375\n",
      "\n",
      "Total Fwd Loss: 1553.8253173828125\n",
      "Total Bwd Loss: 1640.6009521484375\n",
      "Total Inv Consistency Loss: 1125323.875\n",
      "Total Temp Consistency Loss: 202.14024353027344\n",
      "Total Loss: 1128176.5\n",
      "\n",
      "Total Fwd Loss: 1546.6541748046875\n",
      "Total Bwd Loss: 1633.709228515625\n",
      "Total Inv Consistency Loss: 1124827.25\n",
      "Total Temp Consistency Loss: 168.81959533691406\n",
      "Total Loss: 1127609.5\n",
      "\n",
      "Total Fwd Loss: 1519.9552001953125\n",
      "Total Bwd Loss: 1630.408203125\n",
      "Total Inv Consistency Loss: 1124317.75\n",
      "Total Temp Consistency Loss: 141.34230041503906\n",
      "----------Training epoch--------\n",
      "----------------10---------------\n",
      "\n",
      "Total Loss: 1127109.0\n",
      "\n",
      "Total Fwd Loss: 1540.698974609375\n",
      "Total Bwd Loss: 1614.331787109375\n",
      "Total Inv Consistency Loss: 1123830.75\n",
      "Total Temp Consistency Loss: 123.24886322021484\n",
      "Total Loss: 1126601.25\n",
      "\n",
      "Total Fwd Loss: 1515.184326171875\n",
      "Total Bwd Loss: 1625.015380859375\n",
      "Total Inv Consistency Loss: 1123348.75\n",
      "Total Temp Consistency Loss: 112.28953552246094\n",
      "Total Loss: 1126091.875\n",
      "\n",
      "Total Fwd Loss: 1515.70556640625\n",
      "Total Bwd Loss: 1605.4654541015625\n",
      "Total Inv Consistency Loss: 1122862.375\n",
      "Total Temp Consistency Loss: 108.3735580444336\n",
      "Total Loss: 1125591.25\n",
      "\n",
      "Total Fwd Loss: 1487.087890625\n",
      "Total Bwd Loss: 1613.034423828125\n",
      "Total Inv Consistency Loss: 1122381.0\n",
      "Total Temp Consistency Loss: 110.1465072631836\n",
      "Total Loss: 1125066.0\n",
      "\n",
      "Total Fwd Loss: 1473.9844970703125\n",
      "Total Bwd Loss: 1578.727783203125\n",
      "Total Inv Consistency Loss: 1121895.875\n",
      "Total Temp Consistency Loss: 117.40531158447266\n",
      "Total Loss: 1124542.375\n",
      "\n",
      "Total Fwd Loss: 1448.6947021484375\n",
      "Total Bwd Loss: 1549.05078125\n",
      "Total Inv Consistency Loss: 1121414.125\n",
      "Total Temp Consistency Loss: 130.50296020507812\n",
      "----------Training epoch--------\n",
      "----------------11---------------\n",
      "\n",
      "Total Loss: 1124018.875\n",
      "\n",
      "Total Fwd Loss: 1415.23486328125\n",
      "Total Bwd Loss: 1515.239013671875\n",
      "Total Inv Consistency Loss: 1120942.375\n",
      "Total Temp Consistency Loss: 146.00448608398438\n",
      "Total Loss: 1123513.875\n",
      "\n",
      "Total Fwd Loss: 1380.5504150390625\n",
      "Total Bwd Loss: 1501.364501953125\n",
      "Total Inv Consistency Loss: 1120469.375\n",
      "Total Temp Consistency Loss: 162.5865478515625\n",
      "Total Loss: 1123011.875\n",
      "\n",
      "Total Fwd Loss: 1368.0970458984375\n",
      "Total Bwd Loss: 1466.697998046875\n",
      "Total Inv Consistency Loss: 1120001.25\n",
      "Total Temp Consistency Loss: 175.88815307617188\n",
      "Total Loss: 1122495.0\n",
      "\n",
      "Total Fwd Loss: 1331.8902587890625\n",
      "Total Bwd Loss: 1446.846923828125\n",
      "Total Inv Consistency Loss: 1119532.875\n",
      "Total Temp Consistency Loss: 183.4220733642578\n",
      "Total Loss: 1122010.375\n",
      "\n",
      "Total Fwd Loss: 1314.9774169921875\n",
      "Total Bwd Loss: 1435.50146484375\n",
      "Total Inv Consistency Loss: 1119074.75\n",
      "Total Temp Consistency Loss: 185.17620849609375\n",
      "Total Loss: 1121507.625\n",
      "\n",
      "Total Fwd Loss: 1296.156494140625\n",
      "Total Bwd Loss: 1426.557373046875\n",
      "Total Inv Consistency Loss: 1118605.75\n",
      "Total Temp Consistency Loss: 179.13027954101562\n",
      "----------Training epoch--------\n",
      "----------------12---------------\n",
      "\n",
      "Total Loss: 1120967.125\n",
      "\n",
      "Total Fwd Loss: 1269.773193359375\n",
      "Total Bwd Loss: 1387.342529296875\n",
      "Total Inv Consistency Loss: 1118142.75\n",
      "Total Temp Consistency Loss: 167.19711303710938\n",
      "Total Loss: 1120454.375\n",
      "\n",
      "Total Fwd Loss: 1245.103515625\n",
      "Total Bwd Loss: 1372.4779052734375\n",
      "Total Inv Consistency Loss: 1117682.875\n",
      "Total Temp Consistency Loss: 153.89646911621094\n",
      "Total Loss: 1119929.625\n",
      "\n",
      "Total Fwd Loss: 1217.891845703125\n",
      "Total Bwd Loss: 1350.41552734375\n",
      "Total Inv Consistency Loss: 1117221.0\n",
      "Total Temp Consistency Loss: 140.42611694335938\n",
      "Total Loss: 1119390.5\n",
      "\n",
      "Total Fwd Loss: 1190.48583984375\n",
      "Total Bwd Loss: 1311.0843505859375\n",
      "Total Inv Consistency Loss: 1116761.0\n",
      "Total Temp Consistency Loss: 127.88163757324219\n",
      "Total Loss: 1118860.375\n",
      "\n",
      "Total Fwd Loss: 1154.647705078125\n",
      "Total Bwd Loss: 1283.5313720703125\n",
      "Total Inv Consistency Loss: 1116303.625\n",
      "Total Temp Consistency Loss: 118.64595794677734\n",
      "Total Loss: 1118355.0\n",
      "\n",
      "Total Fwd Loss: 1129.560791015625\n",
      "Total Bwd Loss: 1260.3408203125\n",
      "Total Inv Consistency Loss: 1115851.75\n",
      "Total Temp Consistency Loss: 113.43743896484375\n",
      "----------Training epoch--------\n",
      "----------------13---------------\n",
      "\n",
      "Total Loss: 1117816.625\n",
      "\n",
      "Total Fwd Loss: 1091.057373046875\n",
      "Total Bwd Loss: 1220.8841552734375\n",
      "Total Inv Consistency Loss: 1115393.75\n",
      "Total Temp Consistency Loss: 110.81561279296875\n",
      "Total Loss: 1117284.5\n",
      "\n",
      "Total Fwd Loss: 1050.4306640625\n",
      "Total Bwd Loss: 1187.919677734375\n",
      "Total Inv Consistency Loss: 1114934.25\n",
      "Total Temp Consistency Loss: 111.91361999511719\n",
      "Total Loss: 1116759.5\n",
      "\n",
      "Total Fwd Loss: 1009.3279418945312\n",
      "Total Bwd Loss: 1143.912841796875\n",
      "Total Inv Consistency Loss: 1114492.25\n",
      "Total Temp Consistency Loss: 113.95545959472656\n",
      "Total Loss: 1116200.5\n",
      "\n",
      "Total Fwd Loss: 962.31982421875\n",
      "Total Bwd Loss: 1092.0699462890625\n",
      "Total Inv Consistency Loss: 1114029.5\n",
      "Total Temp Consistency Loss: 116.68330383300781\n",
      "Total Loss: 1115646.75\n",
      "\n",
      "Total Fwd Loss: 909.2501220703125\n",
      "Total Bwd Loss: 1041.006103515625\n",
      "Total Inv Consistency Loss: 1113580.25\n",
      "Total Temp Consistency Loss: 116.26773834228516\n",
      "Total Loss: 1115089.625\n",
      "\n",
      "Total Fwd Loss: 856.6666870117188\n",
      "Total Bwd Loss: 997.84765625\n",
      "Total Inv Consistency Loss: 1113123.375\n",
      "Total Temp Consistency Loss: 111.7315673828125\n",
      "----------Training epoch--------\n",
      "----------------14---------------\n",
      "\n",
      "Total Loss: 1114524.75\n",
      "\n",
      "Total Fwd Loss: 803.6177978515625\n",
      "Total Bwd Loss: 941.2164916992188\n",
      "Total Inv Consistency Loss: 1112678.75\n",
      "Total Temp Consistency Loss: 101.17913055419922\n",
      "Total Loss: 1113937.5\n",
      "\n",
      "Total Fwd Loss: 747.164306640625\n",
      "Total Bwd Loss: 879.0123291015625\n",
      "Total Inv Consistency Loss: 1112223.25\n",
      "Total Temp Consistency Loss: 88.086669921875\n",
      "Total Loss: 1113363.125\n",
      "\n",
      "Total Fwd Loss: 688.8623046875\n",
      "Total Bwd Loss: 822.9909057617188\n",
      "Total Inv Consistency Loss: 1111775.75\n",
      "Total Temp Consistency Loss: 75.53218841552734\n",
      "Total Loss: 1112789.0\n",
      "\n",
      "Total Fwd Loss: 627.0371704101562\n",
      "Total Bwd Loss: 770.0106811523438\n",
      "Total Inv Consistency Loss: 1111326.75\n",
      "Total Temp Consistency Loss: 65.29266357421875\n",
      "Total Loss: 1112177.375\n",
      "\n",
      "Total Fwd Loss: 563.1998291015625\n",
      "Total Bwd Loss: 677.9780883789062\n",
      "Total Inv Consistency Loss: 1110876.625\n",
      "Total Temp Consistency Loss: 59.64768600463867\n",
      "Total Loss: 1111580.375\n",
      "\n",
      "Total Fwd Loss: 490.55108642578125\n",
      "Total Bwd Loss: 604.901123046875\n",
      "Total Inv Consistency Loss: 1110426.75\n",
      "Total Temp Consistency Loss: 58.157310485839844\n",
      "----------Training epoch--------\n",
      "----------------15---------------\n",
      "\n",
      "Total Loss: 1110991.75\n",
      "\n",
      "Total Fwd Loss: 418.400146484375\n",
      "Total Bwd Loss: 531.1082763671875\n",
      "Total Inv Consistency Loss: 1109983.5\n",
      "Total Temp Consistency Loss: 58.72111129760742\n",
      "Total Loss: 1110391.0\n",
      "\n",
      "Total Fwd Loss: 345.6878662109375\n",
      "Total Bwd Loss: 452.79132080078125\n",
      "Total Inv Consistency Loss: 1109538.25\n",
      "Total Temp Consistency Loss: 54.21886444091797\n",
      "Total Loss: 1109807.125\n",
      "\n",
      "Total Fwd Loss: 287.2997741699219\n",
      "Total Bwd Loss: 384.7555847167969\n",
      "Total Inv Consistency Loss: 1109083.75\n",
      "Total Temp Consistency Loss: 51.419189453125\n",
      "Total Loss: 1109228.75\n",
      "\n",
      "Total Fwd Loss: 228.1587677001953\n",
      "Total Bwd Loss: 321.20953369140625\n",
      "Total Inv Consistency Loss: 1108638.25\n",
      "Total Temp Consistency Loss: 41.15509796142578\n",
      "Total Loss: 1108660.5\n",
      "\n",
      "Total Fwd Loss: 182.2744598388672\n",
      "Total Bwd Loss: 259.61737060546875\n",
      "Total Inv Consistency Loss: 1108183.0\n",
      "Total Temp Consistency Loss: 35.57769012451172\n",
      "Total Loss: 1108116.875\n",
      "\n",
      "Total Fwd Loss: 141.07838439941406\n",
      "Total Bwd Loss: 204.1955108642578\n",
      "Total Inv Consistency Loss: 1107744.375\n",
      "Total Temp Consistency Loss: 27.24814224243164\n",
      "----------Training epoch--------\n",
      "----------------16---------------\n",
      "\n",
      "Total Loss: 1107561.5\n",
      "\n",
      "Total Fwd Loss: 97.0\n",
      "Total Bwd Loss: 148.55010986328125\n",
      "Total Inv Consistency Loss: 1107301.0\n",
      "Total Temp Consistency Loss: 14.971702575683594\n",
      "Total Loss: 1107041.125\n",
      "\n",
      "Total Fwd Loss: 69.855712890625\n",
      "Total Bwd Loss: 107.2992935180664\n",
      "Total Inv Consistency Loss: 1106853.75\n",
      "Total Temp Consistency Loss: 10.231170654296875\n",
      "Total Loss: 1106540.625\n",
      "\n",
      "Total Fwd Loss: 51.97621536254883\n",
      "Total Bwd Loss: 75.89820098876953\n",
      "Total Inv Consistency Loss: 1106405.625\n",
      "Total Temp Consistency Loss: 7.149057865142822\n",
      "Total Loss: 1106056.875\n",
      "\n",
      "Total Fwd Loss: 38.55705642700195\n",
      "Total Bwd Loss: 51.8026237487793\n",
      "Total Inv Consistency Loss: 1105962.0\n",
      "Total Temp Consistency Loss: 4.451209545135498\n",
      "Total Loss: 1105587.125\n",
      "\n",
      "Total Fwd Loss: 30.419986724853516\n",
      "Total Bwd Loss: 36.18964767456055\n",
      "Total Inv Consistency Loss: 1105517.375\n",
      "Total Temp Consistency Loss: 3.1584601402282715\n",
      "Total Loss: 1105130.0\n",
      "\n",
      "Total Fwd Loss: 27.703317642211914\n",
      "Total Bwd Loss: 24.997133255004883\n",
      "Total Inv Consistency Loss: 1105075.375\n",
      "Total Temp Consistency Loss: 1.8796579837799072\n",
      "----------Training epoch--------\n",
      "----------------17---------------\n",
      "\n",
      "Total Loss: 1104670.375\n",
      "\n",
      "Total Fwd Loss: 24.811662673950195\n",
      "Total Bwd Loss: 18.8821964263916\n",
      "Total Inv Consistency Loss: 1104625.0\n",
      "Total Temp Consistency Loss: 1.5775985717773438\n",
      "Total Loss: 1104213.25\n",
      "\n",
      "Total Fwd Loss: 19.24163246154785\n",
      "Total Bwd Loss: 16.42350196838379\n",
      "Total Inv Consistency Loss: 1104176.25\n",
      "Total Temp Consistency Loss: 1.3599889278411865\n",
      "Total Loss: 1103765.125\n",
      "\n",
      "Total Fwd Loss: 13.301486015319824\n",
      "Total Bwd Loss: 13.448623657226562\n",
      "Total Inv Consistency Loss: 1103737.0\n",
      "Total Temp Consistency Loss: 1.3877127170562744\n",
      "Total Loss: 1103319.75\n",
      "\n",
      "Total Fwd Loss: 12.682683944702148\n",
      "Total Bwd Loss: 15.993980407714844\n",
      "Total Inv Consistency Loss: 1103290.25\n",
      "Total Temp Consistency Loss: 0.8460963368415833\n",
      "Total Loss: 1102883.25\n",
      "\n",
      "Total Fwd Loss: 13.577165603637695\n",
      "Total Bwd Loss: 17.91385269165039\n",
      "Total Inv Consistency Loss: 1102850.875\n",
      "Total Temp Consistency Loss: 0.8821026682853699\n",
      "Total Loss: 1102450.625\n",
      "\n",
      "Total Fwd Loss: 15.133329391479492\n",
      "Total Bwd Loss: 21.282711029052734\n",
      "Total Inv Consistency Loss: 1102412.5\n",
      "Total Temp Consistency Loss: 1.8089863061904907\n",
      "----------Training epoch--------\n",
      "----------------18---------------\n",
      "\n",
      "Total Loss: 1102011.75\n",
      "\n",
      "Total Fwd Loss: 15.956149101257324\n",
      "Total Bwd Loss: 24.340463638305664\n",
      "Total Inv Consistency Loss: 1101969.25\n",
      "Total Temp Consistency Loss: 2.2613637447357178\n",
      "Total Loss: 1101561.625\n",
      "\n",
      "Total Fwd Loss: 14.989286422729492\n",
      "Total Bwd Loss: 23.630733489990234\n",
      "Total Inv Consistency Loss: 1101520.125\n",
      "Total Temp Consistency Loss: 2.8291549682617188\n",
      "Total Loss: 1101120.5\n",
      "\n",
      "Total Fwd Loss: 14.588544845581055\n",
      "Total Bwd Loss: 23.354717254638672\n",
      "Total Inv Consistency Loss: 1101079.125\n",
      "Total Temp Consistency Loss: 3.365204334259033\n",
      "Total Loss: 1100679.5\n",
      "\n",
      "Total Fwd Loss: 14.340459823608398\n",
      "Total Bwd Loss: 28.894107818603516\n",
      "Total Inv Consistency Loss: 1100633.125\n",
      "Total Temp Consistency Loss: 3.116056442260742\n",
      "Total Loss: 1100236.125\n",
      "\n",
      "Total Fwd Loss: 12.591115951538086\n",
      "Total Bwd Loss: 29.47648048400879\n",
      "Total Inv Consistency Loss: 1100191.5\n",
      "Total Temp Consistency Loss: 2.5182547569274902\n",
      "Total Loss: 1099796.25\n",
      "\n",
      "Total Fwd Loss: 12.885995864868164\n",
      "Total Bwd Loss: 32.330020904541016\n",
      "Total Inv Consistency Loss: 1099749.0\n",
      "Total Temp Consistency Loss: 2.0559422969818115\n",
      "----------Training epoch--------\n",
      "----------------19---------------\n",
      "\n",
      "Total Loss: 1099352.875\n",
      "\n",
      "Total Fwd Loss: 11.665047645568848\n",
      "Total Bwd Loss: 33.576595306396484\n",
      "Total Inv Consistency Loss: 1099305.75\n",
      "Total Temp Consistency Loss: 1.8409624099731445\n",
      "Total Loss: 1098914.125\n",
      "\n",
      "Total Fwd Loss: 12.336520195007324\n",
      "Total Bwd Loss: 30.66044044494629\n",
      "Total Inv Consistency Loss: 1098869.375\n",
      "Total Temp Consistency Loss: 1.747281789779663\n",
      "Total Loss: 1098472.125\n",
      "\n",
      "Total Fwd Loss: 13.150776863098145\n",
      "Total Bwd Loss: 30.429550170898438\n",
      "Total Inv Consistency Loss: 1098426.875\n",
      "Total Temp Consistency Loss: 1.6600277423858643\n",
      "Total Loss: 1098023.125\n",
      "\n",
      "Total Fwd Loss: 11.376696586608887\n",
      "Total Bwd Loss: 24.38846206665039\n",
      "Total Inv Consistency Loss: 1097985.75\n",
      "Total Temp Consistency Loss: 1.6176992654800415\n",
      "Total Loss: 1097581.5\n",
      "\n",
      "Total Fwd Loss: 12.21183967590332\n",
      "Total Bwd Loss: 22.558303833007812\n",
      "Total Inv Consistency Loss: 1097545.25\n",
      "Total Temp Consistency Loss: 1.5179156064987183\n",
      "Total Loss: 1097134.25\n",
      "\n",
      "Total Fwd Loss: 12.407163619995117\n",
      "Total Bwd Loss: 22.853708267211914\n",
      "Total Inv Consistency Loss: 1097097.625\n",
      "Total Temp Consistency Loss: 1.3290483951568604\n",
      "----------Training epoch--------\n",
      "----------------20---------------\n",
      "\n",
      "Total Loss: 1096697.0\n",
      "\n",
      "Total Fwd Loss: 12.954136848449707\n",
      "Total Bwd Loss: 22.477046966552734\n",
      "Total Inv Consistency Loss: 1096660.375\n",
      "Total Temp Consistency Loss: 1.2460606098175049\n",
      "Total Loss: 1096252.375\n",
      "\n",
      "Total Fwd Loss: 11.100507736206055\n",
      "Total Bwd Loss: 18.981273651123047\n",
      "Total Inv Consistency Loss: 1096221.125\n",
      "Total Temp Consistency Loss: 1.1779816150665283\n",
      "Total Loss: 1095807.875\n",
      "\n",
      "Total Fwd Loss: 10.611429214477539\n",
      "Total Bwd Loss: 15.581903457641602\n",
      "Total Inv Consistency Loss: 1095780.5\n",
      "Total Temp Consistency Loss: 1.1119189262390137\n",
      "Total Loss: 1095368.625\n",
      "\n",
      "Total Fwd Loss: 9.494344711303711\n",
      "Total Bwd Loss: 16.074575424194336\n",
      "Total Inv Consistency Loss: 1095342.0\n",
      "Total Temp Consistency Loss: 0.9912300109863281\n",
      "Total Loss: 1094929.5\n",
      "\n",
      "Total Fwd Loss: 9.981565475463867\n",
      "Total Bwd Loss: 16.070743560791016\n",
      "Total Inv Consistency Loss: 1094902.625\n",
      "Total Temp Consistency Loss: 0.8735882639884949\n",
      "Total Loss: 1094488.125\n",
      "\n",
      "Total Fwd Loss: 10.078999519348145\n",
      "Total Bwd Loss: 14.088549613952637\n",
      "Total Inv Consistency Loss: 1094463.25\n",
      "Total Temp Consistency Loss: 0.7776846289634705\n",
      "----------Training epoch--------\n",
      "----------------21---------------\n",
      "\n",
      "Total Loss: 1094037.375\n",
      "\n",
      "Total Fwd Loss: 9.365816116333008\n",
      "Total Bwd Loss: 12.873559951782227\n",
      "Total Inv Consistency Loss: 1094014.5\n",
      "Total Temp Consistency Loss: 0.6374670267105103\n",
      "Total Loss: 1093593.625\n",
      "\n",
      "Total Fwd Loss: 9.812474250793457\n",
      "Total Bwd Loss: 12.618005752563477\n",
      "Total Inv Consistency Loss: 1093570.75\n",
      "Total Temp Consistency Loss: 0.4722042679786682\n",
      "Total Loss: 1093166.25\n",
      "\n",
      "Total Fwd Loss: 10.377021789550781\n",
      "Total Bwd Loss: 13.84825611114502\n",
      "Total Inv Consistency Loss: 1093141.75\n",
      "Total Temp Consistency Loss: 0.305031418800354\n",
      "Total Loss: 1092714.875\n",
      "\n",
      "Total Fwd Loss: 8.90978717803955\n",
      "Total Bwd Loss: 10.278192520141602\n",
      "Total Inv Consistency Loss: 1092695.375\n",
      "Total Temp Consistency Loss: 0.2310241013765335\n",
      "Total Loss: 1092277.625\n",
      "\n",
      "Total Fwd Loss: 9.954419136047363\n",
      "Total Bwd Loss: 10.907318115234375\n",
      "Total Inv Consistency Loss: 1092256.5\n",
      "Total Temp Consistency Loss: 0.27577680349349976\n",
      "Total Loss: 1091838.875\n",
      "\n",
      "Total Fwd Loss: 8.474395751953125\n",
      "Total Bwd Loss: 11.227062225341797\n",
      "Total Inv Consistency Loss: 1091818.75\n",
      "Total Temp Consistency Loss: 0.38335925340652466\n",
      "----------Training epoch--------\n",
      "----------------22---------------\n",
      "\n",
      "Total Loss: 1091398.5\n",
      "\n",
      "Total Fwd Loss: 9.697578430175781\n",
      "Total Bwd Loss: 11.076288223266602\n",
      "Total Inv Consistency Loss: 1091377.25\n",
      "Total Temp Consistency Loss: 0.5002623200416565\n",
      "Total Loss: 1090958.25\n",
      "\n",
      "Total Fwd Loss: 8.673730850219727\n",
      "Total Bwd Loss: 8.892557144165039\n",
      "Total Inv Consistency Loss: 1090940.125\n",
      "Total Temp Consistency Loss: 0.5415621995925903\n",
      "Total Loss: 1090525.5\n",
      "\n",
      "Total Fwd Loss: 9.258163452148438\n",
      "Total Bwd Loss: 9.384690284729004\n",
      "Total Inv Consistency Loss: 1090506.25\n",
      "Total Temp Consistency Loss: 0.6026984453201294\n",
      "Total Loss: 1090088.75\n",
      "\n",
      "Total Fwd Loss: 9.064528465270996\n",
      "Total Bwd Loss: 9.649452209472656\n",
      "Total Inv Consistency Loss: 1090069.375\n",
      "Total Temp Consistency Loss: 0.6589952707290649\n",
      "Total Loss: 1089650.625\n",
      "\n",
      "Total Fwd Loss: 9.978489875793457\n",
      "Total Bwd Loss: 10.241655349731445\n",
      "Total Inv Consistency Loss: 1089629.625\n",
      "Total Temp Consistency Loss: 0.7054116129875183\n",
      "Total Loss: 1089213.375\n",
      "\n",
      "Total Fwd Loss: 9.218693733215332\n",
      "Total Bwd Loss: 9.363326072692871\n",
      "Total Inv Consistency Loss: 1089194.0\n",
      "Total Temp Consistency Loss: 0.7045822739601135\n",
      "----------Training epoch--------\n",
      "----------------23---------------\n",
      "\n",
      "Total Loss: 1088769.0\n",
      "\n",
      "Total Fwd Loss: 9.158712387084961\n",
      "Total Bwd Loss: 8.44453239440918\n",
      "Total Inv Consistency Loss: 1088750.75\n",
      "Total Temp Consistency Loss: 0.6859130859375\n",
      "Total Loss: 1088332.25\n",
      "\n",
      "Total Fwd Loss: 9.320943832397461\n",
      "Total Bwd Loss: 8.411038398742676\n",
      "Total Inv Consistency Loss: 1088313.75\n",
      "Total Temp Consistency Loss: 0.6890138387680054\n",
      "Total Loss: 1087900.0\n",
      "\n",
      "Total Fwd Loss: 10.44987678527832\n",
      "Total Bwd Loss: 9.838207244873047\n",
      "Total Inv Consistency Loss: 1087879.125\n",
      "Total Temp Consistency Loss: 0.6472229957580566\n",
      "Total Loss: 1087466.375\n",
      "\n",
      "Total Fwd Loss: 9.31059455871582\n",
      "Total Bwd Loss: 10.41728401184082\n",
      "Total Inv Consistency Loss: 1087446.0\n",
      "Total Temp Consistency Loss: 0.6378867030143738\n",
      "Total Loss: 1087015.875\n",
      "\n",
      "Total Fwd Loss: 8.517926216125488\n",
      "Total Bwd Loss: 8.28659439086914\n",
      "Total Inv Consistency Loss: 1086998.5\n",
      "Total Temp Consistency Loss: 0.5955097675323486\n",
      "Total Loss: 1086586.5\n",
      "\n",
      "Total Fwd Loss: 9.221790313720703\n",
      "Total Bwd Loss: 8.891792297363281\n",
      "Total Inv Consistency Loss: 1086567.75\n",
      "Total Temp Consistency Loss: 0.5883694291114807\n",
      "----------Training epoch--------\n",
      "----------------24---------------\n",
      "\n",
      "Total Loss: 1086147.625\n",
      "\n",
      "Total Fwd Loss: 10.503861427307129\n",
      "Total Bwd Loss: 10.008346557617188\n",
      "Total Inv Consistency Loss: 1086126.625\n",
      "Total Temp Consistency Loss: 0.5614596009254456\n",
      "Total Loss: 1085712.625\n",
      "\n",
      "Total Fwd Loss: 8.550735473632812\n",
      "Total Bwd Loss: 8.838197708129883\n",
      "Total Inv Consistency Loss: 1085694.75\n",
      "Total Temp Consistency Loss: 0.5550140142440796\n",
      "Total Loss: 1085279.5\n",
      "\n",
      "Total Fwd Loss: 9.264762878417969\n",
      "Total Bwd Loss: 8.876344680786133\n",
      "Total Inv Consistency Loss: 1085260.75\n",
      "Total Temp Consistency Loss: 0.567177951335907\n",
      "Total Loss: 1084841.0\n",
      "\n",
      "Total Fwd Loss: 9.422834396362305\n",
      "Total Bwd Loss: 8.054692268371582\n",
      "Total Inv Consistency Loss: 1084823.0\n",
      "Total Temp Consistency Loss: 0.557605504989624\n",
      "Total Loss: 1084405.5\n",
      "\n",
      "Total Fwd Loss: 8.378857612609863\n",
      "Total Bwd Loss: 9.001415252685547\n",
      "Total Inv Consistency Loss: 1084387.625\n",
      "Total Temp Consistency Loss: 0.5422526597976685\n",
      "Total Loss: 1083971.375\n",
      "\n",
      "Total Fwd Loss: 9.31382942199707\n",
      "Total Bwd Loss: 7.784742832183838\n",
      "Total Inv Consistency Loss: 1083953.75\n",
      "Total Temp Consistency Loss: 0.5309618711471558\n",
      "----------Training epoch--------\n",
      "----------------25---------------\n",
      "\n",
      "Total Loss: 1083536.375\n",
      "\n",
      "Total Fwd Loss: 8.777689933776855\n",
      "Total Bwd Loss: 8.290809631347656\n",
      "Total Inv Consistency Loss: 1083518.75\n",
      "Total Temp Consistency Loss: 0.5048366189002991\n",
      "Total Loss: 1083094.875\n",
      "\n",
      "Total Fwd Loss: 10.533208847045898\n",
      "Total Bwd Loss: 9.78597640991211\n",
      "Total Inv Consistency Loss: 1083074.0\n",
      "Total Temp Consistency Loss: 0.4584668278694153\n",
      "Total Loss: 1082659.25\n",
      "\n",
      "Total Fwd Loss: 8.283137321472168\n",
      "Total Bwd Loss: 8.221101760864258\n",
      "Total Inv Consistency Loss: 1082642.375\n",
      "Total Temp Consistency Loss: 0.415679395198822\n",
      "Total Loss: 1082230.375\n",
      "\n",
      "Total Fwd Loss: 9.263647079467773\n",
      "Total Bwd Loss: 8.120373725891113\n",
      "Total Inv Consistency Loss: 1082212.625\n",
      "Total Temp Consistency Loss: 0.3763204514980316\n",
      "Total Loss: 1081794.625\n",
      "\n",
      "Total Fwd Loss: 8.257291793823242\n",
      "Total Bwd Loss: 8.187379837036133\n",
      "Total Inv Consistency Loss: 1081777.75\n",
      "Total Temp Consistency Loss: 0.3347751796245575\n",
      "Total Loss: 1081352.375\n",
      "\n",
      "Total Fwd Loss: 7.519977569580078\n",
      "Total Bwd Loss: 8.892001152038574\n",
      "Total Inv Consistency Loss: 1081335.75\n",
      "Total Temp Consistency Loss: 0.2965998947620392\n",
      "----------Training epoch--------\n",
      "----------------26---------------\n",
      "\n",
      "Total Loss: 1080926.75\n",
      "\n",
      "Total Fwd Loss: 9.523103713989258\n",
      "Total Bwd Loss: 8.734251976013184\n",
      "Total Inv Consistency Loss: 1080908.25\n",
      "Total Temp Consistency Loss: 0.2748388648033142\n",
      "Total Loss: 1080490.125\n",
      "\n",
      "Total Fwd Loss: 8.355435371398926\n",
      "Total Bwd Loss: 9.543169021606445\n",
      "Total Inv Consistency Loss: 1080472.0\n",
      "Total Temp Consistency Loss: 0.2591627836227417\n",
      "Total Loss: 1080051.875\n",
      "\n",
      "Total Fwd Loss: 7.399779319763184\n",
      "Total Bwd Loss: 7.199840545654297\n",
      "Total Inv Consistency Loss: 1080037.0\n",
      "Total Temp Consistency Loss: 0.22442832589149475\n",
      "Total Loss: 1079617.75\n",
      "\n",
      "Total Fwd Loss: 8.58140754699707\n",
      "Total Bwd Loss: 8.471405029296875\n",
      "Total Inv Consistency Loss: 1079600.5\n",
      "Total Temp Consistency Loss: 0.2076282501220703\n",
      "Total Loss: 1079185.375\n",
      "\n",
      "Total Fwd Loss: 9.13814640045166\n",
      "Total Bwd Loss: 8.29288101196289\n",
      "Total Inv Consistency Loss: 1079167.75\n",
      "Total Temp Consistency Loss: 0.1973729431629181\n",
      "Total Loss: 1078751.375\n",
      "\n",
      "Total Fwd Loss: 7.113511085510254\n",
      "Total Bwd Loss: 8.805991172790527\n",
      "Total Inv Consistency Loss: 1078735.25\n",
      "Total Temp Consistency Loss: 0.19107824563980103\n",
      "----------Training epoch--------\n",
      "----------------27---------------\n",
      "\n",
      "Total Loss: 1078324.0\n",
      "\n",
      "Total Fwd Loss: 8.256132125854492\n",
      "Total Bwd Loss: 7.453666687011719\n",
      "Total Inv Consistency Loss: 1078308.125\n",
      "Total Temp Consistency Loss: 0.1843888908624649\n",
      "Total Loss: 1077891.5\n",
      "\n",
      "Total Fwd Loss: 7.289978981018066\n",
      "Total Bwd Loss: 8.964932441711426\n",
      "Total Inv Consistency Loss: 1077875.125\n",
      "Total Temp Consistency Loss: 0.17886564135551453\n",
      "Total Loss: 1077454.375\n",
      "\n",
      "Total Fwd Loss: 7.832818031311035\n",
      "Total Bwd Loss: 8.29366683959961\n",
      "Total Inv Consistency Loss: 1077438.125\n",
      "Total Temp Consistency Loss: 0.16138890385627747\n",
      "Total Loss: 1077024.125\n",
      "\n",
      "Total Fwd Loss: 8.968083381652832\n",
      "Total Bwd Loss: 9.527554512023926\n",
      "Total Inv Consistency Loss: 1077005.5\n",
      "Total Temp Consistency Loss: 0.15666936337947845\n",
      "Total Loss: 1076588.75\n",
      "\n",
      "Total Fwd Loss: 8.738964080810547\n",
      "Total Bwd Loss: 8.903299331665039\n",
      "Total Inv Consistency Loss: 1076571.0\n",
      "Total Temp Consistency Loss: 0.14750030636787415\n",
      "Total Loss: 1076151.375\n",
      "\n",
      "Total Fwd Loss: 7.407959938049316\n",
      "Total Bwd Loss: 8.063904762268066\n",
      "Total Inv Consistency Loss: 1076135.75\n",
      "Total Temp Consistency Loss: 0.1403495818376541\n",
      "----------Training epoch--------\n",
      "----------------28---------------\n",
      "\n",
      "Total Loss: 1075722.875\n",
      "\n",
      "Total Fwd Loss: 7.6679887771606445\n",
      "Total Bwd Loss: 7.752483367919922\n",
      "Total Inv Consistency Loss: 1075707.375\n",
      "Total Temp Consistency Loss: 0.1366790384054184\n",
      "Total Loss: 1075291.0\n",
      "\n",
      "Total Fwd Loss: 7.697671413421631\n",
      "Total Bwd Loss: 7.819514274597168\n",
      "Total Inv Consistency Loss: 1075275.375\n",
      "Total Temp Consistency Loss: 0.1347448229789734\n",
      "Total Loss: 1074858.625\n",
      "\n",
      "Total Fwd Loss: 6.89410924911499\n",
      "Total Bwd Loss: 9.889053344726562\n",
      "Total Inv Consistency Loss: 1074841.75\n",
      "Total Temp Consistency Loss: 0.14461103081703186\n",
      "Total Loss: 1074430.875\n",
      "\n",
      "Total Fwd Loss: 7.585021018981934\n",
      "Total Bwd Loss: 7.930113315582275\n",
      "Total Inv Consistency Loss: 1074415.25\n",
      "Total Temp Consistency Loss: 0.14342407882213593\n",
      "Total Loss: 1073995.5\n",
      "\n",
      "Total Fwd Loss: 9.926504135131836\n",
      "Total Bwd Loss: 9.444851875305176\n",
      "Total Inv Consistency Loss: 1073976.0\n",
      "Total Temp Consistency Loss: 0.1445079743862152\n",
      "Total Loss: 1073559.5\n",
      "\n",
      "Total Fwd Loss: 7.91775369644165\n",
      "Total Bwd Loss: 8.202547073364258\n",
      "Total Inv Consistency Loss: 1073543.25\n",
      "Total Temp Consistency Loss: 0.14144760370254517\n",
      "----------Training epoch--------\n",
      "----------------29---------------\n",
      "\n",
      "Total Loss: 1073138.25\n",
      "\n",
      "Total Fwd Loss: 7.160711765289307\n",
      "Total Bwd Loss: 8.171625137329102\n",
      "Total Inv Consistency Loss: 1073122.75\n",
      "Total Temp Consistency Loss: 0.14090122282505035\n",
      "Total Loss: 1072702.75\n",
      "\n",
      "Total Fwd Loss: 7.5522661209106445\n",
      "Total Bwd Loss: 8.78052806854248\n",
      "Total Inv Consistency Loss: 1072686.25\n",
      "Total Temp Consistency Loss: 0.13882996141910553\n",
      "Total Loss: 1072262.625\n",
      "\n",
      "Total Fwd Loss: 7.737444877624512\n",
      "Total Bwd Loss: 7.739016056060791\n",
      "Total Inv Consistency Loss: 1072247.0\n",
      "Total Temp Consistency Loss: 0.1330878883600235\n",
      "Total Loss: 1071842.25\n",
      "\n",
      "Total Fwd Loss: 8.105877876281738\n",
      "Total Bwd Loss: 8.611204147338867\n",
      "Total Inv Consistency Loss: 1071825.375\n",
      "Total Temp Consistency Loss: 0.13486658036708832\n",
      "Total Loss: 1071406.25\n",
      "\n",
      "Total Fwd Loss: 7.685031890869141\n",
      "Total Bwd Loss: 7.876117706298828\n",
      "Total Inv Consistency Loss: 1071390.625\n",
      "Total Temp Consistency Loss: 0.13694478571414948\n",
      "Total Loss: 1070979.0\n",
      "\n",
      "Total Fwd Loss: 9.132883071899414\n",
      "Total Bwd Loss: 9.432458877563477\n",
      "Total Inv Consistency Loss: 1070960.25\n",
      "Total Temp Consistency Loss: 0.14186158776283264\n",
      "----------Training epoch--------\n",
      "----------------30---------------\n",
      "\n",
      "Total Loss: 1070546.875\n",
      "\n",
      "Total Fwd Loss: 8.21828556060791\n",
      "Total Bwd Loss: 8.785888671875\n",
      "Total Inv Consistency Loss: 1070529.75\n",
      "Total Temp Consistency Loss: 0.13903237879276276\n",
      "Total Loss: 1070115.0\n",
      "\n",
      "Total Fwd Loss: 7.518194675445557\n",
      "Total Bwd Loss: 7.937668800354004\n",
      "Total Inv Consistency Loss: 1070099.375\n",
      "Total Temp Consistency Loss: 0.1362365484237671\n",
      "Total Loss: 1069689.375\n",
      "\n",
      "Total Fwd Loss: 7.103729248046875\n",
      "Total Bwd Loss: 7.872049808502197\n",
      "Total Inv Consistency Loss: 1069674.25\n",
      "Total Temp Consistency Loss: 0.13598161935806274\n",
      "Total Loss: 1069254.375\n",
      "\n",
      "Total Fwd Loss: 7.695864200592041\n",
      "Total Bwd Loss: 8.894397735595703\n",
      "Total Inv Consistency Loss: 1069237.625\n",
      "Total Temp Consistency Loss: 0.13318048417568207\n",
      "Total Loss: 1068828.125\n",
      "\n",
      "Total Fwd Loss: 7.690339088439941\n",
      "Total Bwd Loss: 7.915963172912598\n",
      "Total Inv Consistency Loss: 1068812.375\n",
      "Total Temp Consistency Loss: 0.1309448480606079\n",
      "Total Loss: 1068397.0\n",
      "\n",
      "Total Fwd Loss: 9.302913665771484\n",
      "Total Bwd Loss: 8.648065567016602\n",
      "Total Inv Consistency Loss: 1068378.875\n",
      "Total Temp Consistency Loss: 0.12994655966758728\n",
      "----------Training epoch--------\n",
      "----------------31---------------\n",
      "\n",
      "Total Loss: 1067963.625\n",
      "\n",
      "Total Fwd Loss: 7.956542015075684\n",
      "Total Bwd Loss: 8.76414966583252\n",
      "Total Inv Consistency Loss: 1067946.75\n",
      "Total Temp Consistency Loss: 0.13050054013729095\n",
      "Total Loss: 1067541.5\n",
      "\n",
      "Total Fwd Loss: 6.829592704772949\n",
      "Total Bwd Loss: 7.8281989097595215\n",
      "Total Inv Consistency Loss: 1067526.75\n",
      "Total Temp Consistency Loss: 0.13091066479682922\n",
      "Total Loss: 1067113.625\n",
      "\n",
      "Total Fwd Loss: 8.77619743347168\n",
      "Total Bwd Loss: 8.490583419799805\n",
      "Total Inv Consistency Loss: 1067096.25\n",
      "Total Temp Consistency Loss: 0.12892377376556396\n",
      "Total Loss: 1066683.0\n",
      "\n",
      "Total Fwd Loss: 8.326434135437012\n",
      "Total Bwd Loss: 8.772857666015625\n",
      "Total Inv Consistency Loss: 1066665.75\n",
      "Total Temp Consistency Loss: 0.13197575509548187\n",
      "Total Loss: 1066255.25\n",
      "\n",
      "Total Fwd Loss: 7.603081703186035\n",
      "Total Bwd Loss: 7.615830898284912\n",
      "Total Inv Consistency Loss: 1066239.875\n",
      "Total Temp Consistency Loss: 0.1322939395904541\n",
      "Total Loss: 1065829.25\n",
      "\n",
      "Total Fwd Loss: 8.042158126831055\n",
      "Total Bwd Loss: 8.19453239440918\n",
      "Total Inv Consistency Loss: 1065812.875\n",
      "Total Temp Consistency Loss: 0.1320774108171463\n",
      "----------Training epoch--------\n",
      "----------------32---------------\n",
      "\n",
      "Total Loss: 1065395.875\n",
      "\n",
      "Total Fwd Loss: 8.141972541809082\n",
      "Total Bwd Loss: 8.782623291015625\n",
      "Total Inv Consistency Loss: 1065378.875\n",
      "Total Temp Consistency Loss: 0.12972098588943481\n",
      "Total Loss: 1064962.0\n",
      "\n",
      "Total Fwd Loss: 6.9198503494262695\n",
      "Total Bwd Loss: 8.240621566772461\n",
      "Total Inv Consistency Loss: 1064946.75\n",
      "Total Temp Consistency Loss: 0.12886255979537964\n",
      "Total Loss: 1064535.75\n",
      "\n",
      "Total Fwd Loss: 7.833959102630615\n",
      "Total Bwd Loss: 8.081377029418945\n",
      "Total Inv Consistency Loss: 1064519.75\n",
      "Total Temp Consistency Loss: 0.12821033596992493\n",
      "Total Loss: 1064111.375\n",
      "\n",
      "Total Fwd Loss: 8.626440048217773\n",
      "Total Bwd Loss: 8.434381484985352\n",
      "Total Inv Consistency Loss: 1064094.25\n",
      "Total Temp Consistency Loss: 0.12663498520851135\n",
      "Total Loss: 1063684.875\n",
      "\n",
      "Total Fwd Loss: 7.637332916259766\n",
      "Total Bwd Loss: 7.593056678771973\n",
      "Total Inv Consistency Loss: 1063669.5\n",
      "Total Temp Consistency Loss: 0.13269346952438354\n",
      "Total Loss: 1063259.25\n",
      "\n",
      "Total Fwd Loss: 8.3250093460083\n",
      "Total Bwd Loss: 8.55766487121582\n",
      "Total Inv Consistency Loss: 1063242.25\n",
      "Total Temp Consistency Loss: 0.12816520035266876\n",
      "----------Training epoch--------\n",
      "----------------33---------------\n",
      "\n",
      "Total Loss: 1062834.125\n",
      "\n",
      "Total Fwd Loss: 7.8899383544921875\n",
      "Total Bwd Loss: 8.224967956542969\n",
      "Total Inv Consistency Loss: 1062817.875\n",
      "Total Temp Consistency Loss: 0.12892703711986542\n",
      "Total Loss: 1062398.0\n",
      "\n",
      "Total Fwd Loss: 6.768713474273682\n",
      "Total Bwd Loss: 6.948092460632324\n",
      "Total Inv Consistency Loss: 1062384.125\n",
      "Total Temp Consistency Loss: 0.12706038355827332\n",
      "Total Loss: 1061979.625\n",
      "\n",
      "Total Fwd Loss: 9.107030868530273\n",
      "Total Bwd Loss: 8.798049926757812\n",
      "Total Inv Consistency Loss: 1061961.625\n",
      "Total Temp Consistency Loss: 0.1302729994058609\n",
      "Total Loss: 1061550.75\n",
      "\n",
      "Total Fwd Loss: 7.514725685119629\n",
      "Total Bwd Loss: 7.853725433349609\n",
      "Total Inv Consistency Loss: 1061535.25\n",
      "Total Temp Consistency Loss: 0.13291002810001373\n",
      "Total Loss: 1061129.25\n",
      "\n",
      "Total Fwd Loss: 7.903677940368652\n",
      "Total Bwd Loss: 8.640356063842773\n",
      "Total Inv Consistency Loss: 1061112.625\n",
      "Total Temp Consistency Loss: 0.13143828511238098\n",
      "Total Loss: 1060700.5\n",
      "\n",
      "Total Fwd Loss: 8.398541450500488\n",
      "Total Bwd Loss: 8.972514152526855\n",
      "Total Inv Consistency Loss: 1060683.0\n",
      "Total Temp Consistency Loss: 0.13269834220409393\n",
      "----------Training epoch--------\n",
      "----------------34---------------\n",
      "\n",
      "Total Loss: 1060279.125\n",
      "\n",
      "Total Fwd Loss: 6.804126739501953\n",
      "Total Bwd Loss: 7.319921970367432\n",
      "Total Inv Consistency Loss: 1060264.875\n",
      "Total Temp Consistency Loss: 0.12735524773597717\n",
      "Total Loss: 1059842.125\n",
      "\n",
      "Total Fwd Loss: 7.5730881690979\n",
      "Total Bwd Loss: 7.6479620933532715\n",
      "Total Inv Consistency Loss: 1059826.75\n",
      "Total Temp Consistency Loss: 0.12415866553783417\n",
      "Total Loss: 1059415.875\n",
      "\n",
      "Total Fwd Loss: 9.851651191711426\n",
      "Total Bwd Loss: 8.625093460083008\n",
      "Total Inv Consistency Loss: 1059397.25\n",
      "Total Temp Consistency Loss: 0.12332861125469208\n",
      "Total Loss: 1058996.625\n",
      "\n",
      "Total Fwd Loss: 8.639708518981934\n",
      "Total Bwd Loss: 9.549562454223633\n",
      "Total Inv Consistency Loss: 1058978.25\n",
      "Total Temp Consistency Loss: 0.12582866847515106\n",
      "Total Loss: 1058568.125\n",
      "\n",
      "Total Fwd Loss: 7.730327606201172\n",
      "Total Bwd Loss: 8.969013214111328\n",
      "Total Inv Consistency Loss: 1058551.25\n",
      "Total Temp Consistency Loss: 0.13220767676830292\n",
      "Total Loss: 1058139.375\n",
      "\n",
      "Total Fwd Loss: 6.797336578369141\n",
      "Total Bwd Loss: 7.3866071701049805\n",
      "Total Inv Consistency Loss: 1058125.125\n",
      "Total Temp Consistency Loss: 0.1263548731803894\n",
      "----------Training epoch--------\n",
      "----------------35---------------\n",
      "\n",
      "Total Loss: 1057712.5\n",
      "\n",
      "Total Fwd Loss: 7.616414546966553\n",
      "Total Bwd Loss: 8.473215103149414\n",
      "Total Inv Consistency Loss: 1057696.25\n",
      "Total Temp Consistency Loss: 0.12653414905071259\n",
      "Total Loss: 1057293.875\n",
      "\n",
      "Total Fwd Loss: 7.643280029296875\n",
      "Total Bwd Loss: 7.4590253829956055\n",
      "Total Inv Consistency Loss: 1057278.625\n",
      "Total Temp Consistency Loss: 0.12610147893428802\n",
      "Total Loss: 1056869.5\n",
      "\n",
      "Total Fwd Loss: 8.538897514343262\n",
      "Total Bwd Loss: 8.049739837646484\n",
      "Total Inv Consistency Loss: 1056852.75\n",
      "Total Temp Consistency Loss: 0.12608884274959564\n",
      "Total Loss: 1056446.0\n",
      "\n",
      "Total Fwd Loss: 7.484766483306885\n",
      "Total Bwd Loss: 7.588126182556152\n",
      "Total Inv Consistency Loss: 1056430.75\n",
      "Total Temp Consistency Loss: 0.1298290640115738\n",
      "Total Loss: 1056021.625\n",
      "\n",
      "Total Fwd Loss: 8.854463577270508\n",
      "Total Bwd Loss: 9.447578430175781\n",
      "Total Inv Consistency Loss: 1056003.25\n",
      "Total Temp Consistency Loss: 0.12977054715156555\n",
      "Total Loss: 1055593.625\n",
      "\n",
      "Total Fwd Loss: 7.529904842376709\n",
      "Total Bwd Loss: 8.060197830200195\n",
      "Total Inv Consistency Loss: 1055577.875\n",
      "Total Temp Consistency Loss: 0.132396399974823\n",
      "----------Training epoch--------\n",
      "----------------36---------------\n",
      "\n",
      "Total Loss: 1055167.625\n",
      "\n",
      "Total Fwd Loss: 7.1270012855529785\n",
      "Total Bwd Loss: 7.590681552886963\n",
      "Total Inv Consistency Loss: 1055152.75\n",
      "Total Temp Consistency Loss: 0.12677507102489471\n",
      "Total Loss: 1054750.0\n",
      "\n",
      "Total Fwd Loss: 8.217315673828125\n",
      "Total Bwd Loss: 8.999462127685547\n",
      "Total Inv Consistency Loss: 1054732.625\n",
      "Total Temp Consistency Loss: 0.12091292440891266\n",
      "Total Loss: 1054326.25\n",
      "\n",
      "Total Fwd Loss: 8.603379249572754\n",
      "Total Bwd Loss: 8.621659278869629\n",
      "Total Inv Consistency Loss: 1054308.875\n",
      "Total Temp Consistency Loss: 0.12002046406269073\n",
      "Total Loss: 1053900.25\n",
      "\n",
      "Total Fwd Loss: 8.59708023071289\n",
      "Total Bwd Loss: 8.285493850708008\n",
      "Total Inv Consistency Loss: 1053883.25\n",
      "Total Temp Consistency Loss: 0.12043057382106781\n",
      "Total Loss: 1053474.25\n",
      "\n",
      "Total Fwd Loss: 7.359812259674072\n",
      "Total Bwd Loss: 8.55506420135498\n",
      "Total Inv Consistency Loss: 1053458.25\n",
      "Total Temp Consistency Loss: 0.12394263595342636\n",
      "Total Loss: 1053050.5\n",
      "\n",
      "Total Fwd Loss: 7.655818939208984\n",
      "Total Bwd Loss: 7.241158485412598\n",
      "Total Inv Consistency Loss: 1053035.5\n",
      "Total Temp Consistency Loss: 0.1291504204273224\n",
      "----------Training epoch--------\n",
      "----------------37---------------\n",
      "\n",
      "Total Loss: 1052625.5\n",
      "\n",
      "Total Fwd Loss: 7.100433349609375\n",
      "Total Bwd Loss: 8.036857604980469\n",
      "Total Inv Consistency Loss: 1052610.25\n",
      "Total Temp Consistency Loss: 0.12382201850414276\n",
      "Total Loss: 1052203.875\n",
      "\n",
      "Total Fwd Loss: 8.157456398010254\n",
      "Total Bwd Loss: 9.300764083862305\n",
      "Total Inv Consistency Loss: 1052186.25\n",
      "Total Temp Consistency Loss: 0.12256882339715958\n",
      "Total Loss: 1051783.125\n",
      "\n",
      "Total Fwd Loss: 8.090837478637695\n",
      "Total Bwd Loss: 8.201799392700195\n",
      "Total Inv Consistency Loss: 1051766.75\n",
      "Total Temp Consistency Loss: 0.12138499319553375\n",
      "Total Loss: 1051358.125\n",
      "\n",
      "Total Fwd Loss: 8.142494201660156\n",
      "Total Bwd Loss: 7.937270164489746\n",
      "Total Inv Consistency Loss: 1051341.875\n",
      "Total Temp Consistency Loss: 0.12096921354532242\n",
      "Total Loss: 1050939.5\n",
      "\n",
      "Total Fwd Loss: 8.972309112548828\n",
      "Total Bwd Loss: 7.672035217285156\n",
      "Total Inv Consistency Loss: 1050922.75\n",
      "Total Temp Consistency Loss: 0.1290561854839325\n",
      "Total Loss: 1050515.125\n",
      "\n",
      "Total Fwd Loss: 7.225391387939453\n",
      "Total Bwd Loss: 7.987138271331787\n",
      "Total Inv Consistency Loss: 1050499.75\n",
      "Total Temp Consistency Loss: 0.12848934531211853\n",
      "----------Training epoch--------\n",
      "----------------38---------------\n",
      "\n",
      "Total Loss: 1050092.5\n",
      "\n",
      "Total Fwd Loss: 6.948689937591553\n",
      "Total Bwd Loss: 9.103826522827148\n",
      "Total Inv Consistency Loss: 1050076.375\n",
      "Total Temp Consistency Loss: 0.1257455050945282\n",
      "Total Loss: 1049673.0\n",
      "\n",
      "Total Fwd Loss: 7.28096866607666\n",
      "Total Bwd Loss: 8.140778541564941\n",
      "Total Inv Consistency Loss: 1049657.5\n",
      "Total Temp Consistency Loss: 0.11738785356283188\n",
      "Total Loss: 1049253.375\n",
      "\n",
      "Total Fwd Loss: 8.015439987182617\n",
      "Total Bwd Loss: 7.4978742599487305\n",
      "Total Inv Consistency Loss: 1049237.75\n",
      "Total Temp Consistency Loss: 0.11270419508218765\n",
      "Total Loss: 1048828.125\n",
      "\n",
      "Total Fwd Loss: 8.068748474121094\n",
      "Total Bwd Loss: 6.864161491394043\n",
      "Total Inv Consistency Loss: 1048813.125\n",
      "Total Temp Consistency Loss: 0.11535151302814484\n",
      "Total Loss: 1048406.6875\n",
      "\n",
      "Total Fwd Loss: 7.390536308288574\n",
      "Total Bwd Loss: 8.283021926879883\n",
      "Total Inv Consistency Loss: 1048390.875\n",
      "Total Temp Consistency Loss: 0.12013141065835953\n",
      "Total Loss: 1047984.0625\n",
      "\n",
      "Total Fwd Loss: 9.842798233032227\n",
      "Total Bwd Loss: 9.311635971069336\n",
      "Total Inv Consistency Loss: 1047964.8125\n",
      "Total Temp Consistency Loss: 0.11844011396169662\n",
      "----------Training epoch--------\n",
      "----------------39---------------\n",
      "\n",
      "Total Loss: 1047567.4375\n",
      "\n",
      "Total Fwd Loss: 8.57688045501709\n",
      "Total Bwd Loss: 8.413975715637207\n",
      "Total Inv Consistency Loss: 1047550.3125\n",
      "Total Temp Consistency Loss: 0.11538221687078476\n",
      "Total Loss: 1047143.75\n",
      "\n",
      "Total Fwd Loss: 7.808316230773926\n",
      "Total Bwd Loss: 7.532082557678223\n",
      "Total Inv Consistency Loss: 1047128.3125\n",
      "Total Temp Consistency Loss: 0.11197555065155029\n",
      "Total Loss: 1046725.25\n",
      "\n",
      "Total Fwd Loss: 7.729780673980713\n",
      "Total Bwd Loss: 8.073848724365234\n",
      "Total Inv Consistency Loss: 1046709.3125\n",
      "Total Temp Consistency Loss: 0.11142341792583466\n",
      "Total Loss: 1046304.625\n",
      "\n",
      "Total Fwd Loss: 8.105569839477539\n",
      "Total Bwd Loss: 8.695016860961914\n",
      "Total Inv Consistency Loss: 1046287.6875\n",
      "Total Temp Consistency Loss: 0.1147553101181984\n",
      "Total Loss: 1045878.1875\n",
      "\n",
      "Total Fwd Loss: 7.204301357269287\n",
      "Total Bwd Loss: 7.862218379974365\n",
      "Total Inv Consistency Loss: 1045863.0\n",
      "Total Temp Consistency Loss: 0.1221495270729065\n",
      "Total Loss: 1045453.625\n",
      "\n",
      "Total Fwd Loss: 8.045012474060059\n",
      "Total Bwd Loss: 8.600454330444336\n",
      "Total Inv Consistency Loss: 1045436.875\n",
      "Total Temp Consistency Loss: 0.11842018365859985\n",
      "----------Training epoch--------\n",
      "----------------40---------------\n",
      "\n",
      "Total Loss: 1045039.1875\n",
      "\n",
      "Total Fwd Loss: 6.817756652832031\n",
      "Total Bwd Loss: 8.929991722106934\n",
      "Total Inv Consistency Loss: 1045023.3125\n",
      "Total Temp Consistency Loss: 0.1146184653043747\n",
      "Total Loss: 1044617.375\n",
      "\n",
      "Total Fwd Loss: 8.438409805297852\n",
      "Total Bwd Loss: 7.658492088317871\n",
      "Total Inv Consistency Loss: 1044601.125\n",
      "Total Temp Consistency Loss: 0.1118398904800415\n",
      "Total Loss: 1044203.625\n",
      "\n",
      "Total Fwd Loss: 7.5794196128845215\n",
      "Total Bwd Loss: 8.217425346374512\n",
      "Total Inv Consistency Loss: 1044187.6875\n",
      "Total Temp Consistency Loss: 0.11058832705020905\n",
      "Total Loss: 1043783.9375\n",
      "\n",
      "Total Fwd Loss: 8.199318885803223\n",
      "Total Bwd Loss: 8.098011016845703\n",
      "Total Inv Consistency Loss: 1043767.5\n",
      "Total Temp Consistency Loss: 0.11033706367015839\n",
      "Total Loss: 1043362.4375\n",
      "\n",
      "Total Fwd Loss: 8.956416130065918\n",
      "Total Bwd Loss: 8.559016227722168\n",
      "Total Inv Consistency Loss: 1043344.8125\n",
      "Total Temp Consistency Loss: 0.10996975749731064\n",
      "Total Loss: 1042949.125\n",
      "\n",
      "Total Fwd Loss: 7.45223331451416\n",
      "Total Bwd Loss: 7.719059944152832\n",
      "Total Inv Consistency Loss: 1042933.8125\n",
      "Total Temp Consistency Loss: 0.11047647893428802\n",
      "----------Training epoch--------\n",
      "----------------41---------------\n",
      "\n",
      "Total Loss: 1042523.3125\n",
      "\n",
      "Total Fwd Loss: 7.065194606781006\n",
      "Total Bwd Loss: 8.152532577514648\n",
      "Total Inv Consistency Loss: 1042508.0\n",
      "Total Temp Consistency Loss: 0.11140326410531998\n",
      "Total Loss: 1042189.5\n",
      "\n",
      "Total Fwd Loss: 8.700736045837402\n",
      "Total Bwd Loss: 8.57444953918457\n",
      "Total Inv Consistency Loss: 1042172.125\n",
      "Total Temp Consistency Loss: 0.10764347016811371\n",
      "Total Loss: 1041853.5\n",
      "\n",
      "Total Fwd Loss: 7.105344295501709\n",
      "Total Bwd Loss: 7.175826072692871\n",
      "Total Inv Consistency Loss: 1041839.125\n",
      "Total Temp Consistency Loss: 0.10713998973369598\n",
      "Total Loss: 1041520.25\n",
      "\n",
      "Total Fwd Loss: 7.700478553771973\n",
      "Total Bwd Loss: 8.098073959350586\n",
      "Total Inv Consistency Loss: 1041504.3125\n",
      "Total Temp Consistency Loss: 0.10716471821069717\n",
      "Total Loss: 1041186.3125\n",
      "\n",
      "Total Fwd Loss: 9.170720100402832\n",
      "Total Bwd Loss: 9.348963737487793\n",
      "Total Inv Consistency Loss: 1041167.6875\n",
      "Total Temp Consistency Loss: 0.1105528250336647\n",
      "Total Loss: 1040848.5625\n",
      "\n",
      "Total Fwd Loss: 7.672579765319824\n",
      "Total Bwd Loss: 7.7865118980407715\n",
      "Total Inv Consistency Loss: 1040833.0\n",
      "Total Temp Consistency Loss: 0.1092091053724289\n",
      "----------Training epoch--------\n",
      "----------------42---------------\n",
      "\n",
      "Total Loss: 1040514.1875\n",
      "\n",
      "Total Fwd Loss: 9.01628303527832\n",
      "Total Bwd Loss: 8.364521980285645\n",
      "Total Inv Consistency Loss: 1040496.6875\n",
      "Total Temp Consistency Loss: 0.10900763422250748\n",
      "Total Loss: 1040178.375\n",
      "\n",
      "Total Fwd Loss: 7.4724225997924805\n",
      "Total Bwd Loss: 8.137960433959961\n",
      "Total Inv Consistency Loss: 1040162.625\n",
      "Total Temp Consistency Loss: 0.11107099056243896\n",
      "Total Loss: 1039844.9375\n",
      "\n",
      "Total Fwd Loss: 6.948912143707275\n",
      "Total Bwd Loss: 8.389432907104492\n",
      "Total Inv Consistency Loss: 1039829.5\n",
      "Total Temp Consistency Loss: 0.10615895688533783\n",
      "Total Loss: 1039509.875\n",
      "\n",
      "Total Fwd Loss: 8.17344856262207\n",
      "Total Bwd Loss: 7.955780029296875\n",
      "Total Inv Consistency Loss: 1039493.625\n",
      "Total Temp Consistency Loss: 0.1050686463713646\n",
      "Total Loss: 1039182.875\n",
      "\n",
      "Total Fwd Loss: 7.941361427307129\n",
      "Total Bwd Loss: 8.176270484924316\n",
      "Total Inv Consistency Loss: 1039166.625\n",
      "Total Temp Consistency Loss: 0.10625012218952179\n",
      "Total Loss: 1038846.9375\n",
      "\n",
      "Total Fwd Loss: 7.889571189880371\n",
      "Total Bwd Loss: 8.11204719543457\n",
      "Total Inv Consistency Loss: 1038830.8125\n",
      "Total Temp Consistency Loss: 0.10590539127588272\n",
      "----------Training epoch--------\n",
      "----------------43---------------\n",
      "\n",
      "Total Loss: 1038509.8125\n",
      "\n",
      "Total Fwd Loss: 7.001904487609863\n",
      "Total Bwd Loss: 7.399033546447754\n",
      "Total Inv Consistency Loss: 1038495.3125\n",
      "Total Temp Consistency Loss: 0.1062842383980751\n",
      "Total Loss: 1038177.5625\n",
      "\n",
      "Total Fwd Loss: 7.855710506439209\n",
      "Total Bwd Loss: 8.773542404174805\n",
      "Total Inv Consistency Loss: 1038160.8125\n",
      "Total Temp Consistency Loss: 0.10565674304962158\n",
      "Total Loss: 1037846.5625\n",
      "\n",
      "Total Fwd Loss: 8.215295791625977\n",
      "Total Bwd Loss: 7.8951897621154785\n",
      "Total Inv Consistency Loss: 1037830.3125\n",
      "Total Temp Consistency Loss: 0.10473240911960602\n",
      "Total Loss: 1037508.625\n",
      "\n",
      "Total Fwd Loss: 8.71919059753418\n",
      "Total Bwd Loss: 8.092910766601562\n",
      "Total Inv Consistency Loss: 1037491.6875\n",
      "Total Temp Consistency Loss: 0.10399387776851654\n",
      "Total Loss: 1037182.6875\n",
      "\n",
      "Total Fwd Loss: 7.806752681732178\n",
      "Total Bwd Loss: 8.04885482788086\n",
      "Total Inv Consistency Loss: 1037166.6875\n",
      "Total Temp Consistency Loss: 0.10616268217563629\n",
      "Total Loss: 1036845.125\n",
      "\n",
      "Total Fwd Loss: 7.873045921325684\n",
      "Total Bwd Loss: 8.93073844909668\n",
      "Total Inv Consistency Loss: 1036828.1875\n",
      "Total Temp Consistency Loss: 0.10584809631109238\n",
      "----------Training epoch--------\n",
      "----------------44---------------\n",
      "\n",
      "Total Loss: 1036509.8125\n",
      "\n",
      "Total Fwd Loss: 7.445212364196777\n",
      "Total Bwd Loss: 8.120088577270508\n",
      "Total Inv Consistency Loss: 1036494.125\n",
      "Total Temp Consistency Loss: 0.10388340801000595\n",
      "Total Loss: 1036171.25\n",
      "\n",
      "Total Fwd Loss: 7.701641082763672\n",
      "Total Bwd Loss: 7.109279632568359\n",
      "Total Inv Consistency Loss: 1036156.3125\n",
      "Total Temp Consistency Loss: 0.1034223660826683\n",
      "Total Loss: 1035843.5\n",
      "\n",
      "Total Fwd Loss: 8.791318893432617\n",
      "Total Bwd Loss: 8.88967514038086\n",
      "Total Inv Consistency Loss: 1035825.6875\n",
      "Total Temp Consistency Loss: 0.10353819280862808\n",
      "Total Loss: 1035515.5625\n",
      "\n",
      "Total Fwd Loss: 8.726615905761719\n",
      "Total Bwd Loss: 8.827985763549805\n",
      "Total Inv Consistency Loss: 1035497.875\n",
      "Total Temp Consistency Loss: 0.10303501039743423\n",
      "Total Loss: 1035176.4375\n",
      "\n",
      "Total Fwd Loss: 6.68752384185791\n",
      "Total Bwd Loss: 8.120155334472656\n",
      "Total Inv Consistency Loss: 1035161.5\n",
      "Total Temp Consistency Loss: 0.10522294044494629\n",
      "Total Loss: 1034841.5625\n",
      "\n",
      "Total Fwd Loss: 8.227404594421387\n",
      "Total Bwd Loss: 8.011228561401367\n",
      "Total Inv Consistency Loss: 1034825.1875\n",
      "Total Temp Consistency Loss: 0.10337156057357788\n",
      "----------Training epoch--------\n",
      "----------------45---------------\n",
      "\n",
      "Total Loss: 1034506.625\n",
      "\n",
      "Total Fwd Loss: 7.678409576416016\n",
      "Total Bwd Loss: 8.306831359863281\n",
      "Total Inv Consistency Loss: 1034490.5\n",
      "Total Temp Consistency Loss: 0.10421615839004517\n",
      "Total Loss: 1034180.375\n",
      "\n",
      "Total Fwd Loss: 9.259943008422852\n",
      "Total Bwd Loss: 8.141568183898926\n",
      "Total Inv Consistency Loss: 1034162.875\n",
      "Total Temp Consistency Loss: 0.10125158727169037\n",
      "Total Loss: 1033847.3125\n",
      "\n",
      "Total Fwd Loss: 7.173245906829834\n",
      "Total Bwd Loss: 7.657132625579834\n",
      "Total Inv Consistency Loss: 1033832.375\n",
      "Total Temp Consistency Loss: 0.10230280458927155\n",
      "Total Loss: 1033517.3125\n",
      "\n",
      "Total Fwd Loss: 7.531487941741943\n",
      "Total Bwd Loss: 7.5141801834106445\n",
      "Total Inv Consistency Loss: 1033502.125\n",
      "Total Temp Consistency Loss: 0.10211098194122314\n",
      "Total Loss: 1033184.375\n",
      "\n",
      "Total Fwd Loss: 7.3320746421813965\n",
      "Total Bwd Loss: 8.033665657043457\n",
      "Total Inv Consistency Loss: 1033168.875\n",
      "Total Temp Consistency Loss: 0.10130438953638077\n",
      "Total Loss: 1032850.6875\n",
      "\n",
      "Total Fwd Loss: 8.609312057495117\n",
      "Total Bwd Loss: 9.344675064086914\n",
      "Total Inv Consistency Loss: 1032832.625\n",
      "Total Temp Consistency Loss: 0.10376067459583282\n",
      "----------Training epoch--------\n",
      "----------------46---------------\n",
      "\n",
      "Total Loss: 1032517.9375\n",
      "\n",
      "Total Fwd Loss: 7.596055507659912\n",
      "Total Bwd Loss: 7.8749680519104\n",
      "Total Inv Consistency Loss: 1032502.3125\n",
      "Total Temp Consistency Loss: 0.10170354694128036\n",
      "Total Loss: 1032190.0\n",
      "\n",
      "Total Fwd Loss: 9.28448486328125\n",
      "Total Bwd Loss: 8.614291191101074\n",
      "Total Inv Consistency Loss: 1032172.0\n",
      "Total Temp Consistency Loss: 0.10069730132818222\n",
      "Total Loss: 1031856.1875\n",
      "\n",
      "Total Fwd Loss: 8.90650749206543\n",
      "Total Bwd Loss: 7.784085273742676\n",
      "Total Inv Consistency Loss: 1031839.375\n",
      "Total Temp Consistency Loss: 0.10037276893854141\n",
      "Total Loss: 1031524.25\n",
      "\n",
      "Total Fwd Loss: 7.117238521575928\n",
      "Total Bwd Loss: 7.373139381408691\n",
      "Total Inv Consistency Loss: 1031509.625\n",
      "Total Temp Consistency Loss: 0.1017768383026123\n",
      "Total Loss: 1031194.6875\n",
      "\n",
      "Total Fwd Loss: 7.85671854019165\n",
      "Total Bwd Loss: 8.420730590820312\n",
      "Total Inv Consistency Loss: 1031178.3125\n",
      "Total Temp Consistency Loss: 0.09939281642436981\n",
      "Total Loss: 1030861.3125\n",
      "\n",
      "Total Fwd Loss: 6.849276065826416\n",
      "Total Bwd Loss: 9.000853538513184\n",
      "Total Inv Consistency Loss: 1030845.3125\n",
      "Total Temp Consistency Loss: 0.10097746551036835\n",
      "----------Training epoch--------\n",
      "----------------47---------------\n",
      "\n",
      "Total Loss: 1030527.6875\n",
      "\n",
      "Total Fwd Loss: 8.523968696594238\n",
      "Total Bwd Loss: 7.697091579437256\n",
      "Total Inv Consistency Loss: 1030511.3125\n",
      "Total Temp Consistency Loss: 0.09859076142311096\n",
      "Total Loss: 1030201.9375\n",
      "\n",
      "Total Fwd Loss: 9.093241691589355\n",
      "Total Bwd Loss: 8.930684089660645\n",
      "Total Inv Consistency Loss: 1030183.8125\n",
      "Total Temp Consistency Loss: 0.09813902527093887\n",
      "Total Loss: 1029867.4375\n",
      "\n",
      "Total Fwd Loss: 7.794531345367432\n",
      "Total Bwd Loss: 7.362715244293213\n",
      "Total Inv Consistency Loss: 1029852.125\n",
      "Total Temp Consistency Loss: 0.09873905777931213\n",
      "Total Loss: 1029527.0625\n",
      "\n",
      "Total Fwd Loss: 7.262784004211426\n",
      "Total Bwd Loss: 7.660749912261963\n",
      "Total Inv Consistency Loss: 1029512.0\n",
      "Total Temp Consistency Loss: 0.09811627864837646\n",
      "Total Loss: 1029210.5\n",
      "\n",
      "Total Fwd Loss: 7.990111351013184\n",
      "Total Bwd Loss: 9.412397384643555\n",
      "Total Inv Consistency Loss: 1029193.0\n",
      "Total Temp Consistency Loss: 0.10231062024831772\n",
      "Total Loss: 1028867.8125\n",
      "\n",
      "Total Fwd Loss: 6.856531620025635\n",
      "Total Bwd Loss: 7.975189208984375\n",
      "Total Inv Consistency Loss: 1028852.875\n",
      "Total Temp Consistency Loss: 0.09968877583742142\n",
      "----------Training epoch--------\n",
      "----------------48---------------\n",
      "\n",
      "Total Loss: 1028549.6875\n",
      "\n",
      "Total Fwd Loss: 9.533241271972656\n",
      "Total Bwd Loss: 8.696029663085938\n",
      "Total Inv Consistency Loss: 1028531.3125\n",
      "Total Temp Consistency Loss: 0.0975809395313263\n",
      "Total Loss: 1028207.8125\n",
      "\n",
      "Total Fwd Loss: 6.746401309967041\n",
      "Total Bwd Loss: 8.779892921447754\n",
      "Total Inv Consistency Loss: 1028192.1875\n",
      "Total Temp Consistency Loss: 0.098802350461483\n",
      "Total Loss: 1027880.375\n",
      "\n",
      "Total Fwd Loss: 7.58349084854126\n",
      "Total Bwd Loss: 7.288252353668213\n",
      "Total Inv Consistency Loss: 1027865.375\n",
      "Total Temp Consistency Loss: 0.09801241755485535\n",
      "Total Loss: 1027555.125\n",
      "\n",
      "Total Fwd Loss: 7.311028957366943\n",
      "Total Bwd Loss: 7.68643856048584\n",
      "Total Inv Consistency Loss: 1027540.0\n",
      "Total Temp Consistency Loss: 0.0979856625199318\n",
      "Total Loss: 1027221.5625\n",
      "\n",
      "Total Fwd Loss: 7.40277099609375\n",
      "Total Bwd Loss: 8.1949462890625\n",
      "Total Inv Consistency Loss: 1027205.8125\n",
      "Total Temp Consistency Loss: 0.09929005801677704\n",
      "Total Loss: 1026895.4375\n",
      "\n",
      "Total Fwd Loss: 8.88341236114502\n",
      "Total Bwd Loss: 8.407608032226562\n",
      "Total Inv Consistency Loss: 1026878.0\n",
      "Total Temp Consistency Loss: 0.09623171389102936\n",
      "----------Training epoch--------\n",
      "----------------49---------------\n",
      "\n",
      "Total Loss: 1026561.375\n",
      "\n",
      "Total Fwd Loss: 7.908758640289307\n",
      "Total Bwd Loss: 7.4687910079956055\n",
      "Total Inv Consistency Loss: 1026545.875\n",
      "Total Temp Consistency Loss: 0.09697748720645905\n",
      "Total Loss: 1026232.8125\n",
      "\n",
      "Total Fwd Loss: 7.247783660888672\n",
      "Total Bwd Loss: 8.568452835083008\n",
      "Total Inv Consistency Loss: 1026216.875\n",
      "Total Temp Consistency Loss: 0.09692979604005814\n",
      "Total Loss: 1025905.75\n",
      "\n",
      "Total Fwd Loss: 8.492940902709961\n",
      "Total Bwd Loss: 7.943442344665527\n",
      "Total Inv Consistency Loss: 1025889.1875\n",
      "Total Temp Consistency Loss: 0.0941096842288971\n",
      "Total Loss: 1025579.8125\n",
      "\n",
      "Total Fwd Loss: 7.659139156341553\n",
      "Total Bwd Loss: 8.66667366027832\n",
      "Total Inv Consistency Loss: 1025563.375\n",
      "Total Temp Consistency Loss: 0.09804260730743408\n",
      "Total Loss: 1025244.3125\n",
      "\n",
      "Total Fwd Loss: 7.7371954917907715\n",
      "Total Bwd Loss: 7.640916347503662\n",
      "Total Inv Consistency Loss: 1025228.8125\n",
      "Total Temp Consistency Loss: 0.09643124788999557\n",
      "Total Loss: 1024918.0625\n",
      "\n",
      "Total Fwd Loss: 8.60222053527832\n",
      "Total Bwd Loss: 8.72607421875\n",
      "Total Inv Consistency Loss: 1024900.6875\n",
      "Total Temp Consistency Loss: 0.09104934334754944\n",
      "----------Training epoch--------\n",
      "----------------50---------------\n",
      "\n",
      "Total Loss: 1024585.75\n",
      "\n",
      "Total Fwd Loss: 8.166570663452148\n",
      "Total Bwd Loss: 8.723453521728516\n",
      "Total Inv Consistency Loss: 1024568.8125\n",
      "Total Temp Consistency Loss: 0.0903637558221817\n",
      "Total Loss: 1024256.375\n",
      "\n",
      "Total Fwd Loss: 7.808982849121094\n",
      "Total Bwd Loss: 9.200925827026367\n",
      "Total Inv Consistency Loss: 1024239.3125\n",
      "Total Temp Consistency Loss: 0.08809982985258102\n",
      "Total Loss: 1023926.125\n",
      "\n",
      "Total Fwd Loss: 7.448214530944824\n",
      "Total Bwd Loss: 7.263474941253662\n",
      "Total Inv Consistency Loss: 1023911.375\n",
      "Total Temp Consistency Loss: 0.08570169657468796\n",
      "Total Loss: 1023598.9375\n",
      "\n",
      "Total Fwd Loss: 8.834173202514648\n",
      "Total Bwd Loss: 8.52235221862793\n",
      "Total Inv Consistency Loss: 1023581.5\n",
      "Total Temp Consistency Loss: 0.08730749785900116\n",
      "Total Loss: 1023262.5\n",
      "\n",
      "Total Fwd Loss: 7.155934810638428\n",
      "Total Bwd Loss: 7.569191932678223\n",
      "Total Inv Consistency Loss: 1023247.6875\n",
      "Total Temp Consistency Loss: 0.08296113461256027\n",
      "Total Loss: 1022945.25\n",
      "\n",
      "Total Fwd Loss: 8.120767593383789\n",
      "Total Bwd Loss: 7.677735805511475\n",
      "Total Inv Consistency Loss: 1022929.375\n",
      "Total Temp Consistency Loss: 0.08160091936588287\n",
      "----------Training epoch--------\n",
      "----------------51---------------\n",
      "\n",
      "Total Loss: 1022613.5\n",
      "\n",
      "Total Fwd Loss: 9.128558158874512\n",
      "Total Bwd Loss: 9.158276557922363\n",
      "Total Inv Consistency Loss: 1022595.125\n",
      "Total Temp Consistency Loss: 0.07604289799928665\n",
      "Total Loss: 1022284.9375\n",
      "\n",
      "Total Fwd Loss: 7.718573570251465\n",
      "Total Bwd Loss: 8.16772747039795\n",
      "Total Inv Consistency Loss: 1022269.0\n",
      "Total Temp Consistency Loss: 0.07724372297525406\n",
      "Total Loss: 1021952.1875\n",
      "\n",
      "Total Fwd Loss: 8.060537338256836\n",
      "Total Bwd Loss: 8.419275283813477\n",
      "Total Inv Consistency Loss: 1021935.625\n",
      "Total Temp Consistency Loss: 0.07708702236413956\n",
      "Total Loss: 1021627.9375\n",
      "\n",
      "Total Fwd Loss: 6.965228080749512\n",
      "Total Bwd Loss: 6.939154624938965\n",
      "Total Inv Consistency Loss: 1021614.0\n",
      "Total Temp Consistency Loss: 0.07288451492786407\n",
      "Total Loss: 1021298.6875\n",
      "\n",
      "Total Fwd Loss: 7.959733009338379\n",
      "Total Bwd Loss: 8.058733940124512\n",
      "Total Inv Consistency Loss: 1021282.625\n",
      "Total Temp Consistency Loss: 0.07598869502544403\n",
      "Total Loss: 1020969.4375\n",
      "\n",
      "Total Fwd Loss: 7.647080898284912\n",
      "Total Bwd Loss: 8.336247444152832\n",
      "Total Inv Consistency Loss: 1020953.375\n",
      "Total Temp Consistency Loss: 0.07167203724384308\n",
      "----------Training epoch--------\n",
      "----------------52---------------\n",
      "\n",
      "Total Loss: 1020633.125\n",
      "\n",
      "Total Fwd Loss: 7.288124084472656\n",
      "Total Bwd Loss: 8.250165939331055\n",
      "Total Inv Consistency Loss: 1020617.5\n",
      "Total Temp Consistency Loss: 0.07206550240516663\n",
      "Total Loss: 1020307.875\n",
      "\n",
      "Total Fwd Loss: 7.994678497314453\n",
      "Total Bwd Loss: 7.949492454528809\n",
      "Total Inv Consistency Loss: 1020291.875\n",
      "Total Temp Consistency Loss: 0.07003429532051086\n",
      "Total Loss: 1019983.4375\n",
      "\n",
      "Total Fwd Loss: 7.661683082580566\n",
      "Total Bwd Loss: 7.875405788421631\n",
      "Total Inv Consistency Loss: 1019967.8125\n",
      "Total Temp Consistency Loss: 0.06781785190105438\n",
      "Total Loss: 1019655.125\n",
      "\n",
      "Total Fwd Loss: 7.571772575378418\n",
      "Total Bwd Loss: 7.839254856109619\n",
      "Total Inv Consistency Loss: 1019639.625\n",
      "Total Temp Consistency Loss: 0.06710286438465118\n",
      "Total Loss: 1019333.4375\n",
      "\n",
      "Total Fwd Loss: 8.17579174041748\n",
      "Total Bwd Loss: 9.084566116333008\n",
      "Total Inv Consistency Loss: 1019316.125\n",
      "Total Temp Consistency Loss: 0.06853032857179642\n",
      "Total Loss: 1018999.75\n",
      "\n",
      "Total Fwd Loss: 8.728315353393555\n",
      "Total Bwd Loss: 8.120169639587402\n",
      "Total Inv Consistency Loss: 1018982.8125\n",
      "Total Temp Consistency Loss: 0.06508255004882812\n",
      "----------Training epoch--------\n",
      "----------------53---------------\n",
      "\n",
      "Total Loss: 1018671.5\n",
      "\n",
      "Total Fwd Loss: 9.726102828979492\n",
      "Total Bwd Loss: 8.847606658935547\n",
      "Total Inv Consistency Loss: 1018652.875\n",
      "Total Temp Consistency Loss: 0.06581660360097885\n",
      "Total Loss: 1018347.5\n",
      "\n",
      "Total Fwd Loss: 7.540261745452881\n",
      "Total Bwd Loss: 7.901941776275635\n",
      "Total Inv Consistency Loss: 1018332.0\n",
      "Total Temp Consistency Loss: 0.06758688390254974\n",
      "Total Loss: 1018014.5625\n",
      "\n",
      "Total Fwd Loss: 6.828634738922119\n",
      "Total Bwd Loss: 7.800987243652344\n",
      "Total Inv Consistency Loss: 1017999.875\n",
      "Total Temp Consistency Loss: 0.0689372718334198\n",
      "Total Loss: 1017690.9375\n",
      "\n",
      "Total Fwd Loss: 8.443143844604492\n",
      "Total Bwd Loss: 7.438528537750244\n",
      "Total Inv Consistency Loss: 1017675.0\n",
      "Total Temp Consistency Loss: 0.07067571580410004\n",
      "Total Loss: 1017367.5625\n",
      "\n",
      "Total Fwd Loss: 7.574606895446777\n",
      "Total Bwd Loss: 8.713417053222656\n",
      "Total Inv Consistency Loss: 1017351.1875\n",
      "Total Temp Consistency Loss: 0.06799042969942093\n",
      "Total Loss: 1017037.1875\n",
      "\n",
      "Total Fwd Loss: 7.384424686431885\n",
      "Total Bwd Loss: 8.269274711608887\n",
      "Total Inv Consistency Loss: 1017021.5\n",
      "Total Temp Consistency Loss: 0.06827440112829208\n",
      "----------Training epoch--------\n",
      "----------------54---------------\n",
      "\n",
      "Total Loss: 1016714.875\n",
      "\n",
      "Total Fwd Loss: 8.123737335205078\n",
      "Total Bwd Loss: 9.206732749938965\n",
      "Total Inv Consistency Loss: 1016697.5\n",
      "Total Temp Consistency Loss: 0.06648382544517517\n",
      "Total Loss: 1016385.875\n",
      "\n",
      "Total Fwd Loss: 8.934418678283691\n",
      "Total Bwd Loss: 8.593116760253906\n",
      "Total Inv Consistency Loss: 1016368.3125\n",
      "Total Temp Consistency Loss: 0.06276225298643112\n",
      "Total Loss: 1016052.6875\n",
      "\n",
      "Total Fwd Loss: 7.987978458404541\n",
      "Total Bwd Loss: 7.941718101501465\n",
      "Total Inv Consistency Loss: 1016036.6875\n",
      "Total Temp Consistency Loss: 0.06267949193716049\n",
      "Total Loss: 1015729.0625\n",
      "\n",
      "Total Fwd Loss: 6.867705345153809\n",
      "Total Bwd Loss: 7.1183953285217285\n",
      "Total Inv Consistency Loss: 1015715.0\n",
      "Total Temp Consistency Loss: 0.0655946359038353\n",
      "Total Loss: 1015400.375\n",
      "\n",
      "Total Fwd Loss: 7.8227643966674805\n",
      "Total Bwd Loss: 8.474847793579102\n",
      "Total Inv Consistency Loss: 1015384.0\n",
      "Total Temp Consistency Loss: 0.07084105908870697\n",
      "Total Loss: 1015077.5625\n",
      "\n",
      "Total Fwd Loss: 7.703830718994141\n",
      "Total Bwd Loss: 7.6098198890686035\n",
      "Total Inv Consistency Loss: 1015062.1875\n",
      "Total Temp Consistency Loss: 0.06586024165153503\n",
      "----------Training epoch--------\n",
      "----------------55---------------\n",
      "\n",
      "Total Loss: 1014750.3125\n",
      "\n",
      "Total Fwd Loss: 7.501871585845947\n",
      "Total Bwd Loss: 8.910627365112305\n",
      "Total Inv Consistency Loss: 1014733.8125\n",
      "Total Temp Consistency Loss: 0.06605198979377747\n",
      "Total Loss: 1014426.0625\n",
      "\n",
      "Total Fwd Loss: 7.4015631675720215\n",
      "Total Bwd Loss: 8.114278793334961\n",
      "Total Inv Consistency Loss: 1014410.5\n",
      "Total Temp Consistency Loss: 0.06564099341630936\n",
      "Total Loss: 1014106.9375\n",
      "\n",
      "Total Fwd Loss: 9.220685958862305\n",
      "Total Bwd Loss: 8.643448829650879\n",
      "Total Inv Consistency Loss: 1014089.0\n",
      "Total Temp Consistency Loss: 0.06539912521839142\n",
      "Total Loss: 1013773.4375\n",
      "\n",
      "Total Fwd Loss: 7.676200866699219\n",
      "Total Bwd Loss: 7.075704097747803\n",
      "Total Inv Consistency Loss: 1013758.625\n",
      "Total Temp Consistency Loss: 0.06907963752746582\n",
      "Total Loss: 1013447.25\n",
      "\n",
      "Total Fwd Loss: 7.851261138916016\n",
      "Total Bwd Loss: 8.200014114379883\n",
      "Total Inv Consistency Loss: 1013431.125\n",
      "Total Temp Consistency Loss: 0.06944885104894638\n",
      "Total Loss: 1013119.75\n",
      "\n",
      "Total Fwd Loss: 7.8252272605896\n",
      "Total Bwd Loss: 7.8828911781311035\n",
      "Total Inv Consistency Loss: 1013104.0\n",
      "Total Temp Consistency Loss: 0.0659039169549942\n",
      "----------Training epoch--------\n",
      "----------------56---------------\n",
      "\n",
      "Total Loss: 1012798.375\n",
      "\n",
      "Total Fwd Loss: 8.057985305786133\n",
      "Total Bwd Loss: 8.737340927124023\n",
      "Total Inv Consistency Loss: 1012781.5\n",
      "Total Temp Consistency Loss: 0.0673692524433136\n",
      "Total Loss: 1012468.5\n",
      "\n",
      "Total Fwd Loss: 9.977027893066406\n",
      "Total Bwd Loss: 9.113390922546387\n",
      "Total Inv Consistency Loss: 1012449.375\n",
      "Total Temp Consistency Loss: 0.06113743782043457\n",
      "Total Loss: 1012144.875\n",
      "\n",
      "Total Fwd Loss: 7.976711273193359\n",
      "Total Bwd Loss: 7.053382873535156\n",
      "Total Inv Consistency Loss: 1012129.8125\n",
      "Total Temp Consistency Loss: 0.062064893543720245\n",
      "Total Loss: 1011817.3125\n",
      "\n",
      "Total Fwd Loss: 7.290109157562256\n",
      "Total Bwd Loss: 8.619941711425781\n",
      "Total Inv Consistency Loss: 1011801.3125\n",
      "Total Temp Consistency Loss: 0.06800250709056854\n",
      "Total Loss: 1011494.0625\n",
      "\n",
      "Total Fwd Loss: 7.5680718421936035\n",
      "Total Bwd Loss: 7.0747199058532715\n",
      "Total Inv Consistency Loss: 1011479.375\n",
      "Total Temp Consistency Loss: 0.0627003088593483\n",
      "Total Loss: 1011168.4375\n",
      "\n",
      "Total Fwd Loss: 6.6736931800842285\n",
      "Total Bwd Loss: 8.230015754699707\n",
      "Total Inv Consistency Loss: 1011153.5\n",
      "Total Temp Consistency Loss: 0.06276213377714157\n",
      "----------Training epoch--------\n",
      "----------------57---------------\n",
      "\n",
      "Total Loss: 1010844.75\n",
      "\n",
      "Total Fwd Loss: 8.559770584106445\n",
      "Total Bwd Loss: 7.928666114807129\n",
      "Total Inv Consistency Loss: 1010828.1875\n",
      "Total Temp Consistency Loss: 0.05994380638003349\n",
      "Total Loss: 1010521.3125\n",
      "\n",
      "Total Fwd Loss: 7.112942695617676\n",
      "Total Bwd Loss: 8.46570873260498\n",
      "Total Inv Consistency Loss: 1010505.6875\n",
      "Total Temp Consistency Loss: 0.06749746948480606\n",
      "Total Loss: 1010197.5\n",
      "\n",
      "Total Fwd Loss: 8.160257339477539\n",
      "Total Bwd Loss: 7.626992225646973\n",
      "Total Inv Consistency Loss: 1010181.625\n",
      "Total Temp Consistency Loss: 0.06285257637500763\n",
      "Total Loss: 1009869.25\n",
      "\n",
      "Total Fwd Loss: 8.16040325164795\n",
      "Total Bwd Loss: 7.334264278411865\n",
      "Total Inv Consistency Loss: 1009853.6875\n",
      "Total Temp Consistency Loss: 0.06089787557721138\n",
      "Total Loss: 1009546.75\n",
      "\n",
      "Total Fwd Loss: 7.4812445640563965\n",
      "Total Bwd Loss: 8.826925277709961\n",
      "Total Inv Consistency Loss: 1009530.375\n",
      "Total Temp Consistency Loss: 0.06186508387327194\n",
      "Total Loss: 1009221.8125\n",
      "\n",
      "Total Fwd Loss: 8.037601470947266\n",
      "Total Bwd Loss: 8.735849380493164\n",
      "Total Inv Consistency Loss: 1009205.0\n",
      "Total Temp Consistency Loss: 0.06216462701559067\n",
      "----------Training epoch--------\n",
      "----------------58---------------\n",
      "\n",
      "Total Loss: 1008898.0625\n",
      "\n",
      "Total Fwd Loss: 8.02846622467041\n",
      "Total Bwd Loss: 8.502264976501465\n",
      "Total Inv Consistency Loss: 1008881.5\n",
      "Total Temp Consistency Loss: 0.061265505850315094\n",
      "Total Loss: 1008570.5\n",
      "\n",
      "Total Fwd Loss: 9.325716018676758\n",
      "Total Bwd Loss: 8.6315279006958\n",
      "Total Inv Consistency Loss: 1008552.5\n",
      "Total Temp Consistency Loss: 0.057743340730667114\n",
      "Total Loss: 1008242.625\n",
      "\n",
      "Total Fwd Loss: 6.904623508453369\n",
      "Total Bwd Loss: 7.6545729637146\n",
      "Total Inv Consistency Loss: 1008228.0\n",
      "Total Temp Consistency Loss: 0.062468864023685455\n",
      "Total Loss: 1007924.9375\n",
      "\n",
      "Total Fwd Loss: 7.082366943359375\n",
      "Total Bwd Loss: 8.67968463897705\n",
      "Total Inv Consistency Loss: 1007909.125\n",
      "Total Temp Consistency Loss: 0.06127433106303215\n",
      "Total Loss: 1007596.1875\n",
      "\n",
      "Total Fwd Loss: 8.214274406433105\n",
      "Total Bwd Loss: 7.229475498199463\n",
      "Total Inv Consistency Loss: 1007580.6875\n",
      "Total Temp Consistency Loss: 0.059658538550138474\n",
      "Total Loss: 1007277.75\n",
      "\n",
      "Total Fwd Loss: 7.8688483238220215\n",
      "Total Bwd Loss: 8.19212818145752\n",
      "Total Inv Consistency Loss: 1007261.625\n",
      "Total Temp Consistency Loss: 0.06057344749569893\n",
      "----------Training epoch--------\n",
      "----------------59---------------\n",
      "\n",
      "Total Loss: 1006954.1875\n",
      "\n",
      "Total Fwd Loss: 6.322297096252441\n",
      "Total Bwd Loss: 8.203725814819336\n",
      "Total Inv Consistency Loss: 1006939.625\n",
      "Total Temp Consistency Loss: 0.06366029381752014\n",
      "Total Loss: 1006627.8125\n",
      "\n",
      "Total Fwd Loss: 7.615324974060059\n",
      "Total Bwd Loss: 7.938784599304199\n",
      "Total Inv Consistency Loss: 1006612.1875\n",
      "Total Temp Consistency Loss: 0.060222018510103226\n",
      "Total Loss: 1006302.0\n",
      "\n",
      "Total Fwd Loss: 8.265472412109375\n",
      "Total Bwd Loss: 8.782431602478027\n",
      "Total Inv Consistency Loss: 1006284.875\n",
      "Total Temp Consistency Loss: 0.05778472498059273\n",
      "Total Loss: 1005989.25\n",
      "\n",
      "Total Fwd Loss: 8.973043441772461\n",
      "Total Bwd Loss: 8.38402271270752\n",
      "Total Inv Consistency Loss: 1005971.8125\n",
      "Total Temp Consistency Loss: 0.05662336200475693\n",
      "Total Loss: 1005656.25\n",
      "\n",
      "Total Fwd Loss: 9.069589614868164\n",
      "Total Bwd Loss: 8.503003120422363\n",
      "Total Inv Consistency Loss: 1005638.625\n",
      "Total Temp Consistency Loss: 0.055294014513492584\n",
      "Total Loss: 1005332.1875\n",
      "\n",
      "Total Fwd Loss: 7.2937116622924805\n",
      "Total Bwd Loss: 6.990777492523193\n",
      "Total Inv Consistency Loss: 1005317.8125\n",
      "Total Temp Consistency Loss: 0.05605096369981766\n",
      "----------Training epoch--------\n",
      "----------------60---------------\n",
      "\n",
      "Total Loss: 1005010.0625\n",
      "\n",
      "Total Fwd Loss: 6.592934608459473\n",
      "Total Bwd Loss: 8.302610397338867\n",
      "Total Inv Consistency Loss: 1004995.125\n",
      "Total Temp Consistency Loss: 0.05836843326687813\n",
      "Total Loss: 1004685.5625\n",
      "\n",
      "Total Fwd Loss: 7.737083435058594\n",
      "Total Bwd Loss: 8.259779930114746\n",
      "Total Inv Consistency Loss: 1004669.5\n",
      "Total Temp Consistency Loss: 0.05767161771655083\n",
      "Total Loss: 1004367.5\n",
      "\n",
      "Total Fwd Loss: 7.734248161315918\n",
      "Total Bwd Loss: 7.865038871765137\n",
      "Total Inv Consistency Loss: 1004351.8125\n",
      "Total Temp Consistency Loss: 0.057434119284152985\n",
      "Total Loss: 1004043.375\n",
      "\n",
      "Total Fwd Loss: 7.8312249183654785\n",
      "Total Bwd Loss: 8.099096298217773\n",
      "Total Inv Consistency Loss: 1004027.375\n",
      "Total Temp Consistency Loss: 0.06133502721786499\n",
      "Total Loss: 1003726.5\n",
      "\n",
      "Total Fwd Loss: 9.076164245605469\n",
      "Total Bwd Loss: 8.160844802856445\n",
      "Total Inv Consistency Loss: 1003709.1875\n",
      "Total Temp Consistency Loss: 0.05507843941450119\n",
      "Total Loss: 1003402.0\n",
      "\n",
      "Total Fwd Loss: 8.673921585083008\n",
      "Total Bwd Loss: 8.067885398864746\n",
      "Total Inv Consistency Loss: 1003385.1875\n",
      "Total Temp Consistency Loss: 0.05502300709486008\n",
      "----------Training epoch--------\n",
      "----------------61---------------\n",
      "\n",
      "Total Loss: 1003076.9375\n",
      "\n",
      "Total Fwd Loss: 6.715516567230225\n",
      "Total Bwd Loss: 7.021834373474121\n",
      "Total Inv Consistency Loss: 1003063.125\n",
      "Total Temp Consistency Loss: 0.05382735654711723\n",
      "Total Loss: 1002754.8125\n",
      "\n",
      "Total Fwd Loss: 8.125465393066406\n",
      "Total Bwd Loss: 7.999541282653809\n",
      "Total Inv Consistency Loss: 1002738.625\n",
      "Total Temp Consistency Loss: 0.054948098957538605\n",
      "Total Loss: 1002434.125\n",
      "\n",
      "Total Fwd Loss: 8.308989524841309\n",
      "Total Bwd Loss: 8.728137016296387\n",
      "Total Inv Consistency Loss: 1002417.0\n",
      "Total Temp Consistency Loss: 0.054349303245544434\n",
      "Total Loss: 1002112.5625\n",
      "\n",
      "Total Fwd Loss: 7.386788368225098\n",
      "Total Bwd Loss: 8.279472351074219\n",
      "Total Inv Consistency Loss: 1002096.8125\n",
      "Total Temp Consistency Loss: 0.05736061930656433\n",
      "Total Loss: 1001788.0\n",
      "\n",
      "Total Fwd Loss: 7.40399169921875\n",
      "Total Bwd Loss: 8.816354751586914\n",
      "Total Inv Consistency Loss: 1001771.6875\n",
      "Total Temp Consistency Loss: 0.05819069594144821\n",
      "Total Loss: 1001466.8125\n",
      "\n",
      "Total Fwd Loss: 9.427736282348633\n",
      "Total Bwd Loss: 7.978466987609863\n",
      "Total Inv Consistency Loss: 1001449.375\n",
      "Total Temp Consistency Loss: 0.0536595955491066\n",
      "----------Training epoch--------\n",
      "----------------62---------------\n",
      "\n",
      "Total Loss: 1001146.3125\n",
      "\n",
      "Total Fwd Loss: 9.587514877319336\n",
      "Total Bwd Loss: 9.273648262023926\n",
      "Total Inv Consistency Loss: 1001127.375\n",
      "Total Temp Consistency Loss: 0.05614090710878372\n",
      "Total Loss: 1000816.5625\n",
      "\n",
      "Total Fwd Loss: 7.812049865722656\n",
      "Total Bwd Loss: 7.204458713531494\n",
      "Total Inv Consistency Loss: 1000801.5\n",
      "Total Temp Consistency Loss: 0.05662854388356209\n",
      "Total Loss: 1000497.875\n",
      "\n",
      "Total Fwd Loss: 8.14796257019043\n",
      "Total Bwd Loss: 8.137128829956055\n",
      "Total Inv Consistency Loss: 1000481.5\n",
      "Total Temp Consistency Loss: 0.055822841823101044\n",
      "Total Loss: 1000177.9375\n",
      "\n",
      "Total Fwd Loss: 7.290445804595947\n",
      "Total Bwd Loss: 8.185500144958496\n",
      "Total Inv Consistency Loss: 1000162.375\n",
      "Total Temp Consistency Loss: 0.05547980219125748\n",
      "Total Loss: 999856.5\n",
      "\n",
      "Total Fwd Loss: 6.450106143951416\n",
      "Total Bwd Loss: 7.514260768890381\n",
      "Total Inv Consistency Loss: 999842.5\n",
      "Total Temp Consistency Loss: 0.05777110904455185\n",
      "Total Loss: 999538.25\n",
      "\n",
      "Total Fwd Loss: 8.114418029785156\n",
      "Total Bwd Loss: 8.73809814453125\n",
      "Total Inv Consistency Loss: 999521.3125\n",
      "Total Temp Consistency Loss: 0.053927231580019\n",
      "----------Training epoch--------\n",
      "----------------63---------------\n",
      "\n",
      "Total Loss: 999213.0\n",
      "\n",
      "Total Fwd Loss: 7.376233100891113\n",
      "Total Bwd Loss: 7.775418281555176\n",
      "Total Inv Consistency Loss: 999197.8125\n",
      "Total Temp Consistency Loss: 0.053874898701906204\n",
      "Total Loss: 998890.9375\n",
      "\n",
      "Total Fwd Loss: 7.512972831726074\n",
      "Total Bwd Loss: 8.048437118530273\n",
      "Total Inv Consistency Loss: 998875.3125\n",
      "Total Temp Consistency Loss: 0.05264986678957939\n",
      "Total Loss: 998577.3125\n",
      "\n",
      "Total Fwd Loss: 8.960832595825195\n",
      "Total Bwd Loss: 7.9579644203186035\n",
      "Total Inv Consistency Loss: 998560.3125\n",
      "Total Temp Consistency Loss: 0.05149479955434799\n",
      "Total Loss: 998248.5625\n",
      "\n",
      "Total Fwd Loss: 7.949352264404297\n",
      "Total Bwd Loss: 7.870550632476807\n",
      "Total Inv Consistency Loss: 998232.6875\n",
      "Total Temp Consistency Loss: 0.05235855653882027\n",
      "Total Loss: 997926.0625\n",
      "\n",
      "Total Fwd Loss: 7.247367858886719\n",
      "Total Bwd Loss: 7.404123783111572\n",
      "Total Inv Consistency Loss: 997911.375\n",
      "Total Temp Consistency Loss: 0.05014817789196968\n",
      "Total Loss: 997608.3125\n",
      "\n",
      "Total Fwd Loss: 8.440771102905273\n",
      "Total Bwd Loss: 9.969629287719727\n",
      "Total Inv Consistency Loss: 997589.8125\n",
      "Total Temp Consistency Loss: 0.05398472398519516\n",
      "----------Training epoch--------\n",
      "----------------64---------------\n",
      "\n",
      "Total Loss: 997289.0\n",
      "\n",
      "Total Fwd Loss: 7.457341194152832\n",
      "Total Bwd Loss: 7.318804740905762\n",
      "Total Inv Consistency Loss: 997274.1875\n",
      "Total Temp Consistency Loss: 0.051994241774082184\n",
      "Total Loss: 996968.8125\n",
      "\n",
      "Total Fwd Loss: 8.393576622009277\n",
      "Total Bwd Loss: 7.988704681396484\n",
      "Total Inv Consistency Loss: 996952.375\n",
      "Total Temp Consistency Loss: 0.05263111740350723\n",
      "Total Loss: 996645.75\n",
      "\n",
      "Total Fwd Loss: 8.019415855407715\n",
      "Total Bwd Loss: 8.766538619995117\n",
      "Total Inv Consistency Loss: 996628.875\n",
      "Total Temp Consistency Loss: 0.05414435267448425\n",
      "Total Loss: 996328.8125\n",
      "\n",
      "Total Fwd Loss: 6.577471733093262\n",
      "Total Bwd Loss: 8.349492073059082\n",
      "Total Inv Consistency Loss: 996313.8125\n",
      "Total Temp Consistency Loss: 0.054418571293354034\n",
      "Total Loss: 996014.0\n",
      "\n",
      "Total Fwd Loss: 9.63925838470459\n",
      "Total Bwd Loss: 7.9839582443237305\n",
      "Total Inv Consistency Loss: 995996.3125\n",
      "Total Temp Consistency Loss: 0.049108780920505524\n",
      "Total Loss: 995688.1875\n",
      "\n",
      "Total Fwd Loss: 7.493866920471191\n",
      "Total Bwd Loss: 8.319034576416016\n",
      "Total Inv Consistency Loss: 995672.3125\n",
      "Total Temp Consistency Loss: 0.05359341949224472\n",
      "----------Training epoch--------\n",
      "----------------65---------------\n",
      "\n",
      "Total Loss: 995368.0625\n",
      "\n",
      "Total Fwd Loss: 7.692507743835449\n",
      "Total Bwd Loss: 7.904321193695068\n",
      "Total Inv Consistency Loss: 995352.375\n",
      "Total Temp Consistency Loss: 0.05075615644454956\n",
      "Total Loss: 995050.375\n",
      "\n",
      "Total Fwd Loss: 8.52908706665039\n",
      "Total Bwd Loss: 8.982096672058105\n",
      "Total Inv Consistency Loss: 995032.8125\n",
      "Total Temp Consistency Loss: 0.05114530771970749\n",
      "Total Loss: 994730.0625\n",
      "\n",
      "Total Fwd Loss: 8.523730278015137\n",
      "Total Bwd Loss: 7.818425178527832\n",
      "Total Inv Consistency Loss: 994713.6875\n",
      "Total Temp Consistency Loss: 0.048205338418483734\n",
      "Total Loss: 994406.0625\n",
      "\n",
      "Total Fwd Loss: 7.3180060386657715\n",
      "Total Bwd Loss: 7.69223165512085\n",
      "Total Inv Consistency Loss: 994391.0\n",
      "Total Temp Consistency Loss: 0.04912767559289932\n",
      "Total Loss: 994092.5\n",
      "\n",
      "Total Fwd Loss: 7.949917793273926\n",
      "Total Bwd Loss: 7.7854156494140625\n",
      "Total Inv Consistency Loss: 994076.6875\n",
      "Total Temp Consistency Loss: 0.049073148518800735\n",
      "Total Loss: 993771.6875\n",
      "\n",
      "Total Fwd Loss: 7.458099365234375\n",
      "Total Bwd Loss: 8.6575927734375\n",
      "Total Inv Consistency Loss: 993755.5\n",
      "Total Temp Consistency Loss: 0.051201529800891876\n",
      "----------Training epoch--------\n",
      "----------------66---------------\n",
      "\n",
      "Total Loss: 993447.25\n",
      "\n",
      "Total Fwd Loss: 7.25149393081665\n",
      "Total Bwd Loss: 7.623149871826172\n",
      "Total Inv Consistency Loss: 993432.3125\n",
      "Total Temp Consistency Loss: 0.04951167851686478\n",
      "Total Loss: 993130.5\n",
      "\n",
      "Total Fwd Loss: 7.8723554611206055\n",
      "Total Bwd Loss: 9.09508991241455\n",
      "Total Inv Consistency Loss: 993113.5\n",
      "Total Temp Consistency Loss: 0.050844140350818634\n",
      "Total Loss: 992814.25\n",
      "\n",
      "Total Fwd Loss: 7.789886474609375\n",
      "Total Bwd Loss: 7.89572286605835\n",
      "Total Inv Consistency Loss: 992798.5\n",
      "Total Temp Consistency Loss: 0.04836588352918625\n",
      "Total Loss: 992496.75\n",
      "\n",
      "Total Fwd Loss: 8.030416488647461\n",
      "Total Bwd Loss: 8.46116828918457\n",
      "Total Inv Consistency Loss: 992480.1875\n",
      "Total Temp Consistency Loss: 0.04919387400150299\n",
      "Total Loss: 992171.4375\n",
      "\n",
      "Total Fwd Loss: 7.155938148498535\n",
      "Total Bwd Loss: 7.555442810058594\n",
      "Total Inv Consistency Loss: 992156.6875\n",
      "Total Temp Consistency Loss: 0.047999653965234756\n",
      "Total Loss: 991855.875\n",
      "\n",
      "Total Fwd Loss: 9.22661018371582\n",
      "Total Bwd Loss: 8.36774730682373\n",
      "Total Inv Consistency Loss: 991838.1875\n",
      "Total Temp Consistency Loss: 0.0475383922457695\n",
      "----------Training epoch--------\n",
      "----------------67---------------\n",
      "\n",
      "Total Loss: 991535.9375\n",
      "\n",
      "Total Fwd Loss: 7.558279991149902\n",
      "Total Bwd Loss: 7.829283714294434\n",
      "Total Inv Consistency Loss: 991520.5\n",
      "Total Temp Consistency Loss: 0.04820690676569939\n",
      "Total Loss: 991223.625\n",
      "\n",
      "Total Fwd Loss: 9.834525108337402\n",
      "Total Bwd Loss: 9.420766830444336\n",
      "Total Inv Consistency Loss: 991204.3125\n",
      "Total Temp Consistency Loss: 0.046548809856176376\n",
      "Total Loss: 990900.3125\n",
      "\n",
      "Total Fwd Loss: 7.858705043792725\n",
      "Total Bwd Loss: 8.758512496948242\n",
      "Total Inv Consistency Loss: 990883.625\n",
      "Total Temp Consistency Loss: 0.04903373494744301\n",
      "Total Loss: 990571.625\n",
      "\n",
      "Total Fwd Loss: 7.573516845703125\n",
      "Total Bwd Loss: 6.316169261932373\n",
      "Total Inv Consistency Loss: 990557.6875\n",
      "Total Temp Consistency Loss: 0.046052150428295135\n",
      "Total Loss: 990259.5625\n",
      "\n",
      "Total Fwd Loss: 7.656508445739746\n",
      "Total Bwd Loss: 8.488082885742188\n",
      "Total Inv Consistency Loss: 990243.375\n",
      "Total Temp Consistency Loss: 0.047856900840997696\n",
      "Total Loss: 989941.1875\n",
      "\n",
      "Total Fwd Loss: 6.79034423828125\n",
      "Total Bwd Loss: 8.220468521118164\n",
      "Total Inv Consistency Loss: 989926.125\n",
      "Total Temp Consistency Loss: 0.04647339507937431\n",
      "----------Training epoch--------\n",
      "----------------68---------------\n",
      "\n",
      "Total Loss: 989626.125\n",
      "\n",
      "Total Fwd Loss: 7.8453779220581055\n",
      "Total Bwd Loss: 7.372760772705078\n",
      "Total Inv Consistency Loss: 989610.875\n",
      "Total Temp Consistency Loss: 0.04640214145183563\n",
      "Total Loss: 989305.1875\n",
      "\n",
      "Total Fwd Loss: 7.987928867340088\n",
      "Total Bwd Loss: 8.027403831481934\n",
      "Total Inv Consistency Loss: 989289.125\n",
      "Total Temp Consistency Loss: 0.04763568937778473\n",
      "Total Loss: 988985.125\n",
      "\n",
      "Total Fwd Loss: 8.052950859069824\n",
      "Total Bwd Loss: 7.633488655090332\n",
      "Total Inv Consistency Loss: 988969.375\n",
      "Total Temp Consistency Loss: 0.044295623898506165\n",
      "Total Loss: 988676.0\n",
      "\n",
      "Total Fwd Loss: 8.73746395111084\n",
      "Total Bwd Loss: 8.293983459472656\n",
      "Total Inv Consistency Loss: 988658.875\n",
      "Total Temp Consistency Loss: 0.043792061507701874\n",
      "Total Loss: 988353.6875\n",
      "\n",
      "Total Fwd Loss: 7.689138889312744\n",
      "Total Bwd Loss: 9.255002975463867\n",
      "Total Inv Consistency Loss: 988336.6875\n",
      "Total Temp Consistency Loss: 0.04692201316356659\n",
      "Total Loss: 988030.9375\n",
      "\n",
      "Total Fwd Loss: 6.9582624435424805\n",
      "Total Bwd Loss: 8.413084030151367\n",
      "Total Inv Consistency Loss: 988015.5\n",
      "Total Temp Consistency Loss: 0.047984711825847626\n",
      "----------Training epoch--------\n",
      "----------------69---------------\n",
      "\n",
      "Total Loss: 987718.375\n",
      "\n",
      "Total Fwd Loss: 8.333940505981445\n",
      "Total Bwd Loss: 10.093435287475586\n",
      "Total Inv Consistency Loss: 987699.875\n",
      "Total Temp Consistency Loss: 0.046564750373363495\n",
      "Total Loss: 987406.5\n",
      "\n",
      "Total Fwd Loss: 7.8281073570251465\n",
      "Total Bwd Loss: 7.304808139801025\n",
      "Total Inv Consistency Loss: 987391.3125\n",
      "Total Temp Consistency Loss: 0.04503185674548149\n",
      "Total Loss: 987081.0\n",
      "\n",
      "Total Fwd Loss: 7.486382484436035\n",
      "Total Bwd Loss: 7.816980838775635\n",
      "Total Inv Consistency Loss: 987065.625\n",
      "Total Temp Consistency Loss: 0.050277452915906906\n",
      "Total Loss: 986766.6875\n",
      "\n",
      "Total Fwd Loss: 7.228442192077637\n",
      "Total Bwd Loss: 7.68630838394165\n",
      "Total Inv Consistency Loss: 986751.6875\n",
      "Total Temp Consistency Loss: 0.047807879745960236\n",
      "Total Loss: 986447.3125\n",
      "\n",
      "Total Fwd Loss: 7.789228916168213\n",
      "Total Bwd Loss: 8.148897171020508\n",
      "Total Inv Consistency Loss: 986431.3125\n",
      "Total Temp Consistency Loss: 0.04499340429902077\n",
      "Total Loss: 986130.8125\n",
      "\n",
      "Total Fwd Loss: 8.830191612243652\n",
      "Total Bwd Loss: 7.7174248695373535\n",
      "Total Inv Consistency Loss: 986114.1875\n",
      "Total Temp Consistency Loss: 0.04389285668730736\n",
      "----------Training epoch--------\n",
      "----------------70---------------\n",
      "\n",
      "Total Loss: 985813.0\n",
      "\n",
      "Total Fwd Loss: 7.794574737548828\n",
      "Total Bwd Loss: 7.7702436447143555\n",
      "Total Inv Consistency Loss: 985797.375\n",
      "Total Temp Consistency Loss: 0.043460194021463394\n",
      "Total Loss: 985499.125\n",
      "\n",
      "Total Fwd Loss: 7.121823310852051\n",
      "Total Bwd Loss: 7.631983757019043\n",
      "Total Inv Consistency Loss: 985484.3125\n",
      "Total Temp Consistency Loss: 0.041511133313179016\n",
      "Total Loss: 985179.9375\n",
      "\n",
      "Total Fwd Loss: 7.510054111480713\n",
      "Total Bwd Loss: 7.664466857910156\n",
      "Total Inv Consistency Loss: 985164.6875\n",
      "Total Temp Consistency Loss: 0.04432472959160805\n",
      "Total Loss: 984860.3125\n",
      "\n",
      "Total Fwd Loss: 9.321638107299805\n",
      "Total Bwd Loss: 8.276124954223633\n",
      "Total Inv Consistency Loss: 984842.625\n",
      "Total Temp Consistency Loss: 0.040728382766246796\n",
      "Total Loss: 984544.4375\n",
      "\n",
      "Total Fwd Loss: 7.615314483642578\n",
      "Total Bwd Loss: 8.360681533813477\n",
      "Total Inv Consistency Loss: 984528.375\n",
      "Total Temp Consistency Loss: 0.04555195942521095\n",
      "Total Loss: 984227.1875\n",
      "\n",
      "Total Fwd Loss: 8.122137069702148\n",
      "Total Bwd Loss: 9.134008407592773\n",
      "Total Inv Consistency Loss: 984209.875\n",
      "Total Temp Consistency Loss: 0.04618431255221367\n",
      "----------Training epoch--------\n",
      "----------------71---------------\n",
      "\n",
      "Total Loss: 983912.5625\n",
      "\n",
      "Total Fwd Loss: 7.1057610511779785\n",
      "Total Bwd Loss: 7.505138397216797\n",
      "Total Inv Consistency Loss: 983897.875\n",
      "Total Temp Consistency Loss: 0.04642755165696144\n",
      "Total Loss: 983598.0\n",
      "\n",
      "Total Fwd Loss: 6.915564060211182\n",
      "Total Bwd Loss: 7.36617374420166\n",
      "Total Inv Consistency Loss: 983583.625\n",
      "Total Temp Consistency Loss: 0.043201349675655365\n",
      "Total Loss: 983278.0\n",
      "\n",
      "Total Fwd Loss: 8.880480766296387\n",
      "Total Bwd Loss: 8.701595306396484\n",
      "Total Inv Consistency Loss: 983260.375\n",
      "Total Temp Consistency Loss: 0.04624304920434952\n",
      "Total Loss: 982963.125\n",
      "\n",
      "Total Fwd Loss: 7.763476371765137\n",
      "Total Bwd Loss: 8.086397171020508\n",
      "Total Inv Consistency Loss: 982947.1875\n",
      "Total Temp Consistency Loss: 0.0443229153752327\n",
      "Total Loss: 982654.25\n",
      "\n",
      "Total Fwd Loss: 9.168819427490234\n",
      "Total Bwd Loss: 9.115816116333008\n",
      "Total Inv Consistency Loss: 982635.875\n",
      "Total Temp Consistency Loss: 0.04201827198266983\n",
      "Total Loss: 982333.1875\n",
      "\n",
      "Total Fwd Loss: 7.668303489685059\n",
      "Total Bwd Loss: 8.07175350189209\n",
      "Total Inv Consistency Loss: 982317.375\n",
      "Total Temp Consistency Loss: 0.044379353523254395\n",
      "----------Training epoch--------\n",
      "----------------72---------------\n",
      "\n",
      "Total Loss: 982018.75\n",
      "\n",
      "Total Fwd Loss: 7.466615200042725\n",
      "Total Bwd Loss: 8.888026237487793\n",
      "Total Inv Consistency Loss: 982002.3125\n",
      "Total Temp Consistency Loss: 0.04232820123434067\n",
      "Total Loss: 981709.75\n",
      "\n",
      "Total Fwd Loss: 8.474122047424316\n",
      "Total Bwd Loss: 8.04321002960205\n",
      "Total Inv Consistency Loss: 981693.1875\n",
      "Total Temp Consistency Loss: 0.03945896774530411\n",
      "Total Loss: 981389.0625\n",
      "\n",
      "Total Fwd Loss: 7.429726600646973\n",
      "Total Bwd Loss: 7.2006659507751465\n",
      "Total Inv Consistency Loss: 981374.375\n",
      "Total Temp Consistency Loss: 0.04171224683523178\n",
      "Total Loss: 981069.6875\n",
      "\n",
      "Total Fwd Loss: 7.028162956237793\n",
      "Total Bwd Loss: 8.087202072143555\n",
      "Total Inv Consistency Loss: 981054.5\n",
      "Total Temp Consistency Loss: 0.042102377861738205\n",
      "Total Loss: 980755.4375\n",
      "\n",
      "Total Fwd Loss: 7.907586097717285\n",
      "Total Bwd Loss: 8.804852485656738\n",
      "Total Inv Consistency Loss: 980738.6875\n",
      "Total Temp Consistency Loss: 0.045464493334293365\n",
      "Total Loss: 980443.4375\n",
      "\n",
      "Total Fwd Loss: 9.101858139038086\n",
      "Total Bwd Loss: 7.778491973876953\n",
      "Total Inv Consistency Loss: 980426.5\n",
      "Total Temp Consistency Loss: 0.03983919695019722\n",
      "----------Training epoch--------\n",
      "----------------73---------------\n",
      "\n",
      "Total Loss: 980121.1875\n",
      "\n",
      "Total Fwd Loss: 7.547224998474121\n",
      "Total Bwd Loss: 8.060344696044922\n",
      "Total Inv Consistency Loss: 980105.5\n",
      "Total Temp Consistency Loss: 0.041569992899894714\n",
      "Total Loss: 979814.5\n",
      "\n",
      "Total Fwd Loss: 7.737565517425537\n",
      "Total Bwd Loss: 7.680612087249756\n",
      "Total Inv Consistency Loss: 979799.0\n",
      "Total Temp Consistency Loss: 0.040325406938791275\n",
      "Total Loss: 979493.5\n",
      "\n",
      "Total Fwd Loss: 8.715319633483887\n",
      "Total Bwd Loss: 7.822522163391113\n",
      "Total Inv Consistency Loss: 979476.875\n",
      "Total Temp Consistency Loss: 0.0394420400261879\n",
      "Total Loss: 979179.625\n",
      "\n",
      "Total Fwd Loss: 7.460495948791504\n",
      "Total Bwd Loss: 8.394248962402344\n",
      "Total Inv Consistency Loss: 979163.6875\n",
      "Total Temp Consistency Loss: 0.042793743312358856\n",
      "Total Loss: 978867.8125\n",
      "\n",
      "Total Fwd Loss: 8.366955757141113\n",
      "Total Bwd Loss: 8.534953117370605\n",
      "Total Inv Consistency Loss: 978850.875\n",
      "Total Temp Consistency Loss: 0.03997776657342911\n",
      "Total Loss: 978551.5625\n",
      "\n",
      "Total Fwd Loss: 7.442877292633057\n",
      "Total Bwd Loss: 8.388360977172852\n",
      "Total Inv Consistency Loss: 978535.6875\n",
      "Total Temp Consistency Loss: 0.04049130901694298\n",
      "----------Training epoch--------\n",
      "----------------74---------------\n",
      "\n",
      "Total Loss: 978237.6875\n",
      "\n",
      "Total Fwd Loss: 9.072994232177734\n",
      "Total Bwd Loss: 8.581116676330566\n",
      "Total Inv Consistency Loss: 978220.0\n",
      "Total Temp Consistency Loss: 0.03898965194821358\n",
      "Total Loss: 977924.875\n",
      "\n",
      "Total Fwd Loss: 8.06192684173584\n",
      "Total Bwd Loss: 7.2340407371521\n",
      "Total Inv Consistency Loss: 977909.5\n",
      "Total Temp Consistency Loss: 0.03991932049393654\n",
      "Total Loss: 977609.1875\n",
      "\n",
      "Total Fwd Loss: 8.33873176574707\n",
      "Total Bwd Loss: 7.916192054748535\n",
      "Total Inv Consistency Loss: 977592.875\n",
      "Total Temp Consistency Loss: 0.03979482129216194\n",
      "Total Loss: 977296.0625\n",
      "\n",
      "Total Fwd Loss: 7.039072513580322\n",
      "Total Bwd Loss: 8.58393669128418\n",
      "Total Inv Consistency Loss: 977280.375\n",
      "Total Temp Consistency Loss: 0.040971510112285614\n",
      "Total Loss: 976978.875\n",
      "\n",
      "Total Fwd Loss: 7.745358467102051\n",
      "Total Bwd Loss: 8.74747085571289\n",
      "Total Inv Consistency Loss: 976962.3125\n",
      "Total Temp Consistency Loss: 0.039926376193761826\n",
      "Total Loss: 976669.625\n",
      "\n",
      "Total Fwd Loss: 6.9027509689331055\n",
      "Total Bwd Loss: 8.014266014099121\n",
      "Total Inv Consistency Loss: 976654.625\n",
      "Total Temp Consistency Loss: 0.039860911667346954\n",
      "----------Training epoch--------\n",
      "----------------75---------------\n",
      "\n",
      "Total Loss: 976356.0\n",
      "\n",
      "Total Fwd Loss: 7.857402801513672\n",
      "Total Bwd Loss: 8.935140609741211\n",
      "Total Inv Consistency Loss: 976339.125\n",
      "Total Temp Consistency Loss: 0.04108453541994095\n",
      "Total Loss: 976039.5625\n",
      "\n",
      "Total Fwd Loss: 7.322312355041504\n",
      "Total Bwd Loss: 7.318236351013184\n",
      "Total Inv Consistency Loss: 976024.875\n",
      "Total Temp Consistency Loss: 0.039437200874090195\n",
      "Total Loss: 975722.9375\n",
      "\n",
      "Total Fwd Loss: 8.197588920593262\n",
      "Total Bwd Loss: 7.850988864898682\n",
      "Total Inv Consistency Loss: 975706.8125\n",
      "Total Temp Consistency Loss: 0.03859510272741318\n",
      "Total Loss: 975415.0\n",
      "\n",
      "Total Fwd Loss: 7.798418998718262\n",
      "Total Bwd Loss: 8.357816696166992\n",
      "Total Inv Consistency Loss: 975398.8125\n",
      "Total Temp Consistency Loss: 0.04043110832571983\n",
      "Total Loss: 975094.625\n",
      "\n",
      "Total Fwd Loss: 6.466947078704834\n",
      "Total Bwd Loss: 7.247957706451416\n",
      "Total Inv Consistency Loss: 975080.875\n",
      "Total Temp Consistency Loss: 0.03919532150030136\n",
      "Total Loss: 974782.6875\n",
      "\n",
      "Total Fwd Loss: 9.816692352294922\n",
      "Total Bwd Loss: 9.310247421264648\n",
      "Total Inv Consistency Loss: 974763.5\n",
      "Total Temp Consistency Loss: 0.036864977329969406\n",
      "----------Training epoch--------\n",
      "----------------76---------------\n",
      "\n",
      "Total Loss: 974469.4375\n",
      "\n",
      "Total Fwd Loss: 7.210309028625488\n",
      "Total Bwd Loss: 8.448010444641113\n",
      "Total Inv Consistency Loss: 974453.6875\n",
      "Total Temp Consistency Loss: 0.03827828913927078\n",
      "Total Loss: 974161.4375\n",
      "\n",
      "Total Fwd Loss: 7.160015106201172\n",
      "Total Bwd Loss: 8.32404899597168\n",
      "Total Inv Consistency Loss: 974145.875\n",
      "Total Temp Consistency Loss: 0.03754749149084091\n",
      "Total Loss: 973844.375\n",
      "\n",
      "Total Fwd Loss: 7.5106072425842285\n",
      "Total Bwd Loss: 7.703257083892822\n",
      "Total Inv Consistency Loss: 973829.125\n",
      "Total Temp Consistency Loss: 0.037053655833005905\n",
      "Total Loss: 973533.75\n",
      "\n",
      "Total Fwd Loss: 7.649697303771973\n",
      "Total Bwd Loss: 7.723690032958984\n",
      "Total Inv Consistency Loss: 973518.3125\n",
      "Total Temp Consistency Loss: 0.038097284734249115\n",
      "Total Loss: 973224.625\n",
      "\n",
      "Total Fwd Loss: 10.208709716796875\n",
      "Total Bwd Loss: 9.506200790405273\n",
      "Total Inv Consistency Loss: 973204.875\n",
      "Total Temp Consistency Loss: 0.038416750729084015\n",
      "Total Loss: 972903.625\n",
      "\n",
      "Total Fwd Loss: 7.822722434997559\n",
      "Total Bwd Loss: 7.099179267883301\n",
      "Total Inv Consistency Loss: 972888.625\n",
      "Total Temp Consistency Loss: 0.03774245083332062\n",
      "----------Training epoch--------\n",
      "----------------77---------------\n",
      "\n",
      "Total Loss: 972589.0\n",
      "\n",
      "Total Fwd Loss: 7.501797676086426\n",
      "Total Bwd Loss: 8.033745765686035\n",
      "Total Inv Consistency Loss: 972573.375\n",
      "Total Temp Consistency Loss: 0.039679236710071564\n",
      "Total Loss: 972276.4375\n",
      "\n",
      "Total Fwd Loss: 7.130553245544434\n",
      "Total Bwd Loss: 7.3939666748046875\n",
      "Total Inv Consistency Loss: 972261.875\n",
      "Total Temp Consistency Loss: 0.03641541674733162\n",
      "Total Loss: 971968.875\n",
      "\n",
      "Total Fwd Loss: 7.138670444488525\n",
      "Total Bwd Loss: 7.880091667175293\n",
      "Total Inv Consistency Loss: 971953.8125\n",
      "Total Temp Consistency Loss: 0.03822707384824753\n",
      "Total Loss: 971652.0\n",
      "\n",
      "Total Fwd Loss: 9.024961471557617\n",
      "Total Bwd Loss: 9.123220443725586\n",
      "Total Inv Consistency Loss: 971633.8125\n",
      "Total Temp Consistency Loss: 0.036623843014240265\n",
      "Total Loss: 971346.8125\n",
      "\n",
      "Total Fwd Loss: 8.166962623596191\n",
      "Total Bwd Loss: 8.236733436584473\n",
      "Total Inv Consistency Loss: 971330.375\n",
      "Total Temp Consistency Loss: 0.03793885186314583\n",
      "Total Loss: 971034.8125\n",
      "\n",
      "Total Fwd Loss: 8.470035552978516\n",
      "Total Bwd Loss: 8.185041427612305\n",
      "Total Inv Consistency Loss: 971018.125\n",
      "Total Temp Consistency Loss: 0.03612493723630905\n",
      "----------Training epoch--------\n",
      "----------------78---------------\n",
      "\n",
      "Total Loss: 970717.4375\n",
      "\n",
      "Total Fwd Loss: 7.323277473449707\n",
      "Total Bwd Loss: 8.416348457336426\n",
      "Total Inv Consistency Loss: 970701.625\n",
      "Total Temp Consistency Loss: 0.03604909032583237\n",
      "Total Loss: 970405.0625\n",
      "\n",
      "Total Fwd Loss: 6.322901248931885\n",
      "Total Bwd Loss: 8.283954620361328\n",
      "Total Inv Consistency Loss: 970390.375\n",
      "Total Temp Consistency Loss: 0.0381167009472847\n",
      "Total Loss: 970093.3125\n",
      "\n",
      "Total Fwd Loss: 8.050248146057129\n",
      "Total Bwd Loss: 7.889701843261719\n",
      "Total Inv Consistency Loss: 970077.3125\n",
      "Total Temp Consistency Loss: 0.03751252219080925\n",
      "Total Loss: 969786.8125\n",
      "\n",
      "Total Fwd Loss: 7.868283748626709\n",
      "Total Bwd Loss: 7.553940773010254\n",
      "Total Inv Consistency Loss: 969771.3125\n",
      "Total Temp Consistency Loss: 0.036404386162757874\n",
      "Total Loss: 969467.75\n",
      "\n",
      "Total Fwd Loss: 8.177281379699707\n",
      "Total Bwd Loss: 7.492413520812988\n",
      "Total Inv Consistency Loss: 969452.0\n",
      "Total Temp Consistency Loss: 0.035764358937740326\n",
      "Total Loss: 969162.0625\n",
      "\n",
      "Total Fwd Loss: 9.56013298034668\n",
      "Total Bwd Loss: 9.245322227478027\n",
      "Total Inv Consistency Loss: 969143.1875\n",
      "Total Temp Consistency Loss: 0.035809360444545746\n",
      "----------Training epoch--------\n",
      "----------------79---------------\n",
      "\n",
      "Total Loss: 968848.875\n",
      "\n",
      "Total Fwd Loss: 7.161525726318359\n",
      "Total Bwd Loss: 8.163400650024414\n",
      "Total Inv Consistency Loss: 968833.5\n",
      "Total Temp Consistency Loss: 0.035557717084884644\n",
      "Total Loss: 968542.875\n",
      "\n",
      "Total Fwd Loss: 10.014036178588867\n",
      "Total Bwd Loss: 7.6833014488220215\n",
      "Total Inv Consistency Loss: 968525.125\n",
      "Total Temp Consistency Loss: 0.03232979401946068\n",
      "Total Loss: 968230.625\n",
      "\n",
      "Total Fwd Loss: 7.897276878356934\n",
      "Total Bwd Loss: 7.957356929779053\n",
      "Total Inv Consistency Loss: 968214.6875\n",
      "Total Temp Consistency Loss: 0.03362984582781792\n",
      "Total Loss: 967912.4375\n",
      "\n",
      "Total Fwd Loss: 7.424172401428223\n",
      "Total Bwd Loss: 8.332616806030273\n",
      "Total Inv Consistency Loss: 967896.625\n",
      "Total Temp Consistency Loss: 0.03847148269414902\n",
      "Total Loss: 967599.125\n",
      "\n",
      "Total Fwd Loss: 7.917601585388184\n",
      "Total Bwd Loss: 8.623689651489258\n",
      "Total Inv Consistency Loss: 967582.5\n",
      "Total Temp Consistency Loss: 0.036227840930223465\n",
      "Total Loss: 967288.8125\n",
      "\n",
      "Total Fwd Loss: 6.784819602966309\n",
      "Total Bwd Loss: 8.273938179016113\n",
      "Total Inv Consistency Loss: 967273.6875\n",
      "Total Temp Consistency Loss: 0.03668449446558952\n",
      "----------Training epoch--------\n",
      "----------------80---------------\n",
      "\n",
      "Total Loss: 966968.875\n",
      "\n",
      "Total Fwd Loss: 7.8077263832092285\n",
      "Total Bwd Loss: 7.632399559020996\n",
      "Total Inv Consistency Loss: 966953.375\n",
      "Total Temp Consistency Loss: 0.03480758145451546\n",
      "Total Loss: 966664.75\n",
      "\n",
      "Total Fwd Loss: 7.305020809173584\n",
      "Total Bwd Loss: 7.088833808898926\n",
      "Total Inv Consistency Loss: 966650.3125\n",
      "Total Temp Consistency Loss: 0.03386646881699562\n",
      "Total Loss: 966359.375\n",
      "\n",
      "Total Fwd Loss: 6.2904791831970215\n",
      "Total Bwd Loss: 7.913430690765381\n",
      "Total Inv Consistency Loss: 966345.125\n",
      "Total Temp Consistency Loss: 0.036445967853069305\n",
      "Total Loss: 966054.1875\n",
      "\n",
      "Total Fwd Loss: 9.822835922241211\n",
      "Total Bwd Loss: 9.683523178100586\n",
      "Total Inv Consistency Loss: 966034.625\n",
      "Total Temp Consistency Loss: 0.03420902043581009\n",
      "Total Loss: 965737.5625\n",
      "\n",
      "Total Fwd Loss: 7.928219795227051\n",
      "Total Bwd Loss: 8.719991683959961\n",
      "Total Inv Consistency Loss: 965720.875\n",
      "Total Temp Consistency Loss: 0.03448033705353737\n",
      "Total Loss: 965432.25\n",
      "\n",
      "Total Fwd Loss: 7.988055229187012\n",
      "Total Bwd Loss: 8.004666328430176\n",
      "Total Inv Consistency Loss: 965416.1875\n",
      "Total Temp Consistency Loss: 0.035336144268512726\n",
      "----------Training epoch--------\n",
      "----------------81---------------\n",
      "\n",
      "Total Loss: 965115.5\n",
      "\n",
      "Total Fwd Loss: 7.214478492736816\n",
      "Total Bwd Loss: 8.01806640625\n",
      "Total Inv Consistency Loss: 965100.1875\n",
      "Total Temp Consistency Loss: 0.03590940684080124\n",
      "Total Loss: 964876.875\n",
      "\n",
      "Total Fwd Loss: 9.735977172851562\n",
      "Total Bwd Loss: 8.475071907043457\n",
      "Total Inv Consistency Loss: 964858.625\n",
      "Total Temp Consistency Loss: 0.03294236212968826\n",
      "Total Loss: 964625.6875\n",
      "\n",
      "Total Fwd Loss: 7.281088352203369\n",
      "Total Bwd Loss: 7.186095237731934\n",
      "Total Inv Consistency Loss: 964611.1875\n",
      "Total Temp Consistency Loss: 0.03302036225795746\n",
      "Total Loss: 964371.75\n",
      "\n",
      "Total Fwd Loss: 7.4337639808654785\n",
      "Total Bwd Loss: 9.632972717285156\n",
      "Total Inv Consistency Loss: 964354.625\n",
      "Total Temp Consistency Loss: 0.03498398885130882\n",
      "Total Loss: 964124.3125\n",
      "\n",
      "Total Fwd Loss: 7.954077243804932\n",
      "Total Bwd Loss: 7.638730049133301\n",
      "Total Inv Consistency Loss: 964108.6875\n",
      "Total Temp Consistency Loss: 0.0337100513279438\n",
      "Total Loss: 963878.9375\n",
      "\n",
      "Total Fwd Loss: 7.653866767883301\n",
      "Total Bwd Loss: 7.849565029144287\n",
      "Total Inv Consistency Loss: 963863.375\n",
      "Total Temp Consistency Loss: 0.032005853950977325\n",
      "----------Training epoch--------\n",
      "----------------82---------------\n",
      "\n",
      "Total Loss: 963630.3125\n",
      "\n",
      "Total Fwd Loss: 6.623684883117676\n",
      "Total Bwd Loss: 8.00749397277832\n",
      "Total Inv Consistency Loss: 963615.625\n",
      "Total Temp Consistency Loss: 0.0320899523794651\n",
      "Total Loss: 963383.3125\n",
      "\n",
      "Total Fwd Loss: 7.751035213470459\n",
      "Total Bwd Loss: 8.101496696472168\n",
      "Total Inv Consistency Loss: 963367.375\n",
      "Total Temp Consistency Loss: 0.033474795520305634\n",
      "Total Loss: 963136.5\n",
      "\n",
      "Total Fwd Loss: 8.148164749145508\n",
      "Total Bwd Loss: 7.692225456237793\n",
      "Total Inv Consistency Loss: 963120.625\n",
      "Total Temp Consistency Loss: 0.031722474843263626\n",
      "Total Loss: 962886.9375\n",
      "\n",
      "Total Fwd Loss: 7.916065216064453\n",
      "Total Bwd Loss: 7.342856407165527\n",
      "Total Inv Consistency Loss: 962871.625\n",
      "Total Temp Consistency Loss: 0.03209110349416733\n",
      "Total Loss: 962642.5\n",
      "\n",
      "Total Fwd Loss: 7.733841896057129\n",
      "Total Bwd Loss: 9.183891296386719\n",
      "Total Inv Consistency Loss: 962625.5\n",
      "Total Temp Consistency Loss: 0.0341235026717186\n",
      "Total Loss: 962393.375\n",
      "\n",
      "Total Fwd Loss: 9.20223617553711\n",
      "Total Bwd Loss: 8.611223220825195\n",
      "Total Inv Consistency Loss: 962375.5\n",
      "Total Temp Consistency Loss: 0.03233635798096657\n",
      "----------Training epoch--------\n",
      "----------------83---------------\n",
      "\n",
      "Total Loss: 962140.1875\n",
      "\n",
      "Total Fwd Loss: 8.449750900268555\n",
      "Total Bwd Loss: 8.973361015319824\n",
      "Total Inv Consistency Loss: 962122.6875\n",
      "Total Temp Consistency Loss: 0.03274115175008774\n",
      "Total Loss: 961891.0625\n",
      "\n",
      "Total Fwd Loss: 7.475466251373291\n",
      "Total Bwd Loss: 8.363712310791016\n",
      "Total Inv Consistency Loss: 961875.1875\n",
      "Total Temp Consistency Loss: 0.03541368991136551\n",
      "Total Loss: 961649.1875\n",
      "\n",
      "Total Fwd Loss: 7.337839603424072\n",
      "Total Bwd Loss: 7.692563056945801\n",
      "Total Inv Consistency Loss: 961634.125\n",
      "Total Temp Consistency Loss: 0.03295142203569412\n",
      "Total Loss: 961407.125\n",
      "\n",
      "Total Fwd Loss: 7.179936408996582\n",
      "Total Bwd Loss: 7.48535680770874\n",
      "Total Inv Consistency Loss: 961392.375\n",
      "Total Temp Consistency Loss: 0.0336131826043129\n",
      "Total Loss: 961154.6875\n",
      "\n",
      "Total Fwd Loss: 8.737996101379395\n",
      "Total Bwd Loss: 8.258077621459961\n",
      "Total Inv Consistency Loss: 961137.625\n",
      "Total Temp Consistency Loss: 0.0329790897667408\n",
      "Total Loss: 960909.9375\n",
      "\n",
      "Total Fwd Loss: 8.269206047058105\n",
      "Total Bwd Loss: 8.127443313598633\n",
      "Total Inv Consistency Loss: 960893.5\n",
      "Total Temp Consistency Loss: 0.03307915851473808\n",
      "----------Training epoch--------\n",
      "----------------84---------------\n",
      "\n",
      "Total Loss: 960655.1875\n",
      "\n",
      "Total Fwd Loss: 7.985562324523926\n",
      "Total Bwd Loss: 8.995817184448242\n",
      "Total Inv Consistency Loss: 960638.125\n",
      "Total Temp Consistency Loss: 0.03380519524216652\n",
      "Total Loss: 960412.875\n",
      "\n",
      "Total Fwd Loss: 7.84664249420166\n",
      "Total Bwd Loss: 7.614280700683594\n",
      "Total Inv Consistency Loss: 960397.375\n",
      "Total Temp Consistency Loss: 0.03413153812289238\n",
      "Total Loss: 960167.6875\n",
      "\n",
      "Total Fwd Loss: 7.894620418548584\n",
      "Total Bwd Loss: 7.359269618988037\n",
      "Total Inv Consistency Loss: 960152.375\n",
      "Total Temp Consistency Loss: 0.0314086489379406\n",
      "Total Loss: 959920.3125\n",
      "\n",
      "Total Fwd Loss: 8.612658500671387\n",
      "Total Bwd Loss: 9.341073989868164\n",
      "Total Inv Consistency Loss: 959902.3125\n",
      "Total Temp Consistency Loss: 0.0320756696164608\n",
      "Total Loss: 959671.6875\n",
      "\n",
      "Total Fwd Loss: 6.413919925689697\n",
      "Total Bwd Loss: 7.444100379943848\n",
      "Total Inv Consistency Loss: 959657.8125\n",
      "Total Temp Consistency Loss: 0.030163710936903954\n",
      "Total Loss: 959421.1875\n",
      "\n",
      "Total Fwd Loss: 8.440620422363281\n",
      "Total Bwd Loss: 8.048598289489746\n",
      "Total Inv Consistency Loss: 959404.625\n",
      "Total Temp Consistency Loss: 0.03304993733763695\n",
      "----------Training epoch--------\n",
      "----------------85---------------\n",
      "\n",
      "Total Loss: 959178.8125\n",
      "\n",
      "Total Fwd Loss: 8.299283981323242\n",
      "Total Bwd Loss: 7.8163580894470215\n",
      "Total Inv Consistency Loss: 959162.625\n",
      "Total Temp Consistency Loss: 0.03307568281888962\n",
      "Total Loss: 958929.5\n",
      "\n",
      "Total Fwd Loss: 7.3072943687438965\n",
      "Total Bwd Loss: 7.790719032287598\n",
      "Total Inv Consistency Loss: 958914.3125\n",
      "Total Temp Consistency Loss: 0.03268415480852127\n",
      "Total Loss: 958681.75\n",
      "\n",
      "Total Fwd Loss: 7.266385078430176\n",
      "Total Bwd Loss: 7.563504695892334\n",
      "Total Inv Consistency Loss: 958666.875\n",
      "Total Temp Consistency Loss: 0.03208082169294357\n",
      "Total Loss: 958437.5625\n",
      "\n",
      "Total Fwd Loss: 7.777775764465332\n",
      "Total Bwd Loss: 7.18593692779541\n",
      "Total Inv Consistency Loss: 958422.625\n",
      "Total Temp Consistency Loss: 0.030529463663697243\n",
      "Total Loss: 958192.8125\n",
      "\n",
      "Total Fwd Loss: 7.80994176864624\n",
      "Total Bwd Loss: 8.56279182434082\n",
      "Total Inv Consistency Loss: 958176.375\n",
      "Total Temp Consistency Loss: 0.03319535031914711\n",
      "Total Loss: 957944.625\n",
      "\n",
      "Total Fwd Loss: 8.783594131469727\n",
      "Total Bwd Loss: 9.970212936401367\n",
      "Total Inv Consistency Loss: 957925.8125\n",
      "Total Temp Consistency Loss: 0.031600240617990494\n",
      "----------Training epoch--------\n",
      "----------------86---------------\n",
      "\n",
      "Total Loss: 957702.9375\n",
      "\n",
      "Total Fwd Loss: 7.786860466003418\n",
      "Total Bwd Loss: 7.0193586349487305\n",
      "Total Inv Consistency Loss: 957688.125\n",
      "Total Temp Consistency Loss: 0.03032711148262024\n",
      "Total Loss: 957453.625\n",
      "\n",
      "Total Fwd Loss: 7.700037479400635\n",
      "Total Bwd Loss: 8.421953201293945\n",
      "Total Inv Consistency Loss: 957437.5\n",
      "Total Temp Consistency Loss: 0.031003570184111595\n",
      "Total Loss: 957208.125\n",
      "\n",
      "Total Fwd Loss: 7.241011619567871\n",
      "Total Bwd Loss: 7.97637414932251\n",
      "Total Inv Consistency Loss: 957192.875\n",
      "Total Temp Consistency Loss: 0.0329868458211422\n",
      "Total Loss: 956960.125\n",
      "\n",
      "Total Fwd Loss: 7.481081962585449\n",
      "Total Bwd Loss: 7.683840274810791\n",
      "Total Inv Consistency Loss: 956944.875\n",
      "Total Temp Consistency Loss: 0.03127923607826233\n",
      "Total Loss: 956711.0\n",
      "\n",
      "Total Fwd Loss: 7.770116329193115\n",
      "Total Bwd Loss: 8.088421821594238\n",
      "Total Inv Consistency Loss: 956695.125\n",
      "Total Temp Consistency Loss: 0.031164783984422684\n",
      "Total Loss: 956468.1875\n",
      "\n",
      "Total Fwd Loss: 9.19466781616211\n",
      "Total Bwd Loss: 9.684861183166504\n",
      "Total Inv Consistency Loss: 956449.3125\n",
      "Total Temp Consistency Loss: 0.029936533421278\n",
      "----------Training epoch--------\n",
      "----------------87---------------\n",
      "\n",
      "Total Loss: 956219.9375\n",
      "\n",
      "Total Fwd Loss: 6.758471488952637\n",
      "Total Bwd Loss: 8.459997177124023\n",
      "Total Inv Consistency Loss: 956204.6875\n",
      "Total Temp Consistency Loss: 0.03190101683139801\n",
      "Total Loss: 955977.0625\n",
      "\n",
      "Total Fwd Loss: 8.990293502807617\n",
      "Total Bwd Loss: 8.67223834991455\n",
      "Total Inv Consistency Loss: 955959.375\n",
      "Total Temp Consistency Loss: 0.029749255627393723\n",
      "Total Loss: 955730.6875\n",
      "\n",
      "Total Fwd Loss: 7.643534183502197\n",
      "Total Bwd Loss: 7.212257385253906\n",
      "Total Inv Consistency Loss: 955715.8125\n",
      "Total Temp Consistency Loss: 0.030067797750234604\n",
      "Total Loss: 955479.875\n",
      "\n",
      "Total Fwd Loss: 7.5183844566345215\n",
      "Total Bwd Loss: 8.598710060119629\n",
      "Total Inv Consistency Loss: 955463.6875\n",
      "Total Temp Consistency Loss: 0.03152316436171532\n",
      "Total Loss: 955234.75\n",
      "\n",
      "Total Fwd Loss: 7.218523979187012\n",
      "Total Bwd Loss: 7.4593634605407715\n",
      "Total Inv Consistency Loss: 955220.0\n",
      "Total Temp Consistency Loss: 0.03155917301774025\n",
      "Total Loss: 954987.25\n",
      "\n",
      "Total Fwd Loss: 9.091704368591309\n",
      "Total Bwd Loss: 8.295884132385254\n",
      "Total Inv Consistency Loss: 954969.875\n",
      "Total Temp Consistency Loss: 0.029161453247070312\n",
      "----------Training epoch--------\n",
      "----------------88---------------\n",
      "\n",
      "Total Loss: 954748.6875\n",
      "\n",
      "Total Fwd Loss: 9.100776672363281\n",
      "Total Bwd Loss: 10.072366714477539\n",
      "Total Inv Consistency Loss: 954729.5\n",
      "Total Temp Consistency Loss: 0.030416712164878845\n",
      "Total Loss: 954500.5\n",
      "\n",
      "Total Fwd Loss: 7.5589118003845215\n",
      "Total Bwd Loss: 7.596034049987793\n",
      "Total Inv Consistency Loss: 954485.375\n",
      "Total Temp Consistency Loss: 0.029148777946829796\n",
      "Total Loss: 954254.0625\n",
      "\n",
      "Total Fwd Loss: 8.16185188293457\n",
      "Total Bwd Loss: 8.070046424865723\n",
      "Total Inv Consistency Loss: 954237.8125\n",
      "Total Temp Consistency Loss: 0.030444052070379257\n",
      "Total Loss: 954013.375\n",
      "\n",
      "Total Fwd Loss: 7.822695732116699\n",
      "Total Bwd Loss: 7.3523993492126465\n",
      "Total Inv Consistency Loss: 953998.1875\n",
      "Total Temp Consistency Loss: 0.029318207874894142\n",
      "Total Loss: 953765.6875\n",
      "\n",
      "Total Fwd Loss: 6.566366672515869\n",
      "Total Bwd Loss: 7.974356174468994\n",
      "Total Inv Consistency Loss: 953751.125\n",
      "Total Temp Consistency Loss: 0.03042915090918541\n",
      "Total Loss: 953518.8125\n",
      "\n",
      "Total Fwd Loss: 8.040931701660156\n",
      "Total Bwd Loss: 7.971153259277344\n",
      "Total Inv Consistency Loss: 953502.8125\n",
      "Total Temp Consistency Loss: 0.029656317085027695\n",
      "----------Training epoch--------\n",
      "----------------89---------------\n",
      "\n",
      "Total Loss: 953275.125\n",
      "\n",
      "Total Fwd Loss: 9.153772354125977\n",
      "Total Bwd Loss: 8.50202751159668\n",
      "Total Inv Consistency Loss: 953257.5\n",
      "Total Temp Consistency Loss: 0.029169198125600815\n",
      "Total Loss: 953025.5625\n",
      "\n",
      "Total Fwd Loss: 6.9292802810668945\n",
      "Total Bwd Loss: 8.271241188049316\n",
      "Total Inv Consistency Loss: 953010.375\n",
      "Total Temp Consistency Loss: 0.029875537380576134\n",
      "Total Loss: 952782.0\n",
      "\n",
      "Total Fwd Loss: 7.69143009185791\n",
      "Total Bwd Loss: 7.980358123779297\n",
      "Total Inv Consistency Loss: 952766.3125\n",
      "Total Temp Consistency Loss: 0.02889866754412651\n",
      "Total Loss: 952537.6875\n",
      "\n",
      "Total Fwd Loss: 7.805377960205078\n",
      "Total Bwd Loss: 8.23595905303955\n",
      "Total Inv Consistency Loss: 952521.625\n",
      "Total Temp Consistency Loss: 0.029760470613837242\n",
      "Total Loss: 952294.6875\n",
      "\n",
      "Total Fwd Loss: 7.366555213928223\n",
      "Total Bwd Loss: 7.800085544586182\n",
      "Total Inv Consistency Loss: 952279.5\n",
      "Total Temp Consistency Loss: 0.02984144724905491\n",
      "Total Loss: 952042.8125\n",
      "\n",
      "Total Fwd Loss: 8.048112869262695\n",
      "Total Bwd Loss: 8.294541358947754\n",
      "Total Inv Consistency Loss: 952026.5\n",
      "Total Temp Consistency Loss: 0.02887527272105217\n",
      "----------Training epoch--------\n",
      "----------------90---------------\n",
      "\n",
      "Total Loss: 951802.5625\n",
      "\n",
      "Total Fwd Loss: 9.244974136352539\n",
      "Total Bwd Loss: 10.002248764038086\n",
      "Total Inv Consistency Loss: 951783.3125\n",
      "Total Temp Consistency Loss: 0.029519271105527878\n",
      "Total Loss: 951552.5625\n",
      "\n",
      "Total Fwd Loss: 7.37521505355835\n",
      "Total Bwd Loss: 6.986054420471191\n",
      "Total Inv Consistency Loss: 951538.1875\n",
      "Total Temp Consistency Loss: 0.02739211544394493\n",
      "Total Loss: 951310.125\n",
      "\n",
      "Total Fwd Loss: 7.351234436035156\n",
      "Total Bwd Loss: 7.556069850921631\n",
      "Total Inv Consistency Loss: 951295.1875\n",
      "Total Temp Consistency Loss: 0.02931765839457512\n",
      "Total Loss: 951063.6875\n",
      "\n",
      "Total Fwd Loss: 7.758784294128418\n",
      "Total Bwd Loss: 8.268243789672852\n",
      "Total Inv Consistency Loss: 951047.625\n",
      "Total Temp Consistency Loss: 0.03150097280740738\n",
      "Total Loss: 950820.125\n",
      "\n",
      "Total Fwd Loss: 7.4666948318481445\n",
      "Total Bwd Loss: 8.13173770904541\n",
      "Total Inv Consistency Loss: 950804.5\n",
      "Total Temp Consistency Loss: 0.029012292623519897\n",
      "Total Loss: 950578.1875\n",
      "\n",
      "Total Fwd Loss: 7.91234827041626\n",
      "Total Bwd Loss: 7.966330528259277\n",
      "Total Inv Consistency Loss: 950562.3125\n",
      "Total Temp Consistency Loss: 0.028121059760451317\n",
      "----------Training epoch--------\n",
      "----------------91---------------\n",
      "\n",
      "Total Loss: 950338.9375\n",
      "\n",
      "Total Fwd Loss: 7.528207302093506\n",
      "Total Bwd Loss: 8.560042381286621\n",
      "Total Inv Consistency Loss: 950322.875\n",
      "Total Temp Consistency Loss: 0.029393842443823814\n",
      "Total Loss: 950091.625\n",
      "\n",
      "Total Fwd Loss: 7.938079833984375\n",
      "Total Bwd Loss: 8.162209510803223\n",
      "Total Inv Consistency Loss: 950075.5\n",
      "Total Temp Consistency Loss: 0.028608474880456924\n",
      "Total Loss: 949846.875\n",
      "\n",
      "Total Fwd Loss: 8.487114906311035\n",
      "Total Bwd Loss: 6.765730381011963\n",
      "Total Inv Consistency Loss: 949831.625\n",
      "Total Temp Consistency Loss: 0.0277097187936306\n",
      "Total Loss: 949603.125\n",
      "\n",
      "Total Fwd Loss: 8.750855445861816\n",
      "Total Bwd Loss: 8.845112800598145\n",
      "Total Inv Consistency Loss: 949585.5\n",
      "Total Temp Consistency Loss: 0.026942308992147446\n",
      "Total Loss: 949361.5\n",
      "\n",
      "Total Fwd Loss: 7.428073883056641\n",
      "Total Bwd Loss: 8.756430625915527\n",
      "Total Inv Consistency Loss: 949345.3125\n",
      "Total Temp Consistency Loss: 0.02877102419734001\n",
      "Total Loss: 949110.4375\n",
      "\n",
      "Total Fwd Loss: 7.121915340423584\n",
      "Total Bwd Loss: 7.688783168792725\n",
      "Total Inv Consistency Loss: 949095.625\n",
      "Total Temp Consistency Loss: 0.02782621420919895\n",
      "----------Training epoch--------\n",
      "----------------92---------------\n",
      "\n",
      "Total Loss: 948863.125\n",
      "\n",
      "Total Fwd Loss: 6.538571834564209\n",
      "Total Bwd Loss: 6.8000006675720215\n",
      "Total Inv Consistency Loss: 948849.8125\n",
      "Total Temp Consistency Loss: 0.027187347412109375\n",
      "Total Loss: 948628.875\n",
      "\n",
      "Total Fwd Loss: 8.939251899719238\n",
      "Total Bwd Loss: 9.29708480834961\n",
      "Total Inv Consistency Loss: 948610.625\n",
      "Total Temp Consistency Loss: 0.027678165584802628\n",
      "Total Loss: 948379.5625\n",
      "\n",
      "Total Fwd Loss: 7.595452785491943\n",
      "Total Bwd Loss: 8.816222190856934\n",
      "Total Inv Consistency Loss: 948363.125\n",
      "Total Temp Consistency Loss: 0.02967681922018528\n",
      "Total Loss: 948135.5\n",
      "\n",
      "Total Fwd Loss: 8.374410629272461\n",
      "Total Bwd Loss: 7.621656894683838\n",
      "Total Inv Consistency Loss: 948119.5\n",
      "Total Temp Consistency Loss: 0.02758701518177986\n",
      "Total Loss: 947891.1875\n",
      "\n",
      "Total Fwd Loss: 7.460817813873291\n",
      "Total Bwd Loss: 8.498472213745117\n",
      "Total Inv Consistency Loss: 947875.1875\n",
      "Total Temp Consistency Loss: 0.032283298671245575\n",
      "Total Loss: 947647.25\n",
      "\n",
      "Total Fwd Loss: 8.441516876220703\n",
      "Total Bwd Loss: 7.679225921630859\n",
      "Total Inv Consistency Loss: 947631.125\n",
      "Total Temp Consistency Loss: 0.02924553118646145\n",
      "----------Training epoch--------\n",
      "----------------93---------------\n",
      "\n",
      "Total Loss: 947404.0625\n",
      "\n",
      "Total Fwd Loss: 8.472686767578125\n",
      "Total Bwd Loss: 7.428250789642334\n",
      "Total Inv Consistency Loss: 947388.1875\n",
      "Total Temp Consistency Loss: 0.027197320014238358\n",
      "Total Loss: 947164.0\n",
      "\n",
      "Total Fwd Loss: 8.93414306640625\n",
      "Total Bwd Loss: 8.351371765136719\n",
      "Total Inv Consistency Loss: 947146.6875\n",
      "Total Temp Consistency Loss: 0.02577299252152443\n",
      "Total Loss: 946913.0625\n",
      "\n",
      "Total Fwd Loss: 7.759243965148926\n",
      "Total Bwd Loss: 8.906915664672852\n",
      "Total Inv Consistency Loss: 946896.375\n",
      "Total Temp Consistency Loss: 0.0280161090195179\n",
      "Total Loss: 946675.0625\n",
      "\n",
      "Total Fwd Loss: 7.230309963226318\n",
      "Total Bwd Loss: 8.924104690551758\n",
      "Total Inv Consistency Loss: 946658.875\n",
      "Total Temp Consistency Loss: 0.03138653561472893\n",
      "Total Loss: 946431.875\n",
      "\n",
      "Total Fwd Loss: 6.817117214202881\n",
      "Total Bwd Loss: 7.945765018463135\n",
      "Total Inv Consistency Loss: 946417.125\n",
      "Total Temp Consistency Loss: 0.02721613273024559\n",
      "Total Loss: 946190.0625\n",
      "\n",
      "Total Fwd Loss: 7.963530540466309\n",
      "Total Bwd Loss: 7.4202752113342285\n",
      "Total Inv Consistency Loss: 946174.6875\n",
      "Total Temp Consistency Loss: 0.029211362823843956\n",
      "----------Training epoch--------\n",
      "----------------94---------------\n",
      "\n",
      "Total Loss: 945937.125\n",
      "\n",
      "Total Fwd Loss: 8.01154613494873\n",
      "Total Bwd Loss: 7.747381687164307\n",
      "Total Inv Consistency Loss: 945921.375\n",
      "Total Temp Consistency Loss: 0.0276188887655735\n",
      "Total Loss: 945702.6875\n",
      "\n",
      "Total Fwd Loss: 8.738947868347168\n",
      "Total Bwd Loss: 8.802507400512695\n",
      "Total Inv Consistency Loss: 945685.125\n",
      "Total Temp Consistency Loss: 0.0276730265468359\n",
      "Total Loss: 945455.625\n",
      "\n",
      "Total Fwd Loss: 7.734118461608887\n",
      "Total Bwd Loss: 9.088505744934082\n",
      "Total Inv Consistency Loss: 945438.8125\n",
      "Total Temp Consistency Loss: 0.027498429641127586\n",
      "Total Loss: 945209.125\n",
      "\n",
      "Total Fwd Loss: 7.579270362854004\n",
      "Total Bwd Loss: 7.548584938049316\n",
      "Total Inv Consistency Loss: 945194.0\n",
      "Total Temp Consistency Loss: 0.027237946167588234\n",
      "Total Loss: 944966.5625\n",
      "\n",
      "Total Fwd Loss: 7.5963544845581055\n",
      "Total Bwd Loss: 8.353785514831543\n",
      "Total Inv Consistency Loss: 944950.625\n",
      "Total Temp Consistency Loss: 0.02949024736881256\n",
      "Total Loss: 944729.5625\n",
      "\n",
      "Total Fwd Loss: 7.36522912979126\n",
      "Total Bwd Loss: 7.328683376312256\n",
      "Total Inv Consistency Loss: 944714.875\n",
      "Total Temp Consistency Loss: 0.02700107730925083\n",
      "----------Training epoch--------\n",
      "----------------95---------------\n",
      "\n",
      "Total Loss: 944481.6875\n",
      "\n",
      "Total Fwd Loss: 8.404975891113281\n",
      "Total Bwd Loss: 8.808891296386719\n",
      "Total Inv Consistency Loss: 944464.5\n",
      "Total Temp Consistency Loss: 0.026784654706716537\n",
      "Total Loss: 944239.75\n",
      "\n",
      "Total Fwd Loss: 8.081491470336914\n",
      "Total Bwd Loss: 7.69156551361084\n",
      "Total Inv Consistency Loss: 944224.0\n",
      "Total Temp Consistency Loss: 0.02606537938117981\n",
      "Total Loss: 944002.1875\n",
      "\n",
      "Total Fwd Loss: 8.485071182250977\n",
      "Total Bwd Loss: 9.072646141052246\n",
      "Total Inv Consistency Loss: 943984.625\n",
      "Total Temp Consistency Loss: 0.027566051110625267\n",
      "Total Loss: 943753.3125\n",
      "\n",
      "Total Fwd Loss: 6.852320194244385\n",
      "Total Bwd Loss: 7.591548919677734\n",
      "Total Inv Consistency Loss: 943738.875\n",
      "Total Temp Consistency Loss: 0.025150109082460403\n",
      "Total Loss: 943513.5\n",
      "\n",
      "Total Fwd Loss: 7.387677192687988\n",
      "Total Bwd Loss: 7.456089973449707\n",
      "Total Inv Consistency Loss: 943498.625\n",
      "Total Temp Consistency Loss: 0.027193564921617508\n",
      "Total Loss: 943270.625\n",
      "\n",
      "Total Fwd Loss: 7.904376983642578\n",
      "Total Bwd Loss: 8.192224502563477\n",
      "Total Inv Consistency Loss: 943254.5\n",
      "Total Temp Consistency Loss: 0.026154259219765663\n",
      "----------Training epoch--------\n",
      "----------------96---------------\n",
      "\n",
      "Total Loss: 943021.5625\n",
      "\n",
      "Total Fwd Loss: 7.567502498626709\n",
      "Total Bwd Loss: 7.505088806152344\n",
      "Total Inv Consistency Loss: 943006.5\n",
      "Total Temp Consistency Loss: 0.0254837516695261\n",
      "Total Loss: 942780.6875\n",
      "\n",
      "Total Fwd Loss: 7.414368629455566\n",
      "Total Bwd Loss: 9.058564186096191\n",
      "Total Inv Consistency Loss: 942764.1875\n",
      "Total Temp Consistency Loss: 0.027756046503782272\n",
      "Total Loss: 942539.0\n",
      "\n",
      "Total Fwd Loss: 7.013164520263672\n",
      "Total Bwd Loss: 7.660176753997803\n",
      "Total Inv Consistency Loss: 942524.3125\n",
      "Total Temp Consistency Loss: 0.02375737950205803\n",
      "Total Loss: 942301.9375\n",
      "\n",
      "Total Fwd Loss: 7.235008239746094\n",
      "Total Bwd Loss: 8.41028118133545\n",
      "Total Inv Consistency Loss: 942286.3125\n",
      "Total Temp Consistency Loss: 0.026903722435235977\n",
      "Total Loss: 942065.875\n",
      "\n",
      "Total Fwd Loss: 10.062739372253418\n",
      "Total Bwd Loss: 8.670975685119629\n",
      "Total Inv Consistency Loss: 942047.125\n",
      "Total Temp Consistency Loss: 0.024047130718827248\n",
      "Total Loss: 941815.8125\n",
      "\n",
      "Total Fwd Loss: 7.961897850036621\n",
      "Total Bwd Loss: 7.498925685882568\n",
      "Total Inv Consistency Loss: 941800.375\n",
      "Total Temp Consistency Loss: 0.0249136034399271\n",
      "----------Training epoch--------\n",
      "----------------97---------------\n",
      "\n",
      "Total Loss: 941574.75\n",
      "\n",
      "Total Fwd Loss: 8.952032089233398\n",
      "Total Bwd Loss: 7.98178768157959\n",
      "Total Inv Consistency Loss: 941557.8125\n",
      "Total Temp Consistency Loss: 0.023569177836179733\n",
      "Total Loss: 941324.0625\n",
      "\n",
      "Total Fwd Loss: 7.432614326477051\n",
      "Total Bwd Loss: 7.970877170562744\n",
      "Total Inv Consistency Loss: 941308.6875\n",
      "Total Temp Consistency Loss: 0.025243079289793968\n",
      "Total Loss: 941085.9375\n",
      "\n",
      "Total Fwd Loss: 6.97974157333374\n",
      "Total Bwd Loss: 6.826016902923584\n",
      "Total Inv Consistency Loss: 941072.125\n",
      "Total Temp Consistency Loss: 0.025283217430114746\n",
      "Total Loss: 940849.1875\n",
      "\n",
      "Total Fwd Loss: 8.109702110290527\n",
      "Total Bwd Loss: 8.239191055297852\n",
      "Total Inv Consistency Loss: 940832.8125\n",
      "Total Temp Consistency Loss: 0.024187101051211357\n",
      "Total Loss: 940606.0625\n",
      "\n",
      "Total Fwd Loss: 7.724047660827637\n",
      "Total Bwd Loss: 8.713106155395508\n",
      "Total Inv Consistency Loss: 940589.625\n",
      "Total Temp Consistency Loss: 0.02634616568684578\n",
      "Total Loss: 940365.5625\n",
      "\n",
      "Total Fwd Loss: 7.934317588806152\n",
      "Total Bwd Loss: 9.126677513122559\n",
      "Total Inv Consistency Loss: 940348.5\n",
      "Total Temp Consistency Loss: 0.027184506878256798\n",
      "----------Training epoch--------\n",
      "----------------98---------------\n",
      "\n",
      "Total Loss: 940119.9375\n",
      "\n",
      "Total Fwd Loss: 7.166384220123291\n",
      "Total Bwd Loss: 8.968347549438477\n",
      "Total Inv Consistency Loss: 940103.8125\n",
      "Total Temp Consistency Loss: 0.0251770056784153\n",
      "Total Loss: 939878.875\n",
      "\n",
      "Total Fwd Loss: 7.142500400543213\n",
      "Total Bwd Loss: 8.0750150680542\n",
      "Total Inv Consistency Loss: 939863.6875\n",
      "Total Temp Consistency Loss: 0.026623856276273727\n",
      "Total Loss: 939637.875\n",
      "\n",
      "Total Fwd Loss: 7.7856903076171875\n",
      "Total Bwd Loss: 8.45276927947998\n",
      "Total Inv Consistency Loss: 939621.625\n",
      "Total Temp Consistency Loss: 0.025696327909827232\n",
      "Total Loss: 939394.875\n",
      "\n",
      "Total Fwd Loss: 7.625787258148193\n",
      "Total Bwd Loss: 7.555500030517578\n",
      "Total Inv Consistency Loss: 939379.6875\n",
      "Total Temp Consistency Loss: 0.025205319747328758\n",
      "Total Loss: 939154.0\n",
      "\n",
      "Total Fwd Loss: 7.564613342285156\n",
      "Total Bwd Loss: 6.770515441894531\n",
      "Total Inv Consistency Loss: 939139.6875\n",
      "Total Temp Consistency Loss: 0.023143688216805458\n",
      "Total Loss: 938915.25\n",
      "\n",
      "Total Fwd Loss: 9.746965408325195\n",
      "Total Bwd Loss: 9.023275375366211\n",
      "Total Inv Consistency Loss: 938896.5\n",
      "Total Temp Consistency Loss: 0.023477666079998016\n",
      "----------Training epoch--------\n",
      "----------------99---------------\n",
      "\n",
      "Total Loss: 938673.0\n",
      "\n",
      "Total Fwd Loss: 7.523512840270996\n",
      "Total Bwd Loss: 7.483990669250488\n",
      "Total Inv Consistency Loss: 938658.0\n",
      "Total Temp Consistency Loss: 0.02591324783861637\n",
      "Total Loss: 938431.9375\n",
      "\n",
      "Total Fwd Loss: 6.897869110107422\n",
      "Total Bwd Loss: 8.225447654724121\n",
      "Total Inv Consistency Loss: 938416.8125\n",
      "Total Temp Consistency Loss: 0.02383764646947384\n",
      "Total Loss: 938189.3125\n",
      "\n",
      "Total Fwd Loss: 7.366556644439697\n",
      "Total Bwd Loss: 8.273576736450195\n",
      "Total Inv Consistency Loss: 938173.6875\n",
      "Total Temp Consistency Loss: 0.025347331538796425\n",
      "Total Loss: 937948.9375\n",
      "\n",
      "Total Fwd Loss: 9.048303604125977\n",
      "Total Bwd Loss: 8.363975524902344\n",
      "Total Inv Consistency Loss: 937931.5\n",
      "Total Temp Consistency Loss: 0.024603616446256638\n",
      "Total Loss: 937696.9375\n",
      "\n",
      "Total Fwd Loss: 8.862051963806152\n",
      "Total Bwd Loss: 7.742619514465332\n",
      "Total Inv Consistency Loss: 937680.3125\n",
      "Total Temp Consistency Loss: 0.023487593978643417\n",
      "Total Loss: 937465.75\n",
      "\n",
      "Total Fwd Loss: 7.531097412109375\n",
      "Total Bwd Loss: 8.624677658081055\n",
      "Total Inv Consistency Loss: 937449.625\n",
      "Total Temp Consistency Loss: 0.025134015828371048\n",
      "----------Training epoch--------\n",
      "----------------100---------------\n",
      "\n",
      "Total Loss: 937224.75\n",
      "\n",
      "Total Fwd Loss: 9.29953670501709\n",
      "Total Bwd Loss: 8.573780059814453\n",
      "Total Inv Consistency Loss: 937206.875\n",
      "Total Temp Consistency Loss: 0.02529137395322323\n",
      "Total Loss: 936982.625\n",
      "\n",
      "Total Fwd Loss: 7.523484706878662\n",
      "Total Bwd Loss: 9.096647262573242\n",
      "Total Inv Consistency Loss: 936966.0\n",
      "Total Temp Consistency Loss: 0.02495618537068367\n",
      "Total Loss: 936737.0\n",
      "\n",
      "Total Fwd Loss: 7.898110866546631\n",
      "Total Bwd Loss: 7.913336277008057\n",
      "Total Inv Consistency Loss: 936721.1875\n",
      "Total Temp Consistency Loss: 0.021683501079678535\n",
      "Total Loss: 936497.0\n",
      "\n",
      "Total Fwd Loss: 7.879273414611816\n",
      "Total Bwd Loss: 8.723661422729492\n",
      "Total Inv Consistency Loss: 936480.375\n",
      "Total Temp Consistency Loss: 0.02481369487941265\n",
      "Total Loss: 936257.75\n",
      "\n",
      "Total Fwd Loss: 7.383908271789551\n",
      "Total Bwd Loss: 6.987736701965332\n",
      "Total Inv Consistency Loss: 936243.375\n",
      "Total Temp Consistency Loss: 0.022329898551106453\n",
      "Total Loss: 936013.9375\n",
      "\n",
      "Total Fwd Loss: 7.220418453216553\n",
      "Total Bwd Loss: 7.713092803955078\n",
      "Total Inv Consistency Loss: 935999.0\n",
      "Total Temp Consistency Loss: 0.02448064275085926\n",
      "----------Training epoch--------\n",
      "----------------101---------------\n",
      "\n",
      "Total Loss: 935777.9375\n",
      "\n",
      "Total Fwd Loss: 8.266226768493652\n",
      "Total Bwd Loss: 8.291104316711426\n",
      "Total Inv Consistency Loss: 935761.375\n",
      "Total Temp Consistency Loss: 0.02256462164223194\n",
      "Total Loss: 935535.5625\n",
      "\n",
      "Total Fwd Loss: 8.174729347229004\n",
      "Total Bwd Loss: 6.750679016113281\n",
      "Total Inv Consistency Loss: 935520.625\n",
      "Total Temp Consistency Loss: 0.022111371159553528\n",
      "Total Loss: 935295.125\n",
      "\n",
      "Total Fwd Loss: 8.343247413635254\n",
      "Total Bwd Loss: 8.074647903442383\n",
      "Total Inv Consistency Loss: 935278.6875\n",
      "Total Temp Consistency Loss: 0.024464253336191177\n",
      "Total Loss: 935054.375\n",
      "\n",
      "Total Fwd Loss: 6.73003625869751\n",
      "Total Bwd Loss: 7.993378639221191\n",
      "Total Inv Consistency Loss: 935039.625\n",
      "Total Temp Consistency Loss: 0.024182798340916634\n",
      "Total Loss: 934816.9375\n",
      "\n",
      "Total Fwd Loss: 7.374724388122559\n",
      "Total Bwd Loss: 8.177382469177246\n",
      "Total Inv Consistency Loss: 934801.375\n",
      "Total Temp Consistency Loss: 0.025429293513298035\n",
      "Total Loss: 934574.125\n",
      "\n",
      "Total Fwd Loss: 8.03345012664795\n",
      "Total Bwd Loss: 9.747919082641602\n",
      "Total Inv Consistency Loss: 934556.3125\n",
      "Total Temp Consistency Loss: 0.023729797452688217\n",
      "----------Training epoch--------\n",
      "----------------102---------------\n",
      "\n",
      "Total Loss: 934331.0\n",
      "\n",
      "Total Fwd Loss: 9.116233825683594\n",
      "Total Bwd Loss: 9.080060958862305\n",
      "Total Inv Consistency Loss: 934312.8125\n",
      "Total Temp Consistency Loss: 0.023026954382658005\n",
      "Total Loss: 934093.0\n",
      "\n",
      "Total Fwd Loss: 7.496574401855469\n",
      "Total Bwd Loss: 7.2008490562438965\n",
      "Total Inv Consistency Loss: 934078.3125\n",
      "Total Temp Consistency Loss: 0.023488294333219528\n",
      "Total Loss: 933852.4375\n",
      "\n",
      "Total Fwd Loss: 8.039779663085938\n",
      "Total Bwd Loss: 8.374690055847168\n",
      "Total Inv Consistency Loss: 933836.0\n",
      "Total Temp Consistency Loss: 0.02396947704255581\n",
      "Total Loss: 933608.1875\n",
      "\n",
      "Total Fwd Loss: 7.5576982498168945\n",
      "Total Bwd Loss: 8.248144149780273\n",
      "Total Inv Consistency Loss: 933592.375\n",
      "Total Temp Consistency Loss: 0.02356087416410446\n",
      "Total Loss: 933366.75\n",
      "\n",
      "Total Fwd Loss: 7.649292945861816\n",
      "Total Bwd Loss: 7.382148742675781\n",
      "Total Inv Consistency Loss: 933351.6875\n",
      "Total Temp Consistency Loss: 0.022546347230672836\n",
      "Total Loss: 933126.5\n",
      "\n",
      "Total Fwd Loss: 7.136843681335449\n",
      "Total Bwd Loss: 8.727571487426758\n",
      "Total Inv Consistency Loss: 933110.625\n",
      "Total Temp Consistency Loss: 0.024776164442300797\n",
      "----------Training epoch--------\n",
      "----------------103---------------\n",
      "\n",
      "Total Loss: 932894.0625\n",
      "\n",
      "Total Fwd Loss: 8.371647834777832\n",
      "Total Bwd Loss: 8.050859451293945\n",
      "Total Inv Consistency Loss: 932877.625\n",
      "Total Temp Consistency Loss: 0.02176007069647312\n",
      "Total Loss: 932658.125\n",
      "\n",
      "Total Fwd Loss: 7.775534629821777\n",
      "Total Bwd Loss: 8.961995124816895\n",
      "Total Inv Consistency Loss: 932641.375\n",
      "Total Temp Consistency Loss: 0.02552398107945919\n",
      "Total Loss: 932410.875\n",
      "\n",
      "Total Fwd Loss: 7.36005163192749\n",
      "Total Bwd Loss: 6.69580078125\n",
      "Total Inv Consistency Loss: 932396.8125\n",
      "Total Temp Consistency Loss: 0.02140733227133751\n",
      "Total Loss: 932174.875\n",
      "\n",
      "Total Fwd Loss: 8.27930736541748\n",
      "Total Bwd Loss: 8.948232650756836\n",
      "Total Inv Consistency Loss: 932157.625\n",
      "Total Temp Consistency Loss: 0.023084629327058792\n",
      "Total Loss: 931928.6875\n",
      "\n",
      "Total Fwd Loss: 7.801630973815918\n",
      "Total Bwd Loss: 8.183361053466797\n",
      "Total Inv Consistency Loss: 931912.6875\n",
      "Total Temp Consistency Loss: 0.02316560596227646\n",
      "Total Loss: 931689.0\n",
      "\n",
      "Total Fwd Loss: 7.517743110656738\n",
      "Total Bwd Loss: 7.964456081390381\n",
      "Total Inv Consistency Loss: 931673.5\n",
      "Total Temp Consistency Loss: 0.02517140470445156\n",
      "----------Training epoch--------\n",
      "----------------104---------------\n",
      "\n",
      "Total Loss: 931449.1875\n",
      "\n",
      "Total Fwd Loss: 7.318233489990234\n",
      "Total Bwd Loss: 8.469000816345215\n",
      "Total Inv Consistency Loss: 931433.375\n",
      "Total Temp Consistency Loss: 0.02319652959704399\n",
      "Total Loss: 931215.5\n",
      "\n",
      "Total Fwd Loss: 8.094451904296875\n",
      "Total Bwd Loss: 8.581626892089844\n",
      "Total Inv Consistency Loss: 931198.8125\n",
      "Total Temp Consistency Loss: 0.023886455222964287\n",
      "Total Loss: 930976.375\n",
      "\n",
      "Total Fwd Loss: 8.824411392211914\n",
      "Total Bwd Loss: 8.403556823730469\n",
      "Total Inv Consistency Loss: 930959.125\n",
      "Total Temp Consistency Loss: 0.021681945770978928\n",
      "Total Loss: 930736.6875\n",
      "\n",
      "Total Fwd Loss: 8.113692283630371\n",
      "Total Bwd Loss: 8.865248680114746\n",
      "Total Inv Consistency Loss: 930719.6875\n",
      "Total Temp Consistency Loss: 0.024184513837099075\n",
      "Total Loss: 930493.8125\n",
      "\n",
      "Total Fwd Loss: 7.2609076499938965\n",
      "Total Bwd Loss: 7.5471930503845215\n",
      "Total Inv Consistency Loss: 930479.0\n",
      "Total Temp Consistency Loss: 0.02283129096031189\n",
      "Total Loss: 930256.625\n",
      "\n",
      "Total Fwd Loss: 7.4904656410217285\n",
      "Total Bwd Loss: 6.958268642425537\n",
      "Total Inv Consistency Loss: 930242.1875\n",
      "Total Temp Consistency Loss: 0.021993501111865044\n",
      "----------Training epoch--------\n",
      "----------------105---------------\n",
      "\n",
      "Total Loss: 930012.3125\n",
      "\n",
      "Total Fwd Loss: 7.763821601867676\n",
      "Total Bwd Loss: 7.537829399108887\n",
      "Total Inv Consistency Loss: 929997.0\n",
      "Total Temp Consistency Loss: 0.021427227184176445\n",
      "Total Loss: 929778.5625\n",
      "\n",
      "Total Fwd Loss: 8.14116382598877\n",
      "Total Bwd Loss: 7.139638423919678\n",
      "Total Inv Consistency Loss: 929763.3125\n",
      "Total Temp Consistency Loss: 0.022961633279919624\n",
      "Total Loss: 929538.6875\n",
      "\n",
      "Total Fwd Loss: 6.751600742340088\n",
      "Total Bwd Loss: 7.8236589431762695\n",
      "Total Inv Consistency Loss: 929524.125\n",
      "Total Temp Consistency Loss: 0.02351718582212925\n",
      "Total Loss: 929297.1875\n",
      "\n",
      "Total Fwd Loss: 8.298966407775879\n",
      "Total Bwd Loss: 8.222471237182617\n",
      "Total Inv Consistency Loss: 929280.6875\n",
      "Total Temp Consistency Loss: 0.02273145504295826\n",
      "Total Loss: 929063.8125\n",
      "\n",
      "Total Fwd Loss: 8.497087478637695\n",
      "Total Bwd Loss: 9.146349906921387\n",
      "Total Inv Consistency Loss: 929046.1875\n",
      "Total Temp Consistency Loss: 0.022303100675344467\n",
      "Total Loss: 928822.125\n",
      "\n",
      "Total Fwd Loss: 7.740655422210693\n",
      "Total Bwd Loss: 8.908650398254395\n",
      "Total Inv Consistency Loss: 928805.5\n",
      "Total Temp Consistency Loss: 0.023958055302500725\n",
      "----------Training epoch--------\n",
      "----------------106---------------\n",
      "\n",
      "Total Loss: 928582.25\n",
      "\n",
      "Total Fwd Loss: 7.185437202453613\n",
      "Total Bwd Loss: 7.279473304748535\n",
      "Total Inv Consistency Loss: 928567.8125\n",
      "Total Temp Consistency Loss: 0.02137431874871254\n",
      "Total Loss: 928341.3125\n",
      "\n",
      "Total Fwd Loss: 7.461121559143066\n",
      "Total Bwd Loss: 7.178822994232178\n",
      "Total Inv Consistency Loss: 928326.6875\n",
      "Total Temp Consistency Loss: 0.022637030109763145\n",
      "Total Loss: 928104.375\n",
      "\n",
      "Total Fwd Loss: 7.754366397857666\n",
      "Total Bwd Loss: 7.737460136413574\n",
      "Total Inv Consistency Loss: 928088.875\n",
      "Total Temp Consistency Loss: 0.02286474034190178\n",
      "Total Loss: 927863.75\n",
      "\n",
      "Total Fwd Loss: 7.207779884338379\n",
      "Total Bwd Loss: 8.718317985534668\n",
      "Total Inv Consistency Loss: 927847.8125\n",
      "Total Temp Consistency Loss: 0.02409937232732773\n",
      "Total Loss: 927625.9375\n",
      "\n",
      "Total Fwd Loss: 8.323406219482422\n",
      "Total Bwd Loss: 8.125459671020508\n",
      "Total Inv Consistency Loss: 927609.5\n",
      "Total Temp Consistency Loss: 0.022829569876194\n",
      "Total Loss: 927387.9375\n",
      "\n",
      "Total Fwd Loss: 9.22382926940918\n",
      "Total Bwd Loss: 9.707746505737305\n",
      "Total Inv Consistency Loss: 927369.0\n",
      "Total Temp Consistency Loss: 0.02367190457880497\n",
      "----------Training epoch--------\n",
      "----------------107---------------\n",
      "\n",
      "Total Loss: 927150.5625\n",
      "\n",
      "Total Fwd Loss: 7.9713287353515625\n",
      "Total Bwd Loss: 8.946176528930664\n",
      "Total Inv Consistency Loss: 927133.625\n",
      "Total Temp Consistency Loss: 0.023750033229589462\n",
      "Total Loss: 926912.0\n",
      "\n",
      "Total Fwd Loss: 8.203648567199707\n",
      "Total Bwd Loss: 7.437689781188965\n",
      "Total Inv Consistency Loss: 926896.375\n",
      "Total Temp Consistency Loss: 0.02156336046755314\n",
      "Total Loss: 926678.125\n",
      "\n",
      "Total Fwd Loss: 6.8166022300720215\n",
      "Total Bwd Loss: 8.60411548614502\n",
      "Total Inv Consistency Loss: 926662.6875\n",
      "Total Temp Consistency Loss: 0.024219244718551636\n",
      "Total Loss: 926430.125\n",
      "\n",
      "Total Fwd Loss: 6.9142656326293945\n",
      "Total Bwd Loss: 7.378993034362793\n",
      "Total Inv Consistency Loss: 926415.8125\n",
      "Total Temp Consistency Loss: 0.022667240351438522\n",
      "Total Loss: 926192.75\n",
      "\n",
      "Total Fwd Loss: 7.948330879211426\n",
      "Total Bwd Loss: 7.283164978027344\n",
      "Total Inv Consistency Loss: 926177.5\n",
      "Total Temp Consistency Loss: 0.021218549460172653\n",
      "Total Loss: 925964.3125\n",
      "\n",
      "Total Fwd Loss: 9.34190845489502\n",
      "Total Bwd Loss: 9.073214530944824\n",
      "Total Inv Consistency Loss: 925945.875\n",
      "Total Temp Consistency Loss: 0.0209578238427639\n",
      "----------Training epoch--------\n",
      "----------------108---------------\n",
      "\n",
      "Total Loss: 925717.5\n",
      "\n",
      "Total Fwd Loss: 7.8104376792907715\n",
      "Total Bwd Loss: 7.807314872741699\n",
      "Total Inv Consistency Loss: 925701.875\n",
      "Total Temp Consistency Loss: 0.0210292749106884\n",
      "Total Loss: 925481.0625\n",
      "\n",
      "Total Fwd Loss: 7.42733097076416\n",
      "Total Bwd Loss: 7.9434685707092285\n",
      "Total Inv Consistency Loss: 925465.6875\n",
      "Total Temp Consistency Loss: 0.022828059270977974\n",
      "Total Loss: 925246.875\n",
      "\n",
      "Total Fwd Loss: 7.859177589416504\n",
      "Total Bwd Loss: 8.362107276916504\n",
      "Total Inv Consistency Loss: 925230.625\n",
      "Total Temp Consistency Loss: 0.021923279389739037\n",
      "Total Loss: 925002.125\n",
      "\n",
      "Total Fwd Loss: 7.098133087158203\n",
      "Total Bwd Loss: 7.2178955078125\n",
      "Total Inv Consistency Loss: 924987.8125\n",
      "Total Temp Consistency Loss: 0.021522754803299904\n",
      "Total Loss: 924765.75\n",
      "\n",
      "Total Fwd Loss: 9.166982650756836\n",
      "Total Bwd Loss: 8.187715530395508\n",
      "Total Inv Consistency Loss: 924748.375\n",
      "Total Temp Consistency Loss: 0.021036718040704727\n",
      "Total Loss: 924526.875\n",
      "\n",
      "Total Fwd Loss: 7.706060886383057\n",
      "Total Bwd Loss: 9.33463191986084\n",
      "Total Inv Consistency Loss: 924509.8125\n",
      "Total Temp Consistency Loss: 0.023568781092762947\n",
      "----------Training epoch--------\n",
      "----------------109---------------\n",
      "\n",
      "Total Loss: 924288.6875\n",
      "\n",
      "Total Fwd Loss: 7.137157440185547\n",
      "Total Bwd Loss: 7.396939754486084\n",
      "Total Inv Consistency Loss: 924274.125\n",
      "Total Temp Consistency Loss: 0.021513501182198524\n",
      "Total Loss: 924058.0\n",
      "\n",
      "Total Fwd Loss: 6.889057159423828\n",
      "Total Bwd Loss: 7.956918239593506\n",
      "Total Inv Consistency Loss: 924043.125\n",
      "Total Temp Consistency Loss: 0.02312310039997101\n",
      "Total Loss: 923815.1875\n",
      "\n",
      "Total Fwd Loss: 7.531532287597656\n",
      "Total Bwd Loss: 7.765649318695068\n",
      "Total Inv Consistency Loss: 923799.875\n",
      "Total Temp Consistency Loss: 0.02108599618077278\n",
      "Total Loss: 923578.9375\n",
      "\n",
      "Total Fwd Loss: 7.90691614151001\n",
      "Total Bwd Loss: 9.235074996948242\n",
      "Total Inv Consistency Loss: 923561.8125\n",
      "Total Temp Consistency Loss: 0.022559555247426033\n",
      "Total Loss: 923341.6875\n",
      "\n",
      "Total Fwd Loss: 8.307198524475098\n",
      "Total Bwd Loss: 8.209900856018066\n",
      "Total Inv Consistency Loss: 923325.1875\n",
      "Total Temp Consistency Loss: 0.021747618913650513\n",
      "Total Loss: 923104.625\n",
      "\n",
      "Total Fwd Loss: 9.214972496032715\n",
      "Total Bwd Loss: 8.400049209594727\n",
      "Total Inv Consistency Loss: 923087.0\n",
      "Total Temp Consistency Loss: 0.019896801561117172\n",
      "----------Training epoch--------\n",
      "----------------110---------------\n",
      "\n",
      "Total Loss: 922865.4375\n",
      "\n",
      "Total Fwd Loss: 8.3698091506958\n",
      "Total Bwd Loss: 8.450345993041992\n",
      "Total Inv Consistency Loss: 922848.625\n",
      "Total Temp Consistency Loss: 0.02245219610631466\n",
      "Total Loss: 922624.5\n",
      "\n",
      "Total Fwd Loss: 8.045705795288086\n",
      "Total Bwd Loss: 9.305065155029297\n",
      "Total Inv Consistency Loss: 922607.125\n",
      "Total Temp Consistency Loss: 0.022083664312958717\n",
      "Total Loss: 922393.125\n",
      "\n",
      "Total Fwd Loss: 7.7257280349731445\n",
      "Total Bwd Loss: 7.224420070648193\n",
      "Total Inv Consistency Loss: 922378.1875\n",
      "Total Temp Consistency Loss: 0.021192414686083794\n",
      "Total Loss: 922152.8125\n",
      "\n",
      "Total Fwd Loss: 6.8155694007873535\n",
      "Total Bwd Loss: 7.999439239501953\n",
      "Total Inv Consistency Loss: 922138.0\n",
      "Total Temp Consistency Loss: 0.021400893107056618\n",
      "Total Loss: 921917.0625\n",
      "\n",
      "Total Fwd Loss: 8.454458236694336\n",
      "Total Bwd Loss: 7.984255790710449\n",
      "Total Inv Consistency Loss: 921900.625\n",
      "Total Temp Consistency Loss: 0.019894560799002647\n",
      "Total Loss: 921682.4375\n",
      "\n",
      "Total Fwd Loss: 7.645468711853027\n",
      "Total Bwd Loss: 7.931451320648193\n",
      "Total Inv Consistency Loss: 921666.875\n",
      "Total Temp Consistency Loss: 0.022426124662160873\n",
      "----------Training epoch--------\n",
      "----------------111---------------\n",
      "\n",
      "Total Loss: 921441.1875\n",
      "\n",
      "Total Fwd Loss: 7.048018455505371\n",
      "Total Bwd Loss: 7.665738582611084\n",
      "Total Inv Consistency Loss: 921426.5\n",
      "Total Temp Consistency Loss: 0.01969217322766781\n",
      "Total Loss: 921196.5\n",
      "\n",
      "Total Fwd Loss: 7.397271633148193\n",
      "Total Bwd Loss: 7.129983425140381\n",
      "Total Inv Consistency Loss: 921182.0\n",
      "Total Temp Consistency Loss: 0.020584072917699814\n",
      "Total Loss: 920972.8125\n",
      "\n",
      "Total Fwd Loss: 8.986943244934082\n",
      "Total Bwd Loss: 8.726412773132324\n",
      "Total Inv Consistency Loss: 920955.125\n",
      "Total Temp Consistency Loss: 0.021535713225603104\n",
      "Total Loss: 920735.75\n",
      "\n",
      "Total Fwd Loss: 8.558615684509277\n",
      "Total Bwd Loss: 8.552083969116211\n",
      "Total Inv Consistency Loss: 920718.625\n",
      "Total Temp Consistency Loss: 0.02071523107588291\n",
      "Total Loss: 920499.1875\n",
      "\n",
      "Total Fwd Loss: 7.449987888336182\n",
      "Total Bwd Loss: 8.742332458496094\n",
      "Total Inv Consistency Loss: 920483.0\n",
      "Total Temp Consistency Loss: 0.023173030465841293\n",
      "Total Loss: 920254.0\n",
      "\n",
      "Total Fwd Loss: 7.741268157958984\n",
      "Total Bwd Loss: 8.11218547821045\n",
      "Total Inv Consistency Loss: 920238.125\n",
      "Total Temp Consistency Loss: 0.02306237444281578\n",
      "----------Training epoch--------\n",
      "----------------112---------------\n",
      "\n",
      "Total Loss: 920017.3125\n",
      "\n",
      "Total Fwd Loss: 9.368937492370605\n",
      "Total Bwd Loss: 9.10216999053955\n",
      "Total Inv Consistency Loss: 919998.8125\n",
      "Total Temp Consistency Loss: 0.02107628807425499\n",
      "Total Loss: 919784.9375\n",
      "\n",
      "Total Fwd Loss: 7.951946258544922\n",
      "Total Bwd Loss: 8.658987045288086\n",
      "Total Inv Consistency Loss: 919768.3125\n",
      "Total Temp Consistency Loss: 0.021384170278906822\n",
      "Total Loss: 919540.4375\n",
      "\n",
      "Total Fwd Loss: 8.128522872924805\n",
      "Total Bwd Loss: 7.487471580505371\n",
      "Total Inv Consistency Loss: 919524.8125\n",
      "Total Temp Consistency Loss: 0.021751876920461655\n",
      "Total Loss: 919304.125\n",
      "\n",
      "Total Fwd Loss: 7.291132926940918\n",
      "Total Bwd Loss: 8.471870422363281\n",
      "Total Inv Consistency Loss: 919288.375\n",
      "Total Temp Consistency Loss: 0.023208264261484146\n",
      "Total Loss: 919071.125\n",
      "\n",
      "Total Fwd Loss: 7.083254814147949\n",
      "Total Bwd Loss: 7.5598273277282715\n",
      "Total Inv Consistency Loss: 919056.5\n",
      "Total Temp Consistency Loss: 0.022223705425858498\n",
      "Total Loss: 918836.1875\n",
      "\n",
      "Total Fwd Loss: 7.4226274490356445\n",
      "Total Bwd Loss: 7.777486324310303\n",
      "Total Inv Consistency Loss: 918821.0\n",
      "Total Temp Consistency Loss: 0.022616785019636154\n",
      "----------Training epoch--------\n",
      "----------------113---------------\n",
      "\n",
      "Total Loss: 918598.875\n",
      "\n",
      "Total Fwd Loss: 8.273618698120117\n",
      "Total Bwd Loss: 8.457425117492676\n",
      "Total Inv Consistency Loss: 918582.125\n",
      "Total Temp Consistency Loss: 0.020974744111299515\n",
      "Total Loss: 918364.625\n",
      "\n",
      "Total Fwd Loss: 9.387812614440918\n",
      "Total Bwd Loss: 8.115629196166992\n",
      "Total Inv Consistency Loss: 918347.125\n",
      "Total Temp Consistency Loss: 0.0199192576110363\n",
      "Total Loss: 918125.4375\n",
      "\n",
      "Total Fwd Loss: 7.63274621963501\n",
      "Total Bwd Loss: 7.494540214538574\n",
      "Total Inv Consistency Loss: 918110.3125\n",
      "Total Temp Consistency Loss: 0.021488867700099945\n",
      "Total Loss: 917897.4375\n",
      "\n",
      "Total Fwd Loss: 6.3995795249938965\n",
      "Total Bwd Loss: 7.722513675689697\n",
      "Total Inv Consistency Loss: 917883.3125\n",
      "Total Temp Consistency Loss: 0.021681753918528557\n",
      "Total Loss: 917654.5625\n",
      "\n",
      "Total Fwd Loss: 7.295477390289307\n",
      "Total Bwd Loss: 8.937463760375977\n",
      "Total Inv Consistency Loss: 917638.3125\n",
      "Total Temp Consistency Loss: 0.022677617147564888\n",
      "Total Loss: 917415.9375\n",
      "\n",
      "Total Fwd Loss: 8.078775405883789\n",
      "Total Bwd Loss: 8.148701667785645\n",
      "Total Inv Consistency Loss: 917399.6875\n",
      "Total Temp Consistency Loss: 0.019236741587519646\n",
      "----------Training epoch--------\n",
      "----------------114---------------\n",
      "\n",
      "Total Loss: 917177.0\n",
      "\n",
      "Total Fwd Loss: 8.250051498413086\n",
      "Total Bwd Loss: 8.870443344116211\n",
      "Total Inv Consistency Loss: 917159.875\n",
      "Total Temp Consistency Loss: 0.021804314106702805\n",
      "Total Loss: 916943.125\n",
      "\n",
      "Total Fwd Loss: 7.903270721435547\n",
      "Total Bwd Loss: 6.938845157623291\n",
      "Total Inv Consistency Loss: 916928.3125\n",
      "Total Temp Consistency Loss: 0.019449453800916672\n",
      "Total Loss: 916715.25\n",
      "\n",
      "Total Fwd Loss: 6.596499443054199\n",
      "Total Bwd Loss: 8.772981643676758\n",
      "Total Inv Consistency Loss: 916699.875\n",
      "Total Temp Consistency Loss: 0.021011512726545334\n",
      "Total Loss: 916477.0625\n",
      "\n",
      "Total Fwd Loss: 9.784273147583008\n",
      "Total Bwd Loss: 9.09843635559082\n",
      "Total Inv Consistency Loss: 916458.1875\n",
      "Total Temp Consistency Loss: 0.018987102434039116\n",
      "Total Loss: 916236.3125\n",
      "\n",
      "Total Fwd Loss: 6.544347286224365\n",
      "Total Bwd Loss: 8.112524032592773\n",
      "Total Inv Consistency Loss: 916221.625\n",
      "Total Temp Consistency Loss: 0.020993057638406754\n",
      "Total Loss: 916006.9375\n",
      "\n",
      "Total Fwd Loss: 8.01972770690918\n",
      "Total Bwd Loss: 7.020249366760254\n",
      "Total Inv Consistency Loss: 915991.875\n",
      "Total Temp Consistency Loss: 0.01990865170955658\n",
      "----------Training epoch--------\n",
      "----------------115---------------\n",
      "\n",
      "Total Loss: 915769.8125\n",
      "\n",
      "Total Fwd Loss: 7.934815406799316\n",
      "Total Bwd Loss: 9.236710548400879\n",
      "Total Inv Consistency Loss: 915752.625\n",
      "Total Temp Consistency Loss: 0.022172026336193085\n",
      "Total Loss: 915528.75\n",
      "\n",
      "Total Fwd Loss: 6.748166561126709\n",
      "Total Bwd Loss: 6.828024864196777\n",
      "Total Inv Consistency Loss: 915515.1875\n",
      "Total Temp Consistency Loss: 0.01939893141388893\n",
      "Total Loss: 915292.875\n",
      "\n",
      "Total Fwd Loss: 7.763003349304199\n",
      "Total Bwd Loss: 7.815997123718262\n",
      "Total Inv Consistency Loss: 915277.3125\n",
      "Total Temp Consistency Loss: 0.019207663834095\n",
      "Total Loss: 915066.375\n",
      "\n",
      "Total Fwd Loss: 9.79588794708252\n",
      "Total Bwd Loss: 8.716015815734863\n",
      "Total Inv Consistency Loss: 915047.875\n",
      "Total Temp Consistency Loss: 0.019427072256803513\n",
      "Total Loss: 914825.625\n",
      "\n",
      "Total Fwd Loss: 7.08059549331665\n",
      "Total Bwd Loss: 7.3816633224487305\n",
      "Total Inv Consistency Loss: 914811.1875\n",
      "Total Temp Consistency Loss: 0.01940111443400383\n",
      "Total Loss: 914590.1875\n",
      "\n",
      "Total Fwd Loss: 7.8247389793396\n",
      "Total Bwd Loss: 8.762327194213867\n",
      "Total Inv Consistency Loss: 914573.625\n",
      "Total Temp Consistency Loss: 0.019941309466958046\n",
      "----------Training epoch--------\n",
      "----------------116---------------\n",
      "\n",
      "Total Loss: 914350.8125\n",
      "\n",
      "Total Fwd Loss: 8.01138687133789\n",
      "Total Bwd Loss: 8.468074798583984\n",
      "Total Inv Consistency Loss: 914334.3125\n",
      "Total Temp Consistency Loss: 0.020181410014629364\n",
      "Total Loss: 914121.4375\n",
      "\n",
      "Total Fwd Loss: 8.95504379272461\n",
      "Total Bwd Loss: 8.50634765625\n",
      "Total Inv Consistency Loss: 914104.0\n",
      "Total Temp Consistency Loss: 0.01956101879477501\n",
      "Total Loss: 913879.9375\n",
      "\n",
      "Total Fwd Loss: 7.396554470062256\n",
      "Total Bwd Loss: 7.833664894104004\n",
      "Total Inv Consistency Loss: 913864.6875\n",
      "Total Temp Consistency Loss: 0.01952454261481762\n",
      "Total Loss: 913650.8125\n",
      "\n",
      "Total Fwd Loss: 7.485301971435547\n",
      "Total Bwd Loss: 7.442002296447754\n",
      "Total Inv Consistency Loss: 913635.875\n",
      "Total Temp Consistency Loss: 0.01845230907201767\n",
      "Total Loss: 913423.0\n",
      "\n",
      "Total Fwd Loss: 7.299738883972168\n",
      "Total Bwd Loss: 8.29734992980957\n",
      "Total Inv Consistency Loss: 913407.375\n",
      "Total Temp Consistency Loss: 0.02068130113184452\n",
      "Total Loss: 913185.125\n",
      "\n",
      "Total Fwd Loss: 7.842967987060547\n",
      "Total Bwd Loss: 8.290304183959961\n",
      "Total Inv Consistency Loss: 913169.0\n",
      "Total Temp Consistency Loss: 0.018412400037050247\n",
      "----------Training epoch--------\n",
      "----------------117---------------\n",
      "\n",
      "Total Loss: 912941.3125\n",
      "\n",
      "Total Fwd Loss: 8.129364967346191\n",
      "Total Bwd Loss: 7.803379058837891\n",
      "Total Inv Consistency Loss: 912925.375\n",
      "Total Temp Consistency Loss: 0.019147474318742752\n",
      "Total Loss: 912712.25\n",
      "\n",
      "Total Fwd Loss: 8.097238540649414\n",
      "Total Bwd Loss: 8.164864540100098\n",
      "Total Inv Consistency Loss: 912696.0\n",
      "Total Temp Consistency Loss: 0.020245449617505074\n",
      "Total Loss: 912473.875\n",
      "\n",
      "Total Fwd Loss: 6.237262725830078\n",
      "Total Bwd Loss: 7.157131195068359\n",
      "Total Inv Consistency Loss: 912460.5\n",
      "Total Temp Consistency Loss: 0.017861630767583847\n",
      "Total Loss: 912241.3125\n",
      "\n",
      "Total Fwd Loss: 7.812094211578369\n",
      "Total Bwd Loss: 8.023035049438477\n",
      "Total Inv Consistency Loss: 912225.5\n",
      "Total Temp Consistency Loss: 0.019381294026970863\n",
      "Total Loss: 912010.3125\n",
      "\n",
      "Total Fwd Loss: 8.771430969238281\n",
      "Total Bwd Loss: 8.827656745910645\n",
      "Total Inv Consistency Loss: 911992.6875\n",
      "Total Temp Consistency Loss: 0.01962720789015293\n",
      "Total Loss: 911775.0625\n",
      "\n",
      "Total Fwd Loss: 7.9898200035095215\n",
      "Total Bwd Loss: 8.879018783569336\n",
      "Total Inv Consistency Loss: 911758.1875\n",
      "Total Temp Consistency Loss: 0.01940375566482544\n",
      "----------Training epoch--------\n",
      "----------------118---------------\n",
      "\n",
      "Total Loss: 911533.0\n",
      "\n",
      "Total Fwd Loss: 7.002831935882568\n",
      "Total Bwd Loss: 8.204660415649414\n",
      "Total Inv Consistency Loss: 911517.8125\n",
      "Total Temp Consistency Loss: 0.019395118579268456\n",
      "Total Loss: 911303.9375\n",
      "\n",
      "Total Fwd Loss: 7.4191412925720215\n",
      "Total Bwd Loss: 8.733377456665039\n",
      "Total Inv Consistency Loss: 911287.8125\n",
      "Total Temp Consistency Loss: 0.01937435194849968\n",
      "Total Loss: 911070.3125\n",
      "\n",
      "Total Fwd Loss: 8.591353416442871\n",
      "Total Bwd Loss: 8.049700736999512\n",
      "Total Inv Consistency Loss: 911053.6875\n",
      "Total Temp Consistency Loss: 0.018488746136426926\n",
      "Total Loss: 910833.5625\n",
      "\n",
      "Total Fwd Loss: 9.404460906982422\n",
      "Total Bwd Loss: 9.310404777526855\n",
      "Total Inv Consistency Loss: 910814.875\n",
      "Total Temp Consistency Loss: 0.020554039627313614\n",
      "Total Loss: 910594.5625\n",
      "\n",
      "Total Fwd Loss: 6.830499172210693\n",
      "Total Bwd Loss: 7.049382209777832\n",
      "Total Inv Consistency Loss: 910580.6875\n",
      "Total Temp Consistency Loss: 0.01956118829548359\n",
      "Total Loss: 910360.5625\n",
      "\n",
      "Total Fwd Loss: 7.922205448150635\n",
      "Total Bwd Loss: 7.520460605621338\n",
      "Total Inv Consistency Loss: 910345.125\n",
      "Total Temp Consistency Loss: 0.019601386040449142\n",
      "----------Training epoch--------\n",
      "----------------119---------------\n",
      "\n",
      "Total Loss: 910127.75\n",
      "\n",
      "Total Fwd Loss: 6.901463985443115\n",
      "Total Bwd Loss: 7.328138828277588\n",
      "Total Inv Consistency Loss: 910113.5\n",
      "Total Temp Consistency Loss: 0.01934502087533474\n",
      "Total Loss: 909901.0625\n",
      "\n",
      "Total Fwd Loss: 7.026121616363525\n",
      "Total Bwd Loss: 8.359688758850098\n",
      "Total Inv Consistency Loss: 909885.6875\n",
      "Total Temp Consistency Loss: 0.02134271338582039\n",
      "Total Loss: 909663.75\n",
      "\n",
      "Total Fwd Loss: 7.6663618087768555\n",
      "Total Bwd Loss: 8.19489860534668\n",
      "Total Inv Consistency Loss: 909647.875\n",
      "Total Temp Consistency Loss: 0.019091594964265823\n",
      "Total Loss: 909427.0625\n",
      "\n",
      "Total Fwd Loss: 7.436103820800781\n",
      "Total Bwd Loss: 7.775579929351807\n",
      "Total Inv Consistency Loss: 909411.875\n",
      "Total Temp Consistency Loss: 0.01921231485903263\n",
      "Total Loss: 909198.0625\n",
      "\n",
      "Total Fwd Loss: 9.57973575592041\n",
      "Total Bwd Loss: 8.962190628051758\n",
      "Total Inv Consistency Loss: 909179.5\n",
      "Total Temp Consistency Loss: 0.019365757703781128\n",
      "Total Loss: 908964.5\n",
      "\n",
      "Total Fwd Loss: 8.58025074005127\n",
      "Total Bwd Loss: 8.201428413391113\n",
      "Total Inv Consistency Loss: 908947.6875\n",
      "Total Temp Consistency Loss: 0.020189857110381126\n",
      "----------Training epoch--------\n",
      "----------------120---------------\n",
      "\n",
      "Total Loss: 908727.5\n",
      "\n",
      "Total Fwd Loss: 7.075535774230957\n",
      "Total Bwd Loss: 7.91138219833374\n",
      "Total Inv Consistency Loss: 908712.5\n",
      "Total Temp Consistency Loss: 0.01960364170372486\n",
      "Total Loss: 908494.9375\n",
      "\n",
      "Total Fwd Loss: 7.587161064147949\n",
      "Total Bwd Loss: 6.847692966461182\n",
      "Total Inv Consistency Loss: 908480.5\n",
      "Total Temp Consistency Loss: 0.01809592731297016\n",
      "Total Loss: 908256.6875\n",
      "\n",
      "Total Fwd Loss: 7.95761251449585\n",
      "Total Bwd Loss: 8.057862281799316\n",
      "Total Inv Consistency Loss: 908240.6875\n",
      "Total Temp Consistency Loss: 0.019053149968385696\n",
      "Total Loss: 908024.25\n",
      "\n",
      "Total Fwd Loss: 6.241606712341309\n",
      "Total Bwd Loss: 8.864896774291992\n",
      "Total Inv Consistency Loss: 908009.125\n",
      "Total Temp Consistency Loss: 0.02120477333664894\n",
      "Total Loss: 907796.3125\n",
      "\n",
      "Total Fwd Loss: 9.920757293701172\n",
      "Total Bwd Loss: 8.545018196105957\n",
      "Total Inv Consistency Loss: 907777.875\n",
      "Total Temp Consistency Loss: 0.017515387386083603\n",
      "Total Loss: 907562.3125\n",
      "\n",
      "Total Fwd Loss: 8.396844863891602\n",
      "Total Bwd Loss: 8.556562423706055\n",
      "Total Inv Consistency Loss: 907545.375\n",
      "Total Temp Consistency Loss: 0.018973471596837044\n",
      "----------Training epoch--------\n",
      "----------------121---------------\n",
      "\n",
      "Total Loss: 907324.0\n",
      "\n",
      "Total Fwd Loss: 7.347858428955078\n",
      "Total Bwd Loss: 7.941129207611084\n",
      "Total Inv Consistency Loss: 907308.6875\n",
      "Total Temp Consistency Loss: 0.02055049128830433\n",
      "Total Loss: 907137.9375\n",
      "\n",
      "Total Fwd Loss: 8.199482917785645\n",
      "Total Bwd Loss: 7.597704887390137\n",
      "Total Inv Consistency Loss: 907122.125\n",
      "Total Temp Consistency Loss: 0.01871715672314167\n",
      "Total Loss: 906952.375\n",
      "\n",
      "Total Fwd Loss: 7.232944488525391\n",
      "Total Bwd Loss: 7.969834327697754\n",
      "Total Inv Consistency Loss: 906937.1875\n",
      "Total Temp Consistency Loss: 0.017817530781030655\n",
      "Total Loss: 906763.875\n",
      "\n",
      "Total Fwd Loss: 9.15678596496582\n",
      "Total Bwd Loss: 9.208662033081055\n",
      "Total Inv Consistency Loss: 906745.5\n",
      "Total Temp Consistency Loss: 0.01945021003484726\n",
      "Total Loss: 906575.875\n",
      "\n",
      "Total Fwd Loss: 6.742350101470947\n",
      "Total Bwd Loss: 7.155298709869385\n",
      "Total Inv Consistency Loss: 906562.0\n",
      "Total Temp Consistency Loss: 0.017738038673996925\n",
      "Total Loss: 906395.8125\n",
      "\n",
      "Total Fwd Loss: 8.293920516967773\n",
      "Total Bwd Loss: 8.880494117736816\n",
      "Total Inv Consistency Loss: 906378.625\n",
      "Total Temp Consistency Loss: 0.01799703761935234\n",
      "----------Training epoch--------\n",
      "----------------122---------------\n",
      "\n",
      "Total Loss: 906205.6875\n",
      "\n",
      "Total Fwd Loss: 8.078818321228027\n",
      "Total Bwd Loss: 8.968154907226562\n",
      "Total Inv Consistency Loss: 906188.625\n",
      "Total Temp Consistency Loss: 0.019977081567049026\n",
      "Total Loss: 906023.875\n",
      "\n",
      "Total Fwd Loss: 7.070690155029297\n",
      "Total Bwd Loss: 7.413599491119385\n",
      "Total Inv Consistency Loss: 906009.375\n",
      "Total Temp Consistency Loss: 0.01861317828297615\n",
      "Total Loss: 905829.5625\n",
      "\n",
      "Total Fwd Loss: 7.422646522521973\n",
      "Total Bwd Loss: 7.262551307678223\n",
      "Total Inv Consistency Loss: 905814.875\n",
      "Total Temp Consistency Loss: 0.01793094351887703\n",
      "Total Loss: 905645.6875\n",
      "\n",
      "Total Fwd Loss: 7.076381683349609\n",
      "Total Bwd Loss: 8.898369789123535\n",
      "Total Inv Consistency Loss: 905629.6875\n",
      "Total Temp Consistency Loss: 0.018781166523694992\n",
      "Total Loss: 905458.25\n",
      "\n",
      "Total Fwd Loss: 8.375191688537598\n",
      "Total Bwd Loss: 8.062200546264648\n",
      "Total Inv Consistency Loss: 905441.8125\n",
      "Total Temp Consistency Loss: 0.018401069566607475\n",
      "Total Loss: 905274.75\n",
      "\n",
      "Total Fwd Loss: 9.118438720703125\n",
      "Total Bwd Loss: 8.249338150024414\n",
      "Total Inv Consistency Loss: 905257.375\n",
      "Total Temp Consistency Loss: 0.016999563202261925\n",
      "----------Training epoch--------\n",
      "----------------123---------------\n",
      "\n",
      "Total Loss: 905093.5625\n",
      "\n",
      "Total Fwd Loss: 9.184577941894531\n",
      "Total Bwd Loss: 9.867029190063477\n",
      "Total Inv Consistency Loss: 905074.5\n",
      "Total Temp Consistency Loss: 0.018824126571416855\n",
      "Total Loss: 904898.125\n",
      "\n",
      "Total Fwd Loss: 7.18295955657959\n",
      "Total Bwd Loss: 7.463189125061035\n",
      "Total Inv Consistency Loss: 904883.5\n",
      "Total Temp Consistency Loss: 0.019448012113571167\n",
      "Total Loss: 904717.3125\n",
      "\n",
      "Total Fwd Loss: 7.397231101989746\n",
      "Total Bwd Loss: 8.20388412475586\n",
      "Total Inv Consistency Loss: 904701.6875\n",
      "Total Temp Consistency Loss: 0.017574112862348557\n",
      "Total Loss: 904526.5\n",
      "\n",
      "Total Fwd Loss: 6.949446201324463\n",
      "Total Bwd Loss: 8.231966018676758\n",
      "Total Inv Consistency Loss: 904511.3125\n",
      "Total Temp Consistency Loss: 0.018291423097252846\n",
      "Total Loss: 904345.0\n",
      "\n",
      "Total Fwd Loss: 7.331437110900879\n",
      "Total Bwd Loss: 7.309136867523193\n",
      "Total Inv Consistency Loss: 904330.375\n",
      "Total Temp Consistency Loss: 0.018206855282187462\n",
      "Total Loss: 904158.5625\n",
      "\n",
      "Total Fwd Loss: 8.974691390991211\n",
      "Total Bwd Loss: 7.759799003601074\n",
      "Total Inv Consistency Loss: 904141.8125\n",
      "Total Temp Consistency Loss: 0.01710633561015129\n",
      "----------Training epoch--------\n",
      "----------------124---------------\n",
      "\n",
      "Total Loss: 903970.5\n",
      "\n",
      "Total Fwd Loss: 8.350496292114258\n",
      "Total Bwd Loss: 9.040189743041992\n",
      "Total Inv Consistency Loss: 903953.125\n",
      "Total Temp Consistency Loss: 0.01864544488489628\n",
      "Total Loss: 903783.5625\n",
      "\n",
      "Total Fwd Loss: 7.0750579833984375\n",
      "Total Bwd Loss: 8.010263442993164\n",
      "Total Inv Consistency Loss: 903768.5\n",
      "Total Temp Consistency Loss: 0.017697518691420555\n",
      "Total Loss: 903599.9375\n",
      "\n",
      "Total Fwd Loss: 7.770071506500244\n",
      "Total Bwd Loss: 7.2753448486328125\n",
      "Total Inv Consistency Loss: 903584.875\n",
      "Total Temp Consistency Loss: 0.018645038828253746\n",
      "Total Loss: 903412.3125\n",
      "\n",
      "Total Fwd Loss: 7.8778791427612305\n",
      "Total Bwd Loss: 8.428897857666016\n",
      "Total Inv Consistency Loss: 903396.0\n",
      "Total Temp Consistency Loss: 0.01791468821465969\n",
      "Total Loss: 903230.625\n",
      "\n",
      "Total Fwd Loss: 8.591593742370605\n",
      "Total Bwd Loss: 8.682164192199707\n",
      "Total Inv Consistency Loss: 903213.375\n",
      "Total Temp Consistency Loss: 0.018208278343081474\n",
      "Total Loss: 903038.5\n",
      "\n",
      "Total Fwd Loss: 7.307350158691406\n",
      "Total Bwd Loss: 7.357992649078369\n",
      "Total Inv Consistency Loss: 903023.8125\n",
      "Total Temp Consistency Loss: 0.018662866204977036\n",
      "----------Training epoch--------\n",
      "----------------125---------------\n",
      "\n",
      "Total Loss: 902856.0\n",
      "\n",
      "Total Fwd Loss: 9.12309741973877\n",
      "Total Bwd Loss: 8.075376510620117\n",
      "Total Inv Consistency Loss: 902838.8125\n",
      "Total Temp Consistency Loss: 0.017506901174783707\n",
      "Total Loss: 902663.3125\n",
      "\n",
      "Total Fwd Loss: 6.347202301025391\n",
      "Total Bwd Loss: 8.320958137512207\n",
      "Total Inv Consistency Loss: 902648.625\n",
      "Total Temp Consistency Loss: 0.01968138851225376\n",
      "Total Loss: 902484.75\n",
      "\n",
      "Total Fwd Loss: 7.780329704284668\n",
      "Total Bwd Loss: 8.352453231811523\n",
      "Total Inv Consistency Loss: 902468.625\n",
      "Total Temp Consistency Loss: 0.02013992890715599\n",
      "Total Loss: 902298.6875\n",
      "\n",
      "Total Fwd Loss: 9.01387882232666\n",
      "Total Bwd Loss: 8.039712905883789\n",
      "Total Inv Consistency Loss: 902281.625\n",
      "Total Temp Consistency Loss: 0.018391069024801254\n",
      "Total Loss: 902111.6875\n",
      "\n",
      "Total Fwd Loss: 7.523697853088379\n",
      "Total Bwd Loss: 6.66296911239624\n",
      "Total Inv Consistency Loss: 902097.5\n",
      "Total Temp Consistency Loss: 0.017368517816066742\n",
      "Total Loss: 901924.9375\n",
      "\n",
      "Total Fwd Loss: 7.169084072113037\n",
      "Total Bwd Loss: 9.419965744018555\n",
      "Total Inv Consistency Loss: 901908.375\n",
      "Total Temp Consistency Loss: 0.019420623779296875\n",
      "----------Training epoch--------\n",
      "----------------126---------------\n",
      "\n",
      "Total Loss: 901742.8125\n",
      "\n",
      "Total Fwd Loss: 8.229691505432129\n",
      "Total Bwd Loss: 8.436371803283691\n",
      "Total Inv Consistency Loss: 901726.125\n",
      "Total Temp Consistency Loss: 0.01864336058497429\n",
      "Total Loss: 901555.8125\n",
      "\n",
      "Total Fwd Loss: 9.109404563903809\n",
      "Total Bwd Loss: 7.917601108551025\n",
      "Total Inv Consistency Loss: 901538.8125\n",
      "Total Temp Consistency Loss: 0.01700567454099655\n",
      "Total Loss: 901370.0625\n",
      "\n",
      "Total Fwd Loss: 8.08049201965332\n",
      "Total Bwd Loss: 7.677634239196777\n",
      "Total Inv Consistency Loss: 901354.3125\n",
      "Total Temp Consistency Loss: 0.018428364768624306\n",
      "Total Loss: 901183.125\n",
      "\n",
      "Total Fwd Loss: 7.39594030380249\n",
      "Total Bwd Loss: 7.445518493652344\n",
      "Total Inv Consistency Loss: 901168.3125\n",
      "Total Temp Consistency Loss: 0.018433818593621254\n",
      "Total Loss: 900999.875\n",
      "\n",
      "Total Fwd Loss: 6.599447727203369\n",
      "Total Bwd Loss: 8.9368314743042\n",
      "Total Inv Consistency Loss: 900984.3125\n",
      "Total Temp Consistency Loss: 0.02016846090555191\n",
      "Total Loss: 900815.0\n",
      "\n",
      "Total Fwd Loss: 7.577034950256348\n",
      "Total Bwd Loss: 8.442780494689941\n",
      "Total Inv Consistency Loss: 900799.0\n",
      "Total Temp Consistency Loss: 0.021097559481859207\n",
      "----------Training epoch--------\n",
      "----------------127---------------\n",
      "\n",
      "Total Loss: 900627.1875\n",
      "\n",
      "Total Fwd Loss: 8.873800277709961\n",
      "Total Bwd Loss: 7.410177707672119\n",
      "Total Inv Consistency Loss: 900610.875\n",
      "Total Temp Consistency Loss: 0.01788787916302681\n",
      "Total Loss: 900449.0\n",
      "\n",
      "Total Fwd Loss: 8.49753189086914\n",
      "Total Bwd Loss: 9.659692764282227\n",
      "Total Inv Consistency Loss: 900430.8125\n",
      "Total Temp Consistency Loss: 0.01947731152176857\n",
      "Total Loss: 900253.0625\n",
      "\n",
      "Total Fwd Loss: 7.898255348205566\n",
      "Total Bwd Loss: 7.656540870666504\n",
      "Total Inv Consistency Loss: 900237.5\n",
      "Total Temp Consistency Loss: 0.016998032107949257\n",
      "Total Loss: 900066.6875\n",
      "\n",
      "Total Fwd Loss: 6.677101135253906\n",
      "Total Bwd Loss: 7.026950836181641\n",
      "Total Inv Consistency Loss: 900053.0\n",
      "Total Temp Consistency Loss: 0.016483796760439873\n",
      "Total Loss: 899884.0\n",
      "\n",
      "Total Fwd Loss: 7.2143049240112305\n",
      "Total Bwd Loss: 8.087015151977539\n",
      "Total Inv Consistency Loss: 899868.6875\n",
      "Total Temp Consistency Loss: 0.019923102110624313\n",
      "Total Loss: 899699.9375\n",
      "\n",
      "Total Fwd Loss: 7.845892429351807\n",
      "Total Bwd Loss: 9.066741943359375\n",
      "Total Inv Consistency Loss: 899683.0\n",
      "Total Temp Consistency Loss: 0.020776566118001938\n",
      "----------Training epoch--------\n",
      "----------------128---------------\n",
      "\n",
      "Total Loss: 899513.0625\n",
      "\n",
      "Total Fwd Loss: 7.819105625152588\n",
      "Total Bwd Loss: 7.7324538230896\n",
      "Total Inv Consistency Loss: 899497.5\n",
      "Total Temp Consistency Loss: 0.01847413368523121\n",
      "Total Loss: 899327.8125\n",
      "\n",
      "Total Fwd Loss: 7.152815818786621\n",
      "Total Bwd Loss: 9.054594993591309\n",
      "Total Inv Consistency Loss: 899311.625\n",
      "Total Temp Consistency Loss: 0.021238194778561592\n",
      "Total Loss: 899142.8125\n",
      "\n",
      "Total Fwd Loss: 7.034279823303223\n",
      "Total Bwd Loss: 7.455209255218506\n",
      "Total Inv Consistency Loss: 899128.3125\n",
      "Total Temp Consistency Loss: 0.016934648156166077\n",
      "Total Loss: 898961.5\n",
      "\n",
      "Total Fwd Loss: 8.805899620056152\n",
      "Total Bwd Loss: 8.792722702026367\n",
      "Total Inv Consistency Loss: 898943.875\n",
      "Total Temp Consistency Loss: 0.017251932993531227\n",
      "Total Loss: 898780.5625\n",
      "\n",
      "Total Fwd Loss: 8.126660346984863\n",
      "Total Bwd Loss: 7.748991966247559\n",
      "Total Inv Consistency Loss: 898764.6875\n",
      "Total Temp Consistency Loss: 0.01862507313489914\n",
      "Total Loss: 898584.3125\n",
      "\n",
      "Total Fwd Loss: 8.096314430236816\n",
      "Total Bwd Loss: 8.078032493591309\n",
      "Total Inv Consistency Loss: 898568.125\n",
      "Total Temp Consistency Loss: 0.019102221354842186\n",
      "----------Training epoch--------\n",
      "----------------129---------------\n",
      "\n",
      "Total Loss: 898402.5625\n",
      "\n",
      "Total Fwd Loss: 7.315497398376465\n",
      "Total Bwd Loss: 8.127032279968262\n",
      "Total Inv Consistency Loss: 898387.125\n",
      "Total Temp Consistency Loss: 0.018328968435525894\n",
      "Total Loss: 898221.0\n",
      "\n",
      "Total Fwd Loss: 8.134737968444824\n",
      "Total Bwd Loss: 8.074929237365723\n",
      "Total Inv Consistency Loss: 898204.8125\n",
      "Total Temp Consistency Loss: 0.018021877855062485\n",
      "Total Loss: 898035.875\n",
      "\n",
      "Total Fwd Loss: 7.171864986419678\n",
      "Total Bwd Loss: 7.887890815734863\n",
      "Total Inv Consistency Loss: 898020.8125\n",
      "Total Temp Consistency Loss: 0.019719596952199936\n",
      "Total Loss: 897853.625\n",
      "\n",
      "Total Fwd Loss: 8.104866981506348\n",
      "Total Bwd Loss: 8.88566780090332\n",
      "Total Inv Consistency Loss: 897836.625\n",
      "Total Temp Consistency Loss: 0.017399920150637627\n",
      "Total Loss: 897666.125\n",
      "\n",
      "Total Fwd Loss: 8.230737686157227\n",
      "Total Bwd Loss: 7.269965171813965\n",
      "Total Inv Consistency Loss: 897650.625\n",
      "Total Temp Consistency Loss: 0.0170152448117733\n",
      "Total Loss: 897481.0\n",
      "\n",
      "Total Fwd Loss: 8.063145637512207\n",
      "Total Bwd Loss: 8.461522102355957\n",
      "Total Inv Consistency Loss: 897464.5\n",
      "Total Temp Consistency Loss: 0.01986170932650566\n",
      "----------Training epoch--------\n",
      "----------------130---------------\n",
      "\n",
      "Total Loss: 897296.1875\n",
      "\n",
      "Total Fwd Loss: 7.639363765716553\n",
      "Total Bwd Loss: 8.36007022857666\n",
      "Total Inv Consistency Loss: 897280.1875\n",
      "Total Temp Consistency Loss: 0.019506357610225677\n",
      "Total Loss: 897115.375\n",
      "\n",
      "Total Fwd Loss: 7.629024505615234\n",
      "Total Bwd Loss: 7.906512260437012\n",
      "Total Inv Consistency Loss: 897099.8125\n",
      "Total Temp Consistency Loss: 0.01855328120291233\n",
      "Total Loss: 896931.25\n",
      "\n",
      "Total Fwd Loss: 7.562169551849365\n",
      "Total Bwd Loss: 7.973352909088135\n",
      "Total Inv Consistency Loss: 896915.6875\n",
      "Total Temp Consistency Loss: 0.019852664321660995\n",
      "Total Loss: 896748.375\n",
      "\n",
      "Total Fwd Loss: 7.502920627593994\n",
      "Total Bwd Loss: 8.042665481567383\n",
      "Total Inv Consistency Loss: 896732.8125\n",
      "Total Temp Consistency Loss: 0.017514482140541077\n",
      "Total Loss: 896559.6875\n",
      "\n",
      "Total Fwd Loss: 6.942688941955566\n",
      "Total Bwd Loss: 7.769418239593506\n",
      "Total Inv Consistency Loss: 896545.0\n",
      "Total Temp Consistency Loss: 0.018223781138658524\n",
      "Total Loss: 896379.0625\n",
      "\n",
      "Total Fwd Loss: 9.94165325164795\n",
      "Total Bwd Loss: 8.739166259765625\n",
      "Total Inv Consistency Loss: 896360.375\n",
      "Total Temp Consistency Loss: 0.017429525032639503\n",
      "----------Training epoch--------\n",
      "----------------131---------------\n",
      "\n",
      "Total Loss: 896189.4375\n",
      "\n",
      "Total Fwd Loss: 7.7321271896362305\n",
      "Total Bwd Loss: 7.557165622711182\n",
      "Total Inv Consistency Loss: 896174.125\n",
      "Total Temp Consistency Loss: 0.020008331164717674\n",
      "Total Loss: 896008.375\n",
      "\n",
      "Total Fwd Loss: 6.913031578063965\n",
      "Total Bwd Loss: 8.273303985595703\n",
      "Total Inv Consistency Loss: 895993.1875\n",
      "Total Temp Consistency Loss: 0.018452871590852737\n",
      "Total Loss: 895816.75\n",
      "\n",
      "Total Fwd Loss: 8.427045822143555\n",
      "Total Bwd Loss: 7.819146633148193\n",
      "Total Inv Consistency Loss: 895800.5\n",
      "Total Temp Consistency Loss: 0.016417160630226135\n",
      "Total Loss: 895637.0\n",
      "\n",
      "Total Fwd Loss: 7.873304843902588\n",
      "Total Bwd Loss: 7.924029350280762\n",
      "Total Inv Consistency Loss: 895621.1875\n",
      "Total Temp Consistency Loss: 0.018208038061857224\n",
      "Total Loss: 895451.0625\n",
      "\n",
      "Total Fwd Loss: 8.53584098815918\n",
      "Total Bwd Loss: 9.18770694732666\n",
      "Total Inv Consistency Loss: 895433.3125\n",
      "Total Temp Consistency Loss: 0.01718970201909542\n",
      "Total Loss: 895268.3125\n",
      "\n",
      "Total Fwd Loss: 7.63634729385376\n",
      "Total Bwd Loss: 8.03272533416748\n",
      "Total Inv Consistency Loss: 895252.625\n",
      "Total Temp Consistency Loss: 0.01839921437203884\n",
      "----------Training epoch--------\n",
      "----------------132---------------\n",
      "\n",
      "Total Loss: 895080.625\n",
      "\n",
      "Total Fwd Loss: 8.362821578979492\n",
      "Total Bwd Loss: 7.614610195159912\n",
      "Total Inv Consistency Loss: 895064.625\n",
      "Total Temp Consistency Loss: 0.01828749105334282\n",
      "Total Loss: 894896.25\n",
      "\n",
      "Total Fwd Loss: 7.42758846282959\n",
      "Total Bwd Loss: 7.475795745849609\n",
      "Total Inv Consistency Loss: 894881.375\n",
      "Total Temp Consistency Loss: 0.018054518848657608\n",
      "Total Loss: 894719.4375\n",
      "\n",
      "Total Fwd Loss: 7.64893102645874\n",
      "Total Bwd Loss: 8.6935396194458\n",
      "Total Inv Consistency Loss: 894703.125\n",
      "Total Temp Consistency Loss: 0.0189736969769001\n",
      "Total Loss: 894529.4375\n",
      "\n",
      "Total Fwd Loss: 8.810006141662598\n",
      "Total Bwd Loss: 8.519658088684082\n",
      "Total Inv Consistency Loss: 894512.125\n",
      "Total Temp Consistency Loss: 0.019054841250181198\n",
      "Total Loss: 894347.5625\n",
      "\n",
      "Total Fwd Loss: 6.8553667068481445\n",
      "Total Bwd Loss: 8.233558654785156\n",
      "Total Inv Consistency Loss: 894332.5\n",
      "Total Temp Consistency Loss: 0.01761772111058235\n",
      "Total Loss: 894162.875\n",
      "\n",
      "Total Fwd Loss: 7.864926338195801\n",
      "Total Bwd Loss: 8.342700004577637\n",
      "Total Inv Consistency Loss: 894146.6875\n",
      "Total Temp Consistency Loss: 0.018519999459385872\n",
      "----------Training epoch--------\n",
      "----------------133---------------\n",
      "\n",
      "Total Loss: 893976.5625\n",
      "\n",
      "Total Fwd Loss: 6.780223846435547\n",
      "Total Bwd Loss: 6.805989742279053\n",
      "Total Inv Consistency Loss: 893963.0\n",
      "Total Temp Consistency Loss: 0.017461568117141724\n",
      "Total Loss: 893795.875\n",
      "\n",
      "Total Fwd Loss: 7.472447872161865\n",
      "Total Bwd Loss: 8.741698265075684\n",
      "Total Inv Consistency Loss: 893779.6875\n",
      "Total Temp Consistency Loss: 0.017738517373800278\n",
      "Total Loss: 893611.1875\n",
      "\n",
      "Total Fwd Loss: 8.206904411315918\n",
      "Total Bwd Loss: 7.7941412925720215\n",
      "Total Inv Consistency Loss: 893595.1875\n",
      "Total Temp Consistency Loss: 0.018529463559389114\n",
      "Total Loss: 893435.0\n",
      "\n",
      "Total Fwd Loss: 9.763450622558594\n",
      "Total Bwd Loss: 8.830883026123047\n",
      "Total Inv Consistency Loss: 893416.375\n",
      "Total Temp Consistency Loss: 0.01646239310503006\n",
      "Total Loss: 893242.8125\n",
      "\n",
      "Total Fwd Loss: 6.768660068511963\n",
      "Total Bwd Loss: 8.682561874389648\n",
      "Total Inv Consistency Loss: 893227.375\n",
      "Total Temp Consistency Loss: 0.01713547855615616\n",
      "Total Loss: 893057.375\n",
      "\n",
      "Total Fwd Loss: 7.882200717926025\n",
      "Total Bwd Loss: 8.130480766296387\n",
      "Total Inv Consistency Loss: 893041.375\n",
      "Total Temp Consistency Loss: 0.019642041996121407\n",
      "----------Training epoch--------\n",
      "----------------134---------------\n",
      "\n",
      "Total Loss: 892877.375\n",
      "\n",
      "Total Fwd Loss: 7.673024654388428\n",
      "Total Bwd Loss: 7.879785060882568\n",
      "Total Inv Consistency Loss: 892861.8125\n",
      "Total Temp Consistency Loss: 0.018704932183027267\n",
      "Total Loss: 892692.9375\n",
      "\n",
      "Total Fwd Loss: 7.041151523590088\n",
      "Total Bwd Loss: 8.191915512084961\n",
      "Total Inv Consistency Loss: 892677.6875\n",
      "Total Temp Consistency Loss: 0.01853553019464016\n",
      "Total Loss: 892511.8125\n",
      "\n",
      "Total Fwd Loss: 7.542201042175293\n",
      "Total Bwd Loss: 8.892542839050293\n",
      "Total Inv Consistency Loss: 892495.375\n",
      "Total Temp Consistency Loss: 0.019898509606719017\n",
      "Total Loss: 892327.0\n",
      "\n",
      "Total Fwd Loss: 7.923011779785156\n",
      "Total Bwd Loss: 7.2794952392578125\n",
      "Total Inv Consistency Loss: 892311.8125\n",
      "Total Temp Consistency Loss: 0.016277596354484558\n",
      "Total Loss: 892144.125\n",
      "\n",
      "Total Fwd Loss: 9.71871566772461\n",
      "Total Bwd Loss: 9.263574600219727\n",
      "Total Inv Consistency Loss: 892125.125\n",
      "Total Temp Consistency Loss: 0.016447100788354874\n",
      "Total Loss: 891959.1875\n",
      "\n",
      "Total Fwd Loss: 7.150291442871094\n",
      "Total Bwd Loss: 7.392768859863281\n",
      "Total Inv Consistency Loss: 891944.625\n",
      "Total Temp Consistency Loss: 0.01732493005692959\n",
      "----------Training epoch--------\n",
      "----------------135---------------\n",
      "\n",
      "Total Loss: 891773.75\n",
      "\n",
      "Total Fwd Loss: 7.421753883361816\n",
      "Total Bwd Loss: 7.614048004150391\n",
      "Total Inv Consistency Loss: 891758.6875\n",
      "Total Temp Consistency Loss: 0.019028687849640846\n",
      "Total Loss: 891591.9375\n",
      "\n",
      "Total Fwd Loss: 8.167296409606934\n",
      "Total Bwd Loss: 8.140439987182617\n",
      "Total Inv Consistency Loss: 891575.625\n",
      "Total Temp Consistency Loss: 0.017295796424150467\n",
      "Total Loss: 891409.9375\n",
      "\n",
      "Total Fwd Loss: 7.111419677734375\n",
      "Total Bwd Loss: 8.327157020568848\n",
      "Total Inv Consistency Loss: 891394.5\n",
      "Total Temp Consistency Loss: 0.01786205545067787\n",
      "Total Loss: 891223.5625\n",
      "\n",
      "Total Fwd Loss: 8.692872047424316\n",
      "Total Bwd Loss: 7.474941253662109\n",
      "Total Inv Consistency Loss: 891207.375\n",
      "Total Temp Consistency Loss: 0.015093731693923473\n",
      "Total Loss: 891042.625\n",
      "\n",
      "Total Fwd Loss: 8.231454849243164\n",
      "Total Bwd Loss: 7.876355171203613\n",
      "Total Inv Consistency Loss: 891026.5\n",
      "Total Temp Consistency Loss: 0.015757154673337936\n",
      "Total Loss: 890857.4375\n",
      "\n",
      "Total Fwd Loss: 7.458444118499756\n",
      "Total Bwd Loss: 9.31847095489502\n",
      "Total Inv Consistency Loss: 890840.6875\n",
      "Total Temp Consistency Loss: 0.01952640898525715\n",
      "----------Training epoch--------\n",
      "----------------136---------------\n",
      "\n",
      "Total Loss: 890674.625\n",
      "\n",
      "Total Fwd Loss: 7.821241855621338\n",
      "Total Bwd Loss: 8.177857398986816\n",
      "Total Inv Consistency Loss: 890658.625\n",
      "Total Temp Consistency Loss: 0.017450138926506042\n",
      "Total Loss: 890491.1875\n",
      "\n",
      "Total Fwd Loss: 8.165691375732422\n",
      "Total Bwd Loss: 8.8308744430542\n",
      "Total Inv Consistency Loss: 890474.1875\n",
      "Total Temp Consistency Loss: 0.019266458228230476\n",
      "Total Loss: 890303.125\n",
      "\n",
      "Total Fwd Loss: 6.82167911529541\n",
      "Total Bwd Loss: 8.01866340637207\n",
      "Total Inv Consistency Loss: 890288.3125\n",
      "Total Temp Consistency Loss: 0.017645899206399918\n",
      "Total Loss: 890124.9375\n",
      "\n",
      "Total Fwd Loss: 9.077648162841797\n",
      "Total Bwd Loss: 8.969557762145996\n",
      "Total Inv Consistency Loss: 890106.875\n",
      "Total Temp Consistency Loss: 0.017249729484319687\n",
      "Total Loss: 889938.25\n",
      "\n",
      "Total Fwd Loss: 7.973834991455078\n",
      "Total Bwd Loss: 7.059122562408447\n",
      "Total Inv Consistency Loss: 889923.1875\n",
      "Total Temp Consistency Loss: 0.01679271273314953\n",
      "Total Loss: 889760.4375\n",
      "\n",
      "Total Fwd Loss: 7.30981969833374\n",
      "Total Bwd Loss: 7.768820762634277\n",
      "Total Inv Consistency Loss: 889745.375\n",
      "Total Temp Consistency Loss: 0.01837804540991783\n",
      "----------Training epoch--------\n",
      "----------------137---------------\n",
      "\n",
      "Total Loss: 889569.4375\n",
      "\n",
      "Total Fwd Loss: 6.968659400939941\n",
      "Total Bwd Loss: 7.77274227142334\n",
      "Total Inv Consistency Loss: 889554.6875\n",
      "Total Temp Consistency Loss: 0.017820248380303383\n",
      "Total Loss: 889397.1875\n",
      "\n",
      "Total Fwd Loss: 9.33248519897461\n",
      "Total Bwd Loss: 9.707670211791992\n",
      "Total Inv Consistency Loss: 889378.125\n",
      "Total Temp Consistency Loss: 0.016985077410936356\n",
      "Total Loss: 889209.875\n",
      "\n",
      "Total Fwd Loss: 7.941176414489746\n",
      "Total Bwd Loss: 7.646688938140869\n",
      "Total Inv Consistency Loss: 889194.3125\n",
      "Total Temp Consistency Loss: 0.016796056181192398\n",
      "Total Loss: 889024.125\n",
      "\n",
      "Total Fwd Loss: 7.452766418457031\n",
      "Total Bwd Loss: 8.525546073913574\n",
      "Total Inv Consistency Loss: 889008.125\n",
      "Total Temp Consistency Loss: 0.0171718280762434\n",
      "Total Loss: 888846.75\n",
      "\n",
      "Total Fwd Loss: 8.225522994995117\n",
      "Total Bwd Loss: 7.874266147613525\n",
      "Total Inv Consistency Loss: 888830.625\n",
      "Total Temp Consistency Loss: 0.01887167990207672\n",
      "Total Loss: 888653.0625\n",
      "\n",
      "Total Fwd Loss: 7.181687831878662\n",
      "Total Bwd Loss: 7.38030481338501\n",
      "Total Inv Consistency Loss: 888638.5\n",
      "Total Temp Consistency Loss: 0.01594649814069271\n",
      "----------Training epoch--------\n",
      "----------------138---------------\n",
      "\n",
      "Total Loss: 888477.0\n",
      "\n",
      "Total Fwd Loss: 8.018301010131836\n",
      "Total Bwd Loss: 7.976067543029785\n",
      "Total Inv Consistency Loss: 888461.0\n",
      "Total Temp Consistency Loss: 0.01882665790617466\n",
      "Total Loss: 888295.0\n",
      "\n",
      "Total Fwd Loss: 8.060556411743164\n",
      "Total Bwd Loss: 7.5860443115234375\n",
      "Total Inv Consistency Loss: 888279.375\n",
      "Total Temp Consistency Loss: 0.017487619072198868\n",
      "Total Loss: 888110.3125\n",
      "\n",
      "Total Fwd Loss: 8.068818092346191\n",
      "Total Bwd Loss: 8.940261840820312\n",
      "Total Inv Consistency Loss: 888093.3125\n",
      "Total Temp Consistency Loss: 0.01694754883646965\n",
      "Total Loss: 887924.4375\n",
      "\n",
      "Total Fwd Loss: 7.158611297607422\n",
      "Total Bwd Loss: 7.432912349700928\n",
      "Total Inv Consistency Loss: 887909.875\n",
      "Total Temp Consistency Loss: 0.015970122069120407\n",
      "Total Loss: 887747.0625\n",
      "\n",
      "Total Fwd Loss: 8.174759864807129\n",
      "Total Bwd Loss: 9.482706069946289\n",
      "Total Inv Consistency Loss: 887729.375\n",
      "Total Temp Consistency Loss: 0.017040587961673737\n",
      "Total Loss: 887561.3125\n",
      "\n",
      "Total Fwd Loss: 7.416811466217041\n",
      "Total Bwd Loss: 7.500189304351807\n",
      "Total Inv Consistency Loss: 887546.375\n",
      "Total Temp Consistency Loss: 0.017196359112858772\n",
      "----------Training epoch--------\n",
      "----------------139---------------\n",
      "\n",
      "Total Loss: 887375.375\n",
      "\n",
      "Total Fwd Loss: 6.628159523010254\n",
      "Total Bwd Loss: 7.959881782531738\n",
      "Total Inv Consistency Loss: 887360.8125\n",
      "Total Temp Consistency Loss: 0.018960628658533096\n",
      "Total Loss: 887202.0625\n",
      "\n",
      "Total Fwd Loss: 6.863869667053223\n",
      "Total Bwd Loss: 7.366950035095215\n",
      "Total Inv Consistency Loss: 887187.8125\n",
      "Total Temp Consistency Loss: 0.018854012712836266\n",
      "Total Loss: 887016.0\n",
      "\n",
      "Total Fwd Loss: 7.544073581695557\n",
      "Total Bwd Loss: 8.451388359069824\n",
      "Total Inv Consistency Loss: 887000.0\n",
      "Total Temp Consistency Loss: 0.017104621976614\n",
      "Total Loss: 886833.4375\n",
      "\n",
      "Total Fwd Loss: 7.711723327636719\n",
      "Total Bwd Loss: 7.836089134216309\n",
      "Total Inv Consistency Loss: 886817.875\n",
      "Total Temp Consistency Loss: 0.016099322587251663\n",
      "Total Loss: 886652.6875\n",
      "\n",
      "Total Fwd Loss: 9.835229873657227\n",
      "Total Bwd Loss: 8.876365661621094\n",
      "Total Inv Consistency Loss: 886634.0\n",
      "Total Temp Consistency Loss: 0.016672838479280472\n",
      "Total Loss: 886465.625\n",
      "\n",
      "Total Fwd Loss: 8.44729232788086\n",
      "Total Bwd Loss: 8.485710144042969\n",
      "Total Inv Consistency Loss: 886448.6875\n",
      "Total Temp Consistency Loss: 0.017451142892241478\n",
      "----------Training epoch--------\n",
      "----------------140---------------\n",
      "\n",
      "Total Loss: 886289.5625\n",
      "\n",
      "Total Fwd Loss: 7.391811370849609\n",
      "Total Bwd Loss: 7.777738094329834\n",
      "Total Inv Consistency Loss: 886274.375\n",
      "Total Temp Consistency Loss: 0.018087903037667274\n",
      "Total Loss: 886108.25\n",
      "\n",
      "Total Fwd Loss: 8.042745590209961\n",
      "Total Bwd Loss: 8.083579063415527\n",
      "Total Inv Consistency Loss: 886092.125\n",
      "Total Temp Consistency Loss: 0.018453937023878098\n",
      "Total Loss: 885922.6875\n",
      "\n",
      "Total Fwd Loss: 8.100964546203613\n",
      "Total Bwd Loss: 8.73619270324707\n",
      "Total Inv Consistency Loss: 885905.875\n",
      "Total Temp Consistency Loss: 0.015943460166454315\n",
      "Total Loss: 885735.75\n",
      "\n",
      "Total Fwd Loss: 7.287962436676025\n",
      "Total Bwd Loss: 6.845301151275635\n",
      "Total Inv Consistency Loss: 885721.625\n",
      "Total Temp Consistency Loss: 0.01555151492357254\n",
      "Total Loss: 885563.9375\n",
      "\n",
      "Total Fwd Loss: 9.536927223205566\n",
      "Total Bwd Loss: 8.425167083740234\n",
      "Total Inv Consistency Loss: 885546.0\n",
      "Total Temp Consistency Loss: 0.015574177727103233\n",
      "Total Loss: 885375.375\n",
      "\n",
      "Total Fwd Loss: 6.557919979095459\n",
      "Total Bwd Loss: 8.94316291809082\n",
      "Total Inv Consistency Loss: 885359.875\n",
      "Total Temp Consistency Loss: 0.01707034930586815\n",
      "----------Training epoch--------\n",
      "----------------141---------------\n",
      "\n",
      "Total Loss: 885188.6875\n",
      "\n",
      "Total Fwd Loss: 8.418404579162598\n",
      "Total Bwd Loss: 7.932323455810547\n",
      "Total Inv Consistency Loss: 885172.3125\n",
      "Total Temp Consistency Loss: 0.01572452299296856\n",
      "Total Loss: 885008.0625\n",
      "\n",
      "Total Fwd Loss: 8.783903121948242\n",
      "Total Bwd Loss: 8.974163055419922\n",
      "Total Inv Consistency Loss: 884990.3125\n",
      "Total Temp Consistency Loss: 0.016583601012825966\n",
      "Total Loss: 884825.1875\n",
      "\n",
      "Total Fwd Loss: 7.177168369293213\n",
      "Total Bwd Loss: 8.305315017700195\n",
      "Total Inv Consistency Loss: 884809.6875\n",
      "Total Temp Consistency Loss: 0.018611237406730652\n",
      "Total Loss: 884644.6875\n",
      "\n",
      "Total Fwd Loss: 7.974538326263428\n",
      "Total Bwd Loss: 8.059700965881348\n",
      "Total Inv Consistency Loss: 884628.625\n",
      "Total Temp Consistency Loss: 0.01665373332798481\n",
      "Total Loss: 884461.1875\n",
      "\n",
      "Total Fwd Loss: 7.745656490325928\n",
      "Total Bwd Loss: 7.052336692810059\n",
      "Total Inv Consistency Loss: 884446.375\n",
      "Total Temp Consistency Loss: 0.017557542771100998\n",
      "Total Loss: 884280.5625\n",
      "\n",
      "Total Fwd Loss: 6.946041107177734\n",
      "Total Bwd Loss: 8.6231689453125\n",
      "Total Inv Consistency Loss: 884265.0\n",
      "Total Temp Consistency Loss: 0.01822775788605213\n",
      "----------Training epoch--------\n",
      "----------------142---------------\n",
      "\n",
      "Total Loss: 884097.4375\n",
      "\n",
      "Total Fwd Loss: 7.282162666320801\n",
      "Total Bwd Loss: 7.179306983947754\n",
      "Total Inv Consistency Loss: 884083.0\n",
      "Total Temp Consistency Loss: 0.015304654836654663\n",
      "Total Loss: 883920.25\n",
      "\n",
      "Total Fwd Loss: 9.486753463745117\n",
      "Total Bwd Loss: 9.294351577758789\n",
      "Total Inv Consistency Loss: 883901.5\n",
      "Total Temp Consistency Loss: 0.016650710254907608\n",
      "Total Loss: 883735.1875\n",
      "\n",
      "Total Fwd Loss: 7.652983665466309\n",
      "Total Bwd Loss: 9.014469146728516\n",
      "Total Inv Consistency Loss: 883718.5\n",
      "Total Temp Consistency Loss: 0.017685920000076294\n",
      "Total Loss: 883549.625\n",
      "\n",
      "Total Fwd Loss: 6.735688209533691\n",
      "Total Bwd Loss: 8.765422821044922\n",
      "Total Inv Consistency Loss: 883534.125\n",
      "Total Temp Consistency Loss: 0.0182743351906538\n",
      "Total Loss: 883369.625\n",
      "\n",
      "Total Fwd Loss: 8.090932846069336\n",
      "Total Bwd Loss: 7.360941410064697\n",
      "Total Inv Consistency Loss: 883354.1875\n",
      "Total Temp Consistency Loss: 0.01511231530457735\n",
      "Total Loss: 883188.875\n",
      "\n",
      "Total Fwd Loss: 7.825278282165527\n",
      "Total Bwd Loss: 7.240511894226074\n",
      "Total Inv Consistency Loss: 883173.8125\n",
      "Total Temp Consistency Loss: 0.017459537833929062\n",
      "----------Training epoch--------\n",
      "----------------143---------------\n",
      "\n",
      "Total Loss: 883010.875\n",
      "\n",
      "Total Fwd Loss: 8.184816360473633\n",
      "Total Bwd Loss: 8.188528060913086\n",
      "Total Inv Consistency Loss: 882994.5\n",
      "Total Temp Consistency Loss: 0.01850089803338051\n",
      "Total Loss: 882824.125\n",
      "\n",
      "Total Fwd Loss: 6.887001991271973\n",
      "Total Bwd Loss: 8.453662872314453\n",
      "Total Inv Consistency Loss: 882808.8125\n",
      "Total Temp Consistency Loss: 0.018907759338617325\n",
      "Total Loss: 882642.8125\n",
      "\n",
      "Total Fwd Loss: 7.34714412689209\n",
      "Total Bwd Loss: 7.4767913818359375\n",
      "Total Inv Consistency Loss: 882628.0\n",
      "Total Temp Consistency Loss: 0.015129330568015575\n",
      "Total Loss: 882466.125\n",
      "\n",
      "Total Fwd Loss: 9.476727485656738\n",
      "Total Bwd Loss: 8.355724334716797\n",
      "Total Inv Consistency Loss: 882448.3125\n",
      "Total Temp Consistency Loss: 0.015864958986639977\n",
      "Total Loss: 882281.25\n",
      "\n",
      "Total Fwd Loss: 7.383969306945801\n",
      "Total Bwd Loss: 7.514025688171387\n",
      "Total Inv Consistency Loss: 882266.375\n",
      "Total Temp Consistency Loss: 0.01587255485355854\n",
      "Total Loss: 882100.6875\n",
      "\n",
      "Total Fwd Loss: 7.790801048278809\n",
      "Total Bwd Loss: 8.738431930541992\n",
      "Total Inv Consistency Loss: 882084.1875\n",
      "Total Temp Consistency Loss: 0.016664616763591766\n",
      "----------Training epoch--------\n",
      "----------------144---------------\n",
      "\n",
      "Total Loss: 881919.1875\n",
      "\n",
      "Total Fwd Loss: 7.067599296569824\n",
      "Total Bwd Loss: 7.6126298904418945\n",
      "Total Inv Consistency Loss: 881904.5\n",
      "Total Temp Consistency Loss: 0.016814278438687325\n",
      "Total Loss: 881736.6875\n",
      "\n",
      "Total Fwd Loss: 7.171725273132324\n",
      "Total Bwd Loss: 7.825540065765381\n",
      "Total Inv Consistency Loss: 881721.6875\n",
      "Total Temp Consistency Loss: 0.01589151658117771\n",
      "Total Loss: 881556.3125\n",
      "\n",
      "Total Fwd Loss: 9.12816047668457\n",
      "Total Bwd Loss: 9.28575611114502\n",
      "Total Inv Consistency Loss: 881537.875\n",
      "Total Temp Consistency Loss: 0.017008788883686066\n",
      "Total Loss: 881367.25\n",
      "\n",
      "Total Fwd Loss: 8.33875846862793\n",
      "Total Bwd Loss: 7.091665744781494\n",
      "Total Inv Consistency Loss: 881351.8125\n",
      "Total Temp Consistency Loss: 0.015865419059991837\n",
      "Total Loss: 881194.3125\n",
      "\n",
      "Total Fwd Loss: 7.133409023284912\n",
      "Total Bwd Loss: 8.19810962677002\n",
      "Total Inv Consistency Loss: 881179.0\n",
      "Total Temp Consistency Loss: 0.01707591861486435\n",
      "Total Loss: 881016.3125\n",
      "\n",
      "Total Fwd Loss: 8.136285781860352\n",
      "Total Bwd Loss: 8.817423820495605\n",
      "Total Inv Consistency Loss: 880999.375\n",
      "Total Temp Consistency Loss: 0.01603824645280838\n",
      "----------Training epoch--------\n",
      "----------------145---------------\n",
      "\n",
      "Total Loss: 880830.5\n",
      "\n",
      "Total Fwd Loss: 7.666210174560547\n",
      "Total Bwd Loss: 9.656806945800781\n",
      "Total Inv Consistency Loss: 880813.1875\n",
      "Total Temp Consistency Loss: 0.018713129684329033\n",
      "Total Loss: 880644.375\n",
      "\n",
      "Total Fwd Loss: 7.968724727630615\n",
      "Total Bwd Loss: 8.087865829467773\n",
      "Total Inv Consistency Loss: 880628.3125\n",
      "Total Temp Consistency Loss: 0.015635879710316658\n",
      "Total Loss: 880465.375\n",
      "\n",
      "Total Fwd Loss: 7.262612819671631\n",
      "Total Bwd Loss: 7.40426778793335\n",
      "Total Inv Consistency Loss: 880450.6875\n",
      "Total Temp Consistency Loss: 0.014944540336728096\n",
      "Total Loss: 880290.375\n",
      "\n",
      "Total Fwd Loss: 9.323625564575195\n",
      "Total Bwd Loss: 9.184013366699219\n",
      "Total Inv Consistency Loss: 880271.875\n",
      "Total Temp Consistency Loss: 0.016233693808317184\n",
      "Total Loss: 880103.625\n",
      "\n",
      "Total Fwd Loss: 7.349156856536865\n",
      "Total Bwd Loss: 6.748177528381348\n",
      "Total Inv Consistency Loss: 880089.5\n",
      "Total Temp Consistency Loss: 0.015949394553899765\n",
      "Total Loss: 879928.5625\n",
      "\n",
      "Total Fwd Loss: 7.397038459777832\n",
      "Total Bwd Loss: 7.76140832901001\n",
      "Total Inv Consistency Loss: 879913.375\n",
      "Total Temp Consistency Loss: 0.015827713534235954\n",
      "----------Training epoch--------\n",
      "----------------146---------------\n",
      "\n",
      "Total Loss: 879747.75\n",
      "\n",
      "Total Fwd Loss: 9.459856986999512\n",
      "Total Bwd Loss: 8.770912170410156\n",
      "Total Inv Consistency Loss: 879729.5\n",
      "Total Temp Consistency Loss: 0.014812747947871685\n",
      "Total Loss: 879557.0\n",
      "\n",
      "Total Fwd Loss: 6.8792219161987305\n",
      "Total Bwd Loss: 8.920109748840332\n",
      "Total Inv Consistency Loss: 879541.1875\n",
      "Total Temp Consistency Loss: 0.017988402396440506\n",
      "Total Loss: 879385.6875\n",
      "\n",
      "Total Fwd Loss: 8.012750625610352\n",
      "Total Bwd Loss: 7.805148124694824\n",
      "Total Inv Consistency Loss: 879369.875\n",
      "Total Temp Consistency Loss: 0.014829683117568493\n",
      "Total Loss: 879197.75\n",
      "\n",
      "Total Fwd Loss: 6.9689788818359375\n",
      "Total Bwd Loss: 8.266393661499023\n",
      "Total Inv Consistency Loss: 879182.5\n",
      "Total Temp Consistency Loss: 0.016625452786684036\n",
      "Total Loss: 879019.5\n",
      "\n",
      "Total Fwd Loss: 7.993958950042725\n",
      "Total Bwd Loss: 7.869374752044678\n",
      "Total Inv Consistency Loss: 879003.625\n",
      "Total Temp Consistency Loss: 0.01603217050433159\n",
      "Total Loss: 878839.25\n",
      "\n",
      "Total Fwd Loss: 7.636206150054932\n",
      "Total Bwd Loss: 7.257118225097656\n",
      "Total Inv Consistency Loss: 878824.375\n",
      "Total Temp Consistency Loss: 0.015318125486373901\n",
      "----------Training epoch--------\n",
      "----------------147---------------\n",
      "\n",
      "Total Loss: 878661.5625\n",
      "\n",
      "Total Fwd Loss: 8.074220657348633\n",
      "Total Bwd Loss: 7.975595951080322\n",
      "Total Inv Consistency Loss: 878645.5\n",
      "Total Temp Consistency Loss: 0.015273353084921837\n",
      "Total Loss: 878472.5\n",
      "\n",
      "Total Fwd Loss: 7.117259979248047\n",
      "Total Bwd Loss: 8.5053129196167\n",
      "Total Inv Consistency Loss: 878456.875\n",
      "Total Temp Consistency Loss: 0.018110869452357292\n",
      "Total Loss: 878295.1875\n",
      "\n",
      "Total Fwd Loss: 7.168722629547119\n",
      "Total Bwd Loss: 8.01630973815918\n",
      "Total Inv Consistency Loss: 878280.0\n",
      "Total Temp Consistency Loss: 0.01553555577993393\n",
      "Total Loss: 878118.8125\n",
      "\n",
      "Total Fwd Loss: 9.31917953491211\n",
      "Total Bwd Loss: 8.158452033996582\n",
      "Total Inv Consistency Loss: 878101.3125\n",
      "Total Temp Consistency Loss: 0.014871709048748016\n",
      "Total Loss: 877929.875\n",
      "\n",
      "Total Fwd Loss: 7.202703952789307\n",
      "Total Bwd Loss: 7.467385768890381\n",
      "Total Inv Consistency Loss: 877915.1875\n",
      "Total Temp Consistency Loss: 0.01584017463028431\n",
      "Total Loss: 877756.75\n",
      "\n",
      "Total Fwd Loss: 8.180416107177734\n",
      "Total Bwd Loss: 8.685689926147461\n",
      "Total Inv Consistency Loss: 877739.875\n",
      "Total Temp Consistency Loss: 0.017667127773165703\n",
      "----------Training epoch--------\n",
      "----------------148---------------\n",
      "\n",
      "Total Loss: 877567.5\n",
      "\n",
      "Total Fwd Loss: 7.232133388519287\n",
      "Total Bwd Loss: 7.894460201263428\n",
      "Total Inv Consistency Loss: 877552.375\n",
      "Total Temp Consistency Loss: 0.016056250780820847\n",
      "Total Loss: 877392.8125\n",
      "\n",
      "Total Fwd Loss: 8.419842720031738\n",
      "Total Bwd Loss: 8.542231559753418\n",
      "Total Inv Consistency Loss: 877375.875\n",
      "Total Temp Consistency Loss: 0.01604042388498783\n",
      "Total Loss: 877214.3125\n",
      "\n",
      "Total Fwd Loss: 7.7514519691467285\n",
      "Total Bwd Loss: 8.767987251281738\n",
      "Total Inv Consistency Loss: 877197.8125\n",
      "Total Temp Consistency Loss: 0.01690145954489708\n",
      "Total Loss: 877025.6875\n",
      "\n",
      "Total Fwd Loss: 7.7810516357421875\n",
      "Total Bwd Loss: 6.884713172912598\n",
      "Total Inv Consistency Loss: 877011.0\n",
      "Total Temp Consistency Loss: 0.015120496042072773\n",
      "Total Loss: 876853.375\n",
      "\n",
      "Total Fwd Loss: 8.738903045654297\n",
      "Total Bwd Loss: 9.435474395751953\n",
      "Total Inv Consistency Loss: 876835.1875\n",
      "Total Temp Consistency Loss: 0.01663551665842533\n",
      "Total Loss: 876666.125\n",
      "\n",
      "Total Fwd Loss: 7.1390862464904785\n",
      "Total Bwd Loss: 7.3446245193481445\n",
      "Total Inv Consistency Loss: 876651.625\n",
      "Total Temp Consistency Loss: 0.017176173627376556\n",
      "----------Training epoch--------\n",
      "----------------149---------------\n",
      "\n",
      "Total Loss: 876487.5\n",
      "\n",
      "Total Fwd Loss: 6.386579990386963\n",
      "Total Bwd Loss: 7.638059139251709\n",
      "Total Inv Consistency Loss: 876473.5\n",
      "Total Temp Consistency Loss: 0.016809752210974693\n",
      "Total Loss: 876311.125\n",
      "\n",
      "Total Fwd Loss: 7.737671852111816\n",
      "Total Bwd Loss: 8.36567211151123\n",
      "Total Inv Consistency Loss: 876295.0\n",
      "Total Temp Consistency Loss: 0.016144922003149986\n",
      "Total Loss: 876132.0\n",
      "\n",
      "Total Fwd Loss: 8.020988464355469\n",
      "Total Bwd Loss: 6.973734378814697\n",
      "Total Inv Consistency Loss: 876117.0\n",
      "Total Temp Consistency Loss: 0.0150118637830019\n",
      "Total Loss: 875941.9375\n",
      "\n",
      "Total Fwd Loss: 9.40750503540039\n",
      "Total Bwd Loss: 7.824826717376709\n",
      "Total Inv Consistency Loss: 875924.6875\n",
      "Total Temp Consistency Loss: 0.015554356388747692\n",
      "Total Loss: 875768.625\n",
      "\n",
      "Total Fwd Loss: 8.105684280395508\n",
      "Total Bwd Loss: 8.833483695983887\n",
      "Total Inv Consistency Loss: 875751.6875\n",
      "Total Temp Consistency Loss: 0.016644028946757317\n",
      "Total Loss: 875587.4375\n",
      "\n",
      "Total Fwd Loss: 7.273266792297363\n",
      "Total Bwd Loss: 9.341073989868164\n",
      "Total Inv Consistency Loss: 875570.8125\n",
      "Total Temp Consistency Loss: 0.019007530063390732\n",
      "----------Training epoch--------\n",
      "----------------150---------------\n",
      "\n",
      "Total Loss: 875407.3125\n",
      "\n",
      "Total Fwd Loss: 6.738905906677246\n",
      "Total Bwd Loss: 8.592665672302246\n",
      "Total Inv Consistency Loss: 875392.0\n",
      "Total Temp Consistency Loss: 0.017422890290617943\n",
      "Total Loss: 875231.4375\n",
      "\n",
      "Total Fwd Loss: 7.212603569030762\n",
      "Total Bwd Loss: 6.555616855621338\n",
      "Total Inv Consistency Loss: 875217.6875\n",
      "Total Temp Consistency Loss: 0.01569759100675583\n",
      "Total Loss: 875048.625\n",
      "\n",
      "Total Fwd Loss: 7.832700252532959\n",
      "Total Bwd Loss: 7.987144470214844\n",
      "Total Inv Consistency Loss: 875032.8125\n",
      "Total Temp Consistency Loss: 0.018473850563168526\n",
      "Total Loss: 874871.6875\n",
      "\n",
      "Total Fwd Loss: 7.77385950088501\n",
      "Total Bwd Loss: 8.301559448242188\n",
      "Total Inv Consistency Loss: 874855.625\n",
      "Total Temp Consistency Loss: 0.016312070190906525\n",
      "Total Loss: 874693.75\n",
      "\n",
      "Total Fwd Loss: 9.871620178222656\n",
      "Total Bwd Loss: 8.855416297912598\n",
      "Total Inv Consistency Loss: 874675.0\n",
      "Total Temp Consistency Loss: 0.014526816084980965\n",
      "Total Loss: 874503.9375\n",
      "\n",
      "Total Fwd Loss: 7.402695655822754\n",
      "Total Bwd Loss: 8.674221992492676\n",
      "Total Inv Consistency Loss: 874487.875\n",
      "Total Temp Consistency Loss: 0.01694771647453308\n",
      "----------Training epoch--------\n",
      "----------------151---------------\n",
      "\n",
      "Total Loss: 874327.1875\n",
      "\n",
      "Total Fwd Loss: 7.079725742340088\n",
      "Total Bwd Loss: 8.085405349731445\n",
      "Total Inv Consistency Loss: 874312.0\n",
      "Total Temp Consistency Loss: 0.017713189125061035\n",
      "Total Loss: 874153.5\n",
      "\n",
      "Total Fwd Loss: 8.753171920776367\n",
      "Total Bwd Loss: 8.361961364746094\n",
      "Total Inv Consistency Loss: 874136.375\n",
      "Total Temp Consistency Loss: 0.018252581357955933\n",
      "Total Loss: 873971.0\n",
      "\n",
      "Total Fwd Loss: 7.269122123718262\n",
      "Total Bwd Loss: 7.939878940582275\n",
      "Total Inv Consistency Loss: 873955.8125\n",
      "Total Temp Consistency Loss: 0.017417743802070618\n",
      "Total Loss: 873789.125\n",
      "\n",
      "Total Fwd Loss: 8.266328811645508\n",
      "Total Bwd Loss: 8.998805046081543\n",
      "Total Inv Consistency Loss: 873771.875\n",
      "Total Temp Consistency Loss: 0.015441549941897392\n",
      "Total Loss: 873608.8125\n",
      "\n",
      "Total Fwd Loss: 8.366022109985352\n",
      "Total Bwd Loss: 8.146493911743164\n",
      "Total Inv Consistency Loss: 873592.3125\n",
      "Total Temp Consistency Loss: 0.014610039070248604\n",
      "Total Loss: 873426.1875\n",
      "\n",
      "Total Fwd Loss: 7.204263210296631\n",
      "Total Bwd Loss: 7.2966437339782715\n",
      "Total Inv Consistency Loss: 873411.6875\n",
      "Total Temp Consistency Loss: 0.01551441103219986\n",
      "----------Training epoch--------\n",
      "----------------152---------------\n",
      "\n",
      "Total Loss: 873243.0625\n",
      "\n",
      "Total Fwd Loss: 7.094893455505371\n",
      "Total Bwd Loss: 7.281198978424072\n",
      "Total Inv Consistency Loss: 873228.6875\n",
      "Total Temp Consistency Loss: 0.01483239233493805\n",
      "Total Loss: 873068.1875\n",
      "\n",
      "Total Fwd Loss: 7.317719459533691\n",
      "Total Bwd Loss: 8.033723831176758\n",
      "Total Inv Consistency Loss: 873052.8125\n",
      "Total Temp Consistency Loss: 0.015261871740221977\n",
      "Total Loss: 872888.375\n",
      "\n",
      "Total Fwd Loss: 9.707179069519043\n",
      "Total Bwd Loss: 8.536686897277832\n",
      "Total Inv Consistency Loss: 872870.125\n",
      "Total Temp Consistency Loss: 0.01658080518245697\n",
      "Total Loss: 872714.8125\n",
      "\n",
      "Total Fwd Loss: 7.000531196594238\n",
      "Total Bwd Loss: 7.510466575622559\n",
      "Total Inv Consistency Loss: 872700.3125\n",
      "Total Temp Consistency Loss: 0.014471365138888359\n",
      "Total Loss: 872532.1875\n",
      "\n",
      "Total Fwd Loss: 7.814785003662109\n",
      "Total Bwd Loss: 8.897268295288086\n",
      "Total Inv Consistency Loss: 872515.5\n",
      "Total Temp Consistency Loss: 0.01585194654762745\n",
      "Total Loss: 872353.5\n",
      "\n",
      "Total Fwd Loss: 8.130537033081055\n",
      "Total Bwd Loss: 8.490692138671875\n",
      "Total Inv Consistency Loss: 872336.875\n",
      "Total Temp Consistency Loss: 0.01604660414159298\n",
      "----------Training epoch--------\n",
      "----------------153---------------\n",
      "\n",
      "Total Loss: 872169.1875\n",
      "\n",
      "Total Fwd Loss: 7.040693759918213\n",
      "Total Bwd Loss: 8.622773170471191\n",
      "Total Inv Consistency Loss: 872153.5\n",
      "Total Temp Consistency Loss: 0.015978308394551277\n",
      "Total Loss: 871994.625\n",
      "\n",
      "Total Fwd Loss: 8.219331741333008\n",
      "Total Bwd Loss: 8.569588661193848\n",
      "Total Inv Consistency Loss: 871977.8125\n",
      "Total Temp Consistency Loss: 0.01451913733035326\n",
      "Total Loss: 871815.875\n",
      "\n",
      "Total Fwd Loss: 9.369592666625977\n",
      "Total Bwd Loss: 9.006660461425781\n",
      "Total Inv Consistency Loss: 871797.5\n",
      "Total Temp Consistency Loss: 0.015284702181816101\n",
      "Total Loss: 871628.9375\n",
      "\n",
      "Total Fwd Loss: 7.712612152099609\n",
      "Total Bwd Loss: 6.73126220703125\n",
      "Total Inv Consistency Loss: 871614.5\n",
      "Total Temp Consistency Loss: 0.014605790376663208\n",
      "Total Loss: 871453.875\n",
      "\n",
      "Total Fwd Loss: 7.4407525062561035\n",
      "Total Bwd Loss: 8.539368629455566\n",
      "Total Inv Consistency Loss: 871437.875\n",
      "Total Temp Consistency Loss: 0.01585134118795395\n",
      "Total Loss: 871273.3125\n",
      "\n",
      "Total Fwd Loss: 7.451536655426025\n",
      "Total Bwd Loss: 7.265345573425293\n",
      "Total Inv Consistency Loss: 871258.625\n",
      "Total Temp Consistency Loss: 0.014921081252396107\n",
      "----------Training epoch--------\n",
      "----------------154---------------\n",
      "\n",
      "Total Loss: 871102.0\n",
      "\n",
      "Total Fwd Loss: 8.70718765258789\n",
      "Total Bwd Loss: 9.650041580200195\n",
      "Total Inv Consistency Loss: 871083.625\n",
      "Total Temp Consistency Loss: 0.015436744317412376\n",
      "Total Loss: 870917.375\n",
      "\n",
      "Total Fwd Loss: 8.203497886657715\n",
      "Total Bwd Loss: 7.3152289390563965\n",
      "Total Inv Consistency Loss: 870901.875\n",
      "Total Temp Consistency Loss: 0.014234362170100212\n",
      "Total Loss: 870732.5625\n",
      "\n",
      "Total Fwd Loss: 7.3380913734436035\n",
      "Total Bwd Loss: 7.4304070472717285\n",
      "Total Inv Consistency Loss: 870717.8125\n",
      "Total Temp Consistency Loss: 0.016037235036492348\n",
      "Total Loss: 870552.9375\n",
      "\n",
      "Total Fwd Loss: 7.084712982177734\n",
      "Total Bwd Loss: 7.709901332855225\n",
      "Total Inv Consistency Loss: 870538.125\n",
      "Total Temp Consistency Loss: 0.014751140959560871\n",
      "Total Loss: 870377.6875\n",
      "\n",
      "Total Fwd Loss: 7.8944549560546875\n",
      "Total Bwd Loss: 7.91968297958374\n",
      "Total Inv Consistency Loss: 870361.875\n",
      "Total Temp Consistency Loss: 0.015440806746482849\n",
      "Total Loss: 870196.4375\n",
      "\n",
      "Total Fwd Loss: 7.849740028381348\n",
      "Total Bwd Loss: 8.69074535369873\n",
      "Total Inv Consistency Loss: 870179.875\n",
      "Total Temp Consistency Loss: 0.014586158096790314\n",
      "----------Training epoch--------\n",
      "----------------155---------------\n",
      "\n",
      "Total Loss: 870016.3125\n",
      "\n",
      "Total Fwd Loss: 7.010658264160156\n",
      "Total Bwd Loss: 8.160877227783203\n",
      "Total Inv Consistency Loss: 870001.125\n",
      "Total Temp Consistency Loss: 0.015422177501022816\n",
      "Total Loss: 869839.5\n",
      "\n",
      "Total Fwd Loss: 6.232557773590088\n",
      "Total Bwd Loss: 8.42953109741211\n",
      "Total Inv Consistency Loss: 869824.8125\n",
      "Total Temp Consistency Loss: 0.014403382316231728\n",
      "Total Loss: 869659.4375\n",
      "\n",
      "Total Fwd Loss: 8.260331153869629\n",
      "Total Bwd Loss: 7.559477806091309\n",
      "Total Inv Consistency Loss: 869643.625\n",
      "Total Temp Consistency Loss: 0.01347468513995409\n",
      "Total Loss: 869479.6875\n",
      "\n",
      "Total Fwd Loss: 9.250005722045898\n",
      "Total Bwd Loss: 8.331610679626465\n",
      "Total Inv Consistency Loss: 869462.125\n",
      "Total Temp Consistency Loss: 0.014881539158523083\n",
      "Total Loss: 869305.1875\n",
      "\n",
      "Total Fwd Loss: 8.737936019897461\n",
      "Total Bwd Loss: 8.460230827331543\n",
      "Total Inv Consistency Loss: 869288.0\n",
      "Total Temp Consistency Loss: 0.013100060634315014\n",
      "Total Loss: 869129.3125\n",
      "\n",
      "Total Fwd Loss: 7.520132541656494\n",
      "Total Bwd Loss: 7.963658332824707\n",
      "Total Inv Consistency Loss: 869113.8125\n",
      "Total Temp Consistency Loss: 0.015181501396000385\n",
      "----------Training epoch--------\n",
      "----------------156---------------\n",
      "\n",
      "Total Loss: 868942.75\n",
      "\n",
      "Total Fwd Loss: 7.4987945556640625\n",
      "Total Bwd Loss: 7.655848026275635\n",
      "Total Inv Consistency Loss: 868927.625\n",
      "Total Temp Consistency Loss: 0.0138326957821846\n",
      "Total Loss: 868767.875\n",
      "\n",
      "Total Fwd Loss: 7.786410331726074\n",
      "Total Bwd Loss: 8.558631896972656\n",
      "Total Inv Consistency Loss: 868751.5\n",
      "Total Temp Consistency Loss: 0.015194828622043133\n",
      "Total Loss: 868593.625\n",
      "\n",
      "Total Fwd Loss: 8.03217887878418\n",
      "Total Bwd Loss: 8.577245712280273\n",
      "Total Inv Consistency Loss: 868577.0\n",
      "Total Temp Consistency Loss: 0.01406190823763609\n",
      "Total Loss: 868407.0\n",
      "\n",
      "Total Fwd Loss: 8.48028564453125\n",
      "Total Bwd Loss: 7.731935977935791\n",
      "Total Inv Consistency Loss: 868390.8125\n",
      "Total Temp Consistency Loss: 0.014276720583438873\n",
      "Total Loss: 868226.0\n",
      "\n",
      "Total Fwd Loss: 7.2724199295043945\n",
      "Total Bwd Loss: 7.710997104644775\n",
      "Total Inv Consistency Loss: 868211.0\n",
      "Total Temp Consistency Loss: 0.013873470947146416\n",
      "Total Loss: 868052.1875\n",
      "\n",
      "Total Fwd Loss: 7.787581443786621\n",
      "Total Bwd Loss: 8.789761543273926\n",
      "Total Inv Consistency Loss: 868035.625\n",
      "Total Temp Consistency Loss: 0.015646304935216904\n",
      "----------Training epoch--------\n",
      "----------------157---------------\n",
      "\n",
      "Total Loss: 867873.3125\n",
      "\n",
      "Total Fwd Loss: 6.813696384429932\n",
      "Total Bwd Loss: 7.496054649353027\n",
      "Total Inv Consistency Loss: 867859.0\n",
      "Total Temp Consistency Loss: 0.013209173455834389\n",
      "Total Loss: 867696.6875\n",
      "\n",
      "Total Fwd Loss: 9.03398323059082\n",
      "Total Bwd Loss: 8.168594360351562\n",
      "Total Inv Consistency Loss: 867679.5\n",
      "Total Temp Consistency Loss: 0.013582704588770866\n",
      "Total Loss: 867513.5\n",
      "\n",
      "Total Fwd Loss: 6.523406982421875\n",
      "Total Bwd Loss: 8.463254928588867\n",
      "Total Inv Consistency Loss: 867498.5\n",
      "Total Temp Consistency Loss: 0.016996700316667557\n",
      "Total Loss: 867339.125\n",
      "\n",
      "Total Fwd Loss: 8.152880668640137\n",
      "Total Bwd Loss: 8.07158088684082\n",
      "Total Inv Consistency Loss: 867322.875\n",
      "Total Temp Consistency Loss: 0.01540452428162098\n",
      "Total Loss: 867161.1875\n",
      "\n",
      "Total Fwd Loss: 7.863133907318115\n",
      "Total Bwd Loss: 9.648889541625977\n",
      "Total Inv Consistency Loss: 867143.6875\n",
      "Total Temp Consistency Loss: 0.017070572823286057\n",
      "Total Loss: 866980.125\n",
      "\n",
      "Total Fwd Loss: 8.465234756469727\n",
      "Total Bwd Loss: 7.144234657287598\n",
      "Total Inv Consistency Loss: 866964.5\n",
      "Total Temp Consistency Loss: 0.01300264336168766\n",
      "----------Training epoch--------\n",
      "----------------158---------------\n",
      "\n",
      "Total Loss: 866802.3125\n",
      "\n",
      "Total Fwd Loss: 7.912021636962891\n",
      "Total Bwd Loss: 8.202219009399414\n",
      "Total Inv Consistency Loss: 866786.1875\n",
      "Total Temp Consistency Loss: 0.013895104639232159\n",
      "Total Loss: 866623.75\n",
      "\n",
      "Total Fwd Loss: 7.677809715270996\n",
      "Total Bwd Loss: 7.912314414978027\n",
      "Total Inv Consistency Loss: 866608.1875\n",
      "Total Temp Consistency Loss: 0.014696681872010231\n",
      "Total Loss: 866441.6875\n",
      "\n",
      "Total Fwd Loss: 8.187886238098145\n",
      "Total Bwd Loss: 6.869911193847656\n",
      "Total Inv Consistency Loss: 866426.625\n",
      "Total Temp Consistency Loss: 0.01566295325756073\n",
      "Total Loss: 866270.9375\n",
      "\n",
      "Total Fwd Loss: 7.976963996887207\n",
      "Total Bwd Loss: 8.660551071166992\n",
      "Total Inv Consistency Loss: 866254.3125\n",
      "Total Temp Consistency Loss: 0.01635899767279625\n",
      "Total Loss: 866085.875\n",
      "\n",
      "Total Fwd Loss: 8.712178230285645\n",
      "Total Bwd Loss: 9.320026397705078\n",
      "Total Inv Consistency Loss: 866067.8125\n",
      "Total Temp Consistency Loss: 0.016367007046937943\n",
      "Total Loss: 865905.5\n",
      "\n",
      "Total Fwd Loss: 6.69310998916626\n",
      "Total Bwd Loss: 7.807604789733887\n",
      "Total Inv Consistency Loss: 865891.0\n",
      "Total Temp Consistency Loss: 0.01445179432630539\n",
      "----------Training epoch--------\n",
      "----------------159---------------\n",
      "\n",
      "Total Loss: 865737.625\n",
      "\n",
      "Total Fwd Loss: 8.081426620483398\n",
      "Total Bwd Loss: 9.914590835571289\n",
      "Total Inv Consistency Loss: 865719.625\n",
      "Total Temp Consistency Loss: 0.01564718410372734\n",
      "Total Loss: 865552.4375\n",
      "\n",
      "Total Fwd Loss: 7.1313652992248535\n",
      "Total Bwd Loss: 7.629014015197754\n",
      "Total Inv Consistency Loss: 865537.6875\n",
      "Total Temp Consistency Loss: 0.013844509609043598\n",
      "Total Loss: 865378.125\n",
      "\n",
      "Total Fwd Loss: 7.884413719177246\n",
      "Total Bwd Loss: 8.561639785766602\n",
      "Total Inv Consistency Loss: 865361.6875\n",
      "Total Temp Consistency Loss: 0.01579277031123638\n",
      "Total Loss: 865199.0625\n",
      "\n",
      "Total Fwd Loss: 7.886404991149902\n",
      "Total Bwd Loss: 6.534952640533447\n",
      "Total Inv Consistency Loss: 865184.625\n",
      "Total Temp Consistency Loss: 0.014216028153896332\n",
      "Total Loss: 865020.3125\n",
      "\n",
      "Total Fwd Loss: 7.270829200744629\n",
      "Total Bwd Loss: 7.9199066162109375\n",
      "Total Inv Consistency Loss: 865005.125\n",
      "Total Temp Consistency Loss: 0.014781517907977104\n",
      "Total Loss: 864843.125\n",
      "\n",
      "Total Fwd Loss: 8.915143013000488\n",
      "Total Bwd Loss: 8.069398880004883\n",
      "Total Inv Consistency Loss: 864826.125\n",
      "Total Temp Consistency Loss: 0.014285728335380554\n",
      "----------Training epoch--------\n",
      "----------------160---------------\n",
      "\n",
      "Total Loss: 864669.3125\n",
      "\n",
      "Total Fwd Loss: 9.433303833007812\n",
      "Total Bwd Loss: 7.525163173675537\n",
      "Total Inv Consistency Loss: 864652.375\n",
      "Total Temp Consistency Loss: 0.013643605634570122\n",
      "Total Loss: 864483.5\n",
      "\n",
      "Total Fwd Loss: 8.029989242553711\n",
      "Total Bwd Loss: 9.114209175109863\n",
      "Total Inv Consistency Loss: 864466.375\n",
      "Total Temp Consistency Loss: 0.013833368197083473\n",
      "Total Loss: 864307.5625\n",
      "\n",
      "Total Fwd Loss: 8.399190902709961\n",
      "Total Bwd Loss: 7.829852104187012\n",
      "Total Inv Consistency Loss: 864291.3125\n",
      "Total Temp Consistency Loss: 0.013398921117186546\n",
      "Total Loss: 864135.8125\n",
      "\n",
      "Total Fwd Loss: 6.874556064605713\n",
      "Total Bwd Loss: 8.768754959106445\n",
      "Total Inv Consistency Loss: 864120.1875\n",
      "Total Temp Consistency Loss: 0.014596173539757729\n",
      "Total Loss: 863954.0\n",
      "\n",
      "Total Fwd Loss: 7.284970760345459\n",
      "Total Bwd Loss: 8.023286819458008\n",
      "Total Inv Consistency Loss: 863938.6875\n",
      "Total Temp Consistency Loss: 0.013872483745217323\n",
      "Total Loss: 863779.0625\n",
      "\n",
      "Total Fwd Loss: 6.965418338775635\n",
      "Total Bwd Loss: 7.613831520080566\n",
      "Total Inv Consistency Loss: 863764.5\n",
      "Total Temp Consistency Loss: 0.013977140188217163\n",
      "----------Training epoch--------\n",
      "----------------161---------------\n",
      "\n",
      "Total Loss: 863595.25\n",
      "\n",
      "Total Fwd Loss: 7.609614372253418\n",
      "Total Bwd Loss: 8.266393661499023\n",
      "Total Inv Consistency Loss: 863579.375\n",
      "Total Temp Consistency Loss: 0.014558164402842522\n",
      "Total Loss: 863454.625\n",
      "\n",
      "Total Fwd Loss: 8.11184310913086\n",
      "Total Bwd Loss: 8.529123306274414\n",
      "Total Inv Consistency Loss: 863438.0\n",
      "Total Temp Consistency Loss: 0.013225652277469635\n",
      "Total Loss: 863309.5625\n",
      "\n",
      "Total Fwd Loss: 7.441681861877441\n",
      "Total Bwd Loss: 8.25744342803955\n",
      "Total Inv Consistency Loss: 863293.875\n",
      "Total Temp Consistency Loss: 0.013039834797382355\n",
      "Total Loss: 863171.75\n",
      "\n",
      "Total Fwd Loss: 8.830101013183594\n",
      "Total Bwd Loss: 8.609149932861328\n",
      "Total Inv Consistency Loss: 863154.3125\n",
      "Total Temp Consistency Loss: 0.013339792378246784\n",
      "Total Loss: 863030.3125\n",
      "\n",
      "Total Fwd Loss: 6.989724159240723\n",
      "Total Bwd Loss: 7.680685520172119\n",
      "Total Inv Consistency Loss: 863015.625\n",
      "Total Temp Consistency Loss: 0.012785474769771099\n",
      "Total Loss: 862888.4375\n",
      "\n",
      "Total Fwd Loss: 7.765987396240234\n",
      "Total Bwd Loss: 7.563007354736328\n",
      "Total Inv Consistency Loss: 862873.125\n",
      "Total Temp Consistency Loss: 0.012428263202309608\n",
      "----------Training epoch--------\n",
      "----------------162---------------\n",
      "\n",
      "Total Loss: 862747.6875\n",
      "\n",
      "Total Fwd Loss: 7.3361945152282715\n",
      "Total Bwd Loss: 7.824021816253662\n",
      "Total Inv Consistency Loss: 862732.5\n",
      "Total Temp Consistency Loss: 0.01322292722761631\n",
      "Total Loss: 862602.75\n",
      "\n",
      "Total Fwd Loss: 6.537454128265381\n",
      "Total Bwd Loss: 7.608899116516113\n",
      "Total Inv Consistency Loss: 862588.625\n",
      "Total Temp Consistency Loss: 0.013091109693050385\n",
      "Total Loss: 862459.875\n",
      "\n",
      "Total Fwd Loss: 8.13926887512207\n",
      "Total Bwd Loss: 8.031277656555176\n",
      "Total Inv Consistency Loss: 862443.6875\n",
      "Total Temp Consistency Loss: 0.014748538844287395\n",
      "Total Loss: 862316.8125\n",
      "\n",
      "Total Fwd Loss: 7.819571018218994\n",
      "Total Bwd Loss: 7.8456902503967285\n",
      "Total Inv Consistency Loss: 862301.125\n",
      "Total Temp Consistency Loss: 0.014332096092402935\n",
      "Total Loss: 862182.0625\n",
      "\n",
      "Total Fwd Loss: 9.451364517211914\n",
      "Total Bwd Loss: 9.941679954528809\n",
      "Total Inv Consistency Loss: 862162.6875\n",
      "Total Temp Consistency Loss: 0.0143014220520854\n",
      "Total Loss: 862034.3125\n",
      "\n",
      "Total Fwd Loss: 7.697691440582275\n",
      "Total Bwd Loss: 7.623269081115723\n",
      "Total Inv Consistency Loss: 862019.0\n",
      "Total Temp Consistency Loss: 0.01182897761464119\n",
      "----------Training epoch--------\n",
      "----------------163---------------\n",
      "\n",
      "Total Loss: 861894.875\n",
      "\n",
      "Total Fwd Loss: 8.148622512817383\n",
      "Total Bwd Loss: 7.697542667388916\n",
      "Total Inv Consistency Loss: 861879.0\n",
      "Total Temp Consistency Loss: 0.012524893507361412\n",
      "Total Loss: 861751.9375\n",
      "\n",
      "Total Fwd Loss: 7.696761131286621\n",
      "Total Bwd Loss: 7.561105251312256\n",
      "Total Inv Consistency Loss: 861736.6875\n",
      "Total Temp Consistency Loss: 0.012416975572705269\n",
      "Total Loss: 861612.4375\n",
      "\n",
      "Total Fwd Loss: 8.03222370147705\n",
      "Total Bwd Loss: 8.501007080078125\n",
      "Total Inv Consistency Loss: 861595.875\n",
      "Total Temp Consistency Loss: 0.013531546108424664\n",
      "Total Loss: 861470.4375\n",
      "\n",
      "Total Fwd Loss: 8.661369323730469\n",
      "Total Bwd Loss: 9.63227653503418\n",
      "Total Inv Consistency Loss: 861452.125\n",
      "Total Temp Consistency Loss: 0.01315041072666645\n",
      "Total Loss: 861325.375\n",
      "\n",
      "Total Fwd Loss: 7.095884799957275\n",
      "Total Bwd Loss: 8.291363716125488\n",
      "Total Inv Consistency Loss: 861310.0\n",
      "Total Temp Consistency Loss: 0.01246717944741249\n",
      "Total Loss: 861183.5\n",
      "\n",
      "Total Fwd Loss: 7.405640602111816\n",
      "Total Bwd Loss: 7.108685493469238\n",
      "Total Inv Consistency Loss: 861169.0\n",
      "Total Temp Consistency Loss: 0.012214766815304756\n",
      "----------Training epoch--------\n",
      "----------------164---------------\n",
      "\n",
      "Total Loss: 861045.875\n",
      "\n",
      "Total Fwd Loss: 8.834303855895996\n",
      "Total Bwd Loss: 8.833528518676758\n",
      "Total Inv Consistency Loss: 861028.1875\n",
      "Total Temp Consistency Loss: 0.012504513375461102\n",
      "Total Loss: 860899.1875\n",
      "\n",
      "Total Fwd Loss: 6.62224817276001\n",
      "Total Bwd Loss: 7.716370582580566\n",
      "Total Inv Consistency Loss: 860884.875\n",
      "Total Temp Consistency Loss: 0.012790493667125702\n",
      "Total Loss: 860756.9375\n",
      "\n",
      "Total Fwd Loss: 8.160746574401855\n",
      "Total Bwd Loss: 8.949064254760742\n",
      "Total Inv Consistency Loss: 860739.8125\n",
      "Total Temp Consistency Loss: 0.012477625161409378\n",
      "Total Loss: 860612.9375\n",
      "\n",
      "Total Fwd Loss: 8.00080680847168\n",
      "Total Bwd Loss: 7.778044700622559\n",
      "Total Inv Consistency Loss: 860597.1875\n",
      "Total Temp Consistency Loss: 0.012275329791009426\n",
      "Total Loss: 860477.6875\n",
      "\n",
      "Total Fwd Loss: 7.696620941162109\n",
      "Total Bwd Loss: 7.857431888580322\n",
      "Total Inv Consistency Loss: 860462.125\n",
      "Total Temp Consistency Loss: 0.012763100676238537\n",
      "Total Loss: 860333.9375\n",
      "\n",
      "Total Fwd Loss: 7.608631134033203\n",
      "Total Bwd Loss: 7.66063928604126\n",
      "Total Inv Consistency Loss: 860318.6875\n",
      "Total Temp Consistency Loss: 0.012715613469481468\n",
      "----------Training epoch--------\n",
      "----------------165---------------\n",
      "\n",
      "Total Loss: 860194.25\n",
      "\n",
      "Total Fwd Loss: 8.022958755493164\n",
      "Total Bwd Loss: 8.936273574829102\n",
      "Total Inv Consistency Loss: 860177.3125\n",
      "Total Temp Consistency Loss: 0.012865823693573475\n",
      "Total Loss: 860050.75\n",
      "\n",
      "Total Fwd Loss: 7.691472053527832\n",
      "Total Bwd Loss: 7.65613317489624\n",
      "Total Inv Consistency Loss: 860035.375\n",
      "Total Temp Consistency Loss: 0.01211681216955185\n",
      "Total Loss: 859906.875\n",
      "\n",
      "Total Fwd Loss: 8.782509803771973\n",
      "Total Bwd Loss: 8.376226425170898\n",
      "Total Inv Consistency Loss: 859889.6875\n",
      "Total Temp Consistency Loss: 0.01163147296756506\n",
      "Total Loss: 859768.5625\n",
      "\n",
      "Total Fwd Loss: 7.43481969833374\n",
      "Total Bwd Loss: 7.807557106018066\n",
      "Total Inv Consistency Loss: 859753.3125\n",
      "Total Temp Consistency Loss: 0.012827995233237743\n",
      "Total Loss: 859619.8125\n",
      "\n",
      "Total Fwd Loss: 7.7696332931518555\n",
      "Total Bwd Loss: 7.9099860191345215\n",
      "Total Inv Consistency Loss: 859604.125\n",
      "Total Temp Consistency Loss: 0.013218378648161888\n",
      "Total Loss: 859483.8125\n",
      "\n",
      "Total Fwd Loss: 7.193024635314941\n",
      "Total Bwd Loss: 8.098405838012695\n",
      "Total Inv Consistency Loss: 859468.5\n",
      "Total Temp Consistency Loss: 0.012246412225067616\n",
      "----------Training epoch--------\n",
      "----------------166---------------\n",
      "\n",
      "Total Loss: 859344.625\n",
      "\n",
      "Total Fwd Loss: 8.18237018585205\n",
      "Total Bwd Loss: 7.954624176025391\n",
      "Total Inv Consistency Loss: 859328.5\n",
      "Total Temp Consistency Loss: 0.01167795155197382\n",
      "Total Loss: 859202.875\n",
      "\n",
      "Total Fwd Loss: 7.473349571228027\n",
      "Total Bwd Loss: 9.284412384033203\n",
      "Total Inv Consistency Loss: 859186.125\n",
      "Total Temp Consistency Loss: 0.014398391358554363\n",
      "Total Loss: 859062.5\n",
      "\n",
      "Total Fwd Loss: 7.910186767578125\n",
      "Total Bwd Loss: 7.592205047607422\n",
      "Total Inv Consistency Loss: 859047.0\n",
      "Total Temp Consistency Loss: 0.0117014329880476\n",
      "Total Loss: 858920.75\n",
      "\n",
      "Total Fwd Loss: 7.557435512542725\n",
      "Total Bwd Loss: 7.825117588043213\n",
      "Total Inv Consistency Loss: 858905.375\n",
      "Total Temp Consistency Loss: 0.012204518541693687\n",
      "Total Loss: 858775.625\n",
      "\n",
      "Total Fwd Loss: 8.36529541015625\n",
      "Total Bwd Loss: 8.655984878540039\n",
      "Total Inv Consistency Loss: 858758.625\n",
      "Total Temp Consistency Loss: 0.011466125957667828\n",
      "Total Loss: 858631.5\n",
      "\n",
      "Total Fwd Loss: 7.4188361167907715\n",
      "Total Bwd Loss: 7.431704521179199\n",
      "Total Inv Consistency Loss: 858616.625\n",
      "Total Temp Consistency Loss: 0.011559401638805866\n",
      "----------Training epoch--------\n",
      "----------------167---------------\n",
      "\n",
      "Total Loss: 858493.875\n",
      "\n",
      "Total Fwd Loss: 8.862981796264648\n",
      "Total Bwd Loss: 8.516641616821289\n",
      "Total Inv Consistency Loss: 858476.5\n",
      "Total Temp Consistency Loss: 0.011003573425114155\n",
      "Total Loss: 858354.875\n",
      "\n",
      "Total Fwd Loss: 7.932473659515381\n",
      "Total Bwd Loss: 7.3131513595581055\n",
      "Total Inv Consistency Loss: 858339.625\n",
      "Total Temp Consistency Loss: 0.013099970296025276\n",
      "Total Loss: 858208.1875\n",
      "\n",
      "Total Fwd Loss: 7.338593482971191\n",
      "Total Bwd Loss: 7.966536045074463\n",
      "Total Inv Consistency Loss: 858192.875\n",
      "Total Temp Consistency Loss: 0.011905635707080364\n",
      "Total Loss: 858068.5625\n",
      "\n",
      "Total Fwd Loss: 7.429819583892822\n",
      "Total Bwd Loss: 8.827055931091309\n",
      "Total Inv Consistency Loss: 858052.3125\n",
      "Total Temp Consistency Loss: 0.013056999072432518\n",
      "Total Loss: 857929.125\n",
      "\n",
      "Total Fwd Loss: 6.697124481201172\n",
      "Total Bwd Loss: 7.4183349609375\n",
      "Total Inv Consistency Loss: 857915.0\n",
      "Total Temp Consistency Loss: 0.011662544682621956\n",
      "Total Loss: 857788.4375\n",
      "\n",
      "Total Fwd Loss: 8.71786880493164\n",
      "Total Bwd Loss: 8.706686019897461\n",
      "Total Inv Consistency Loss: 857771.0\n",
      "Total Temp Consistency Loss: 0.012119030579924583\n",
      "----------Training epoch--------\n",
      "----------------168---------------\n",
      "\n",
      "Total Loss: 857651.6875\n",
      "\n",
      "Total Fwd Loss: 8.065207481384277\n",
      "Total Bwd Loss: 9.737640380859375\n",
      "Total Inv Consistency Loss: 857633.875\n",
      "Total Temp Consistency Loss: 0.012913445942103863\n",
      "Total Loss: 857505.375\n",
      "\n",
      "Total Fwd Loss: 7.791207313537598\n",
      "Total Bwd Loss: 7.199189186096191\n",
      "Total Inv Consistency Loss: 857490.375\n",
      "Total Temp Consistency Loss: 0.011282214894890785\n",
      "Total Loss: 857367.9375\n",
      "\n",
      "Total Fwd Loss: 7.037220001220703\n",
      "Total Bwd Loss: 7.394648551940918\n",
      "Total Inv Consistency Loss: 857353.5\n",
      "Total Temp Consistency Loss: 0.011777235195040703\n",
      "Total Loss: 857220.125\n",
      "\n",
      "Total Fwd Loss: 7.268824100494385\n",
      "Total Bwd Loss: 8.357489585876465\n",
      "Total Inv Consistency Loss: 857204.5\n",
      "Total Temp Consistency Loss: 0.01394746731966734\n",
      "Total Loss: 857086.375\n",
      "\n",
      "Total Fwd Loss: 9.98194694519043\n",
      "Total Bwd Loss: 8.67957878112793\n",
      "Total Inv Consistency Loss: 857067.6875\n",
      "Total Temp Consistency Loss: 0.011624785140156746\n",
      "Total Loss: 856935.6875\n",
      "\n",
      "Total Fwd Loss: 6.910181999206543\n",
      "Total Bwd Loss: 7.433833122253418\n",
      "Total Inv Consistency Loss: 856921.3125\n",
      "Total Temp Consistency Loss: 0.011669017374515533\n",
      "----------Training epoch--------\n",
      "----------------169---------------\n",
      "\n",
      "Total Loss: 856799.0625\n",
      "\n",
      "Total Fwd Loss: 8.08626651763916\n",
      "Total Bwd Loss: 8.346409797668457\n",
      "Total Inv Consistency Loss: 856782.625\n",
      "Total Temp Consistency Loss: 0.013026866130530834\n",
      "Total Loss: 856661.5\n",
      "\n",
      "Total Fwd Loss: 8.529524803161621\n",
      "Total Bwd Loss: 9.147432327270508\n",
      "Total Inv Consistency Loss: 856643.8125\n",
      "Total Temp Consistency Loss: 0.011850548908114433\n",
      "Total Loss: 856516.875\n",
      "\n",
      "Total Fwd Loss: 7.96700382232666\n",
      "Total Bwd Loss: 7.508233070373535\n",
      "Total Inv Consistency Loss: 856501.375\n",
      "Total Temp Consistency Loss: 0.010940076783299446\n",
      "Total Loss: 856378.25\n",
      "\n",
      "Total Fwd Loss: 7.139082431793213\n",
      "Total Bwd Loss: 7.714634895324707\n",
      "Total Inv Consistency Loss: 856363.375\n",
      "Total Temp Consistency Loss: 0.011710007674992085\n",
      "Total Loss: 856240.6875\n",
      "\n",
      "Total Fwd Loss: 8.313957214355469\n",
      "Total Bwd Loss: 8.673055648803711\n",
      "Total Inv Consistency Loss: 856223.6875\n",
      "Total Temp Consistency Loss: 0.011311950162053108\n",
      "Total Loss: 856089.8125\n",
      "\n",
      "Total Fwd Loss: 6.908204555511475\n",
      "Total Bwd Loss: 7.418328285217285\n",
      "Total Inv Consistency Loss: 856075.5\n",
      "Total Temp Consistency Loss: 0.011400667019188404\n",
      "----------Training epoch--------\n",
      "----------------170---------------\n",
      "\n",
      "Total Loss: 855952.4375\n",
      "\n",
      "Total Fwd Loss: 8.61943531036377\n",
      "Total Bwd Loss: 8.67147159576416\n",
      "Total Inv Consistency Loss: 855935.125\n",
      "Total Temp Consistency Loss: 0.010634200647473335\n",
      "Total Loss: 855813.1875\n",
      "\n",
      "Total Fwd Loss: 7.741166114807129\n",
      "Total Bwd Loss: 7.1049041748046875\n",
      "Total Inv Consistency Loss: 855798.3125\n",
      "Total Temp Consistency Loss: 0.011156037449836731\n",
      "Total Loss: 855676.4375\n",
      "\n",
      "Total Fwd Loss: 8.371644973754883\n",
      "Total Bwd Loss: 9.186466217041016\n",
      "Total Inv Consistency Loss: 855658.875\n",
      "Total Temp Consistency Loss: 0.012362347915768623\n",
      "Total Loss: 855536.3125\n",
      "\n",
      "Total Fwd Loss: 7.013234615325928\n",
      "Total Bwd Loss: 7.924923896789551\n",
      "Total Inv Consistency Loss: 855521.375\n",
      "Total Temp Consistency Loss: 0.010950868017971516\n",
      "Total Loss: 855381.0\n",
      "\n",
      "Total Fwd Loss: 7.90057897567749\n",
      "Total Bwd Loss: 7.751242160797119\n",
      "Total Inv Consistency Loss: 855365.375\n",
      "Total Temp Consistency Loss: 0.01158979069441557\n",
      "Total Loss: 855252.875\n",
      "\n",
      "Total Fwd Loss: 7.156659126281738\n",
      "Total Bwd Loss: 8.312280654907227\n",
      "Total Inv Consistency Loss: 855237.375\n",
      "Total Temp Consistency Loss: 0.013557101599872112\n",
      "----------Training epoch--------\n",
      "----------------171---------------\n",
      "\n",
      "Total Loss: 855111.875\n",
      "\n",
      "Total Fwd Loss: 8.066825866699219\n",
      "Total Bwd Loss: 8.978400230407715\n",
      "Total Inv Consistency Loss: 855094.8125\n",
      "Total Temp Consistency Loss: 0.012990253046154976\n",
      "Total Loss: 854969.0625\n",
      "\n",
      "Total Fwd Loss: 6.916220188140869\n",
      "Total Bwd Loss: 6.818758964538574\n",
      "Total Inv Consistency Loss: 854955.3125\n",
      "Total Temp Consistency Loss: 0.010995539836585522\n",
      "Total Loss: 854828.125\n",
      "\n",
      "Total Fwd Loss: 8.098722457885742\n",
      "Total Bwd Loss: 7.895333290100098\n",
      "Total Inv Consistency Loss: 854812.125\n",
      "Total Temp Consistency Loss: 0.011312408372759819\n",
      "Total Loss: 854684.1875\n",
      "\n",
      "Total Fwd Loss: 7.974907875061035\n",
      "Total Bwd Loss: 7.402884483337402\n",
      "Total Inv Consistency Loss: 854668.8125\n",
      "Total Temp Consistency Loss: 0.010575646534562111\n",
      "Total Loss: 854549.375\n",
      "\n",
      "Total Fwd Loss: 9.251476287841797\n",
      "Total Bwd Loss: 8.637248992919922\n",
      "Total Inv Consistency Loss: 854531.5\n",
      "Total Temp Consistency Loss: 0.010402721352875233\n",
      "Total Loss: 854405.625\n",
      "\n",
      "Total Fwd Loss: 6.6128950119018555\n",
      "Total Bwd Loss: 9.136418342590332\n",
      "Total Inv Consistency Loss: 854389.875\n",
      "Total Temp Consistency Loss: 0.01280963234603405\n",
      "----------Training epoch--------\n",
      "----------------172---------------\n",
      "\n",
      "Total Loss: 854262.3125\n",
      "\n",
      "Total Fwd Loss: 7.931244850158691\n",
      "Total Bwd Loss: 8.503249168395996\n",
      "Total Inv Consistency Loss: 854245.875\n",
      "Total Temp Consistency Loss: 0.011089358478784561\n",
      "Total Loss: 854126.0\n",
      "\n",
      "Total Fwd Loss: 8.306139945983887\n",
      "Total Bwd Loss: 8.004650115966797\n",
      "Total Inv Consistency Loss: 854109.6875\n",
      "Total Temp Consistency Loss: 0.011474955826997757\n",
      "Total Loss: 853989.75\n",
      "\n",
      "Total Fwd Loss: 8.025964736938477\n",
      "Total Bwd Loss: 8.360746383666992\n",
      "Total Inv Consistency Loss: 853973.375\n",
      "Total Temp Consistency Loss: 0.011769440025091171\n",
      "Total Loss: 853841.0\n",
      "\n",
      "Total Fwd Loss: 6.960906028747559\n",
      "Total Bwd Loss: 8.41080379486084\n",
      "Total Inv Consistency Loss: 853825.625\n",
      "Total Temp Consistency Loss: 0.011328798718750477\n",
      "Total Loss: 853702.5625\n",
      "\n",
      "Total Fwd Loss: 7.36785888671875\n",
      "Total Bwd Loss: 8.167207717895508\n",
      "Total Inv Consistency Loss: 853687.0\n",
      "Total Temp Consistency Loss: 0.012460379861295223\n",
      "Total Loss: 853566.8125\n",
      "\n",
      "Total Fwd Loss: 8.425976753234863\n",
      "Total Bwd Loss: 7.219902992248535\n",
      "Total Inv Consistency Loss: 853551.1875\n",
      "Total Temp Consistency Loss: 0.01035348791629076\n",
      "----------Training epoch--------\n",
      "----------------173---------------\n",
      "\n",
      "Total Loss: 853423.125\n",
      "\n",
      "Total Fwd Loss: 7.474835395812988\n",
      "Total Bwd Loss: 8.7503023147583\n",
      "Total Inv Consistency Loss: 853406.875\n",
      "Total Temp Consistency Loss: 0.013342221267521381\n",
      "Total Loss: 853280.9375\n",
      "\n",
      "Total Fwd Loss: 9.099581718444824\n",
      "Total Bwd Loss: 8.315170288085938\n",
      "Total Inv Consistency Loss: 853263.5\n",
      "Total Temp Consistency Loss: 0.010147658176720142\n",
      "Total Loss: 853141.9375\n",
      "\n",
      "Total Fwd Loss: 7.625100135803223\n",
      "Total Bwd Loss: 7.486701965332031\n",
      "Total Inv Consistency Loss: 853126.8125\n",
      "Total Temp Consistency Loss: 0.01035097986459732\n",
      "Total Loss: 853001.8125\n",
      "\n",
      "Total Fwd Loss: 8.717890739440918\n",
      "Total Bwd Loss: 7.961402893066406\n",
      "Total Inv Consistency Loss: 852985.125\n",
      "Total Temp Consistency Loss: 0.010617488995194435\n",
      "Total Loss: 852861.4375\n",
      "\n",
      "Total Fwd Loss: 6.899517059326172\n",
      "Total Bwd Loss: 7.7549028396606445\n",
      "Total Inv Consistency Loss: 852846.8125\n",
      "Total Temp Consistency Loss: 0.012091446667909622\n",
      "Total Loss: 852721.375\n",
      "\n",
      "Total Fwd Loss: 7.243672847747803\n",
      "Total Bwd Loss: 8.531914710998535\n",
      "Total Inv Consistency Loss: 852705.625\n",
      "Total Temp Consistency Loss: 0.012296594679355621\n",
      "----------Training epoch--------\n",
      "----------------174---------------\n",
      "\n",
      "Total Loss: 852579.5625\n",
      "\n",
      "Total Fwd Loss: 8.214170455932617\n",
      "Total Bwd Loss: 8.159239768981934\n",
      "Total Inv Consistency Loss: 852563.1875\n",
      "Total Temp Consistency Loss: 0.011999891139566898\n",
      "Total Loss: 852444.0625\n",
      "\n",
      "Total Fwd Loss: 6.785529136657715\n",
      "Total Bwd Loss: 7.588780403137207\n",
      "Total Inv Consistency Loss: 852429.6875\n",
      "Total Temp Consistency Loss: 0.010503409430384636\n",
      "Total Loss: 852297.6875\n",
      "\n",
      "Total Fwd Loss: 8.601818084716797\n",
      "Total Bwd Loss: 8.471776962280273\n",
      "Total Inv Consistency Loss: 852280.625\n",
      "Total Temp Consistency Loss: 0.011777309700846672\n",
      "Total Loss: 852166.1875\n",
      "\n",
      "Total Fwd Loss: 8.809581756591797\n",
      "Total Bwd Loss: 9.856719017028809\n",
      "Total Inv Consistency Loss: 852147.5\n",
      "Total Temp Consistency Loss: 0.011762944050133228\n",
      "Total Loss: 852015.375\n",
      "\n",
      "Total Fwd Loss: 6.898106575012207\n",
      "Total Bwd Loss: 7.668492794036865\n",
      "Total Inv Consistency Loss: 852000.8125\n",
      "Total Temp Consistency Loss: 0.0111005874350667\n",
      "Total Loss: 851872.875\n",
      "\n",
      "Total Fwd Loss: 7.66049337387085\n",
      "Total Bwd Loss: 7.043369293212891\n",
      "Total Inv Consistency Loss: 851858.1875\n",
      "Total Temp Consistency Loss: 0.010405776090919971\n",
      "----------Training epoch--------\n",
      "----------------175---------------\n",
      "\n",
      "Total Loss: 851744.375\n",
      "\n",
      "Total Fwd Loss: 9.151676177978516\n",
      "Total Bwd Loss: 9.587244033813477\n",
      "Total Inv Consistency Loss: 851725.625\n",
      "Total Temp Consistency Loss: 0.011397073976695538\n",
      "Total Loss: 851597.5\n",
      "\n",
      "Total Fwd Loss: 8.475553512573242\n",
      "Total Bwd Loss: 7.193482875823975\n",
      "Total Inv Consistency Loss: 851581.8125\n",
      "Total Temp Consistency Loss: 0.010189218446612358\n",
      "Total Loss: 851457.875\n",
      "\n",
      "Total Fwd Loss: 6.688295841217041\n",
      "Total Bwd Loss: 8.590343475341797\n",
      "Total Inv Consistency Loss: 851442.625\n",
      "Total Temp Consistency Loss: 0.012048996984958649\n",
      "Total Loss: 851317.8125\n",
      "\n",
      "Total Fwd Loss: 7.140037536621094\n",
      "Total Bwd Loss: 7.880803108215332\n",
      "Total Inv Consistency Loss: 851302.8125\n",
      "Total Temp Consistency Loss: 0.010745413601398468\n",
      "Total Loss: 851176.6875\n",
      "\n",
      "Total Fwd Loss: 8.04983139038086\n",
      "Total Bwd Loss: 7.971722602844238\n",
      "Total Inv Consistency Loss: 851160.6875\n",
      "Total Temp Consistency Loss: 0.01048132311552763\n",
      "Total Loss: 851033.8125\n",
      "\n",
      "Total Fwd Loss: 7.441478729248047\n",
      "Total Bwd Loss: 7.575364589691162\n",
      "Total Inv Consistency Loss: 851018.8125\n",
      "Total Temp Consistency Loss: 0.011035085655748844\n",
      "----------Training epoch--------\n",
      "----------------176---------------\n",
      "\n",
      "Total Loss: 850894.0625\n",
      "\n",
      "Total Fwd Loss: 7.776681423187256\n",
      "Total Bwd Loss: 6.936891078948975\n",
      "Total Inv Consistency Loss: 850879.375\n",
      "Total Temp Consistency Loss: 0.010660295374691486\n",
      "Total Loss: 850752.1875\n",
      "\n",
      "Total Fwd Loss: 7.9087324142456055\n",
      "Total Bwd Loss: 8.649614334106445\n",
      "Total Inv Consistency Loss: 850735.625\n",
      "Total Temp Consistency Loss: 0.012011210434138775\n",
      "Total Loss: 850616.5625\n",
      "\n",
      "Total Fwd Loss: 7.215185642242432\n",
      "Total Bwd Loss: 8.064135551452637\n",
      "Total Inv Consistency Loss: 850601.3125\n",
      "Total Temp Consistency Loss: 0.010520918294787407\n",
      "Total Loss: 850470.25\n",
      "\n",
      "Total Fwd Loss: 6.670664310455322\n",
      "Total Bwd Loss: 7.67450475692749\n",
      "Total Inv Consistency Loss: 850455.875\n",
      "Total Temp Consistency Loss: 0.011165705509483814\n",
      "Total Loss: 850344.0\n",
      "\n",
      "Total Fwd Loss: 9.848482131958008\n",
      "Total Bwd Loss: 9.286178588867188\n",
      "Total Inv Consistency Loss: 850324.875\n",
      "Total Temp Consistency Loss: 0.0112090352922678\n",
      "Total Loss: 850200.625\n",
      "\n",
      "Total Fwd Loss: 7.511086463928223\n",
      "Total Bwd Loss: 8.29718017578125\n",
      "Total Inv Consistency Loss: 850184.8125\n",
      "Total Temp Consistency Loss: 0.01149200089275837\n",
      "----------Training epoch--------\n",
      "----------------177---------------\n",
      "\n",
      "Total Loss: 850057.125\n",
      "\n",
      "Total Fwd Loss: 7.314222812652588\n",
      "Total Bwd Loss: 7.796965599060059\n",
      "Total Inv Consistency Loss: 850042.0\n",
      "Total Temp Consistency Loss: 0.012020360678434372\n",
      "Total Loss: 849916.0625\n",
      "\n",
      "Total Fwd Loss: 7.65753698348999\n",
      "Total Bwd Loss: 8.906717300415039\n",
      "Total Inv Consistency Loss: 849899.5\n",
      "Total Temp Consistency Loss: 0.011877532117068768\n",
      "Total Loss: 849777.5\n",
      "\n",
      "Total Fwd Loss: 6.9275007247924805\n",
      "Total Bwd Loss: 7.1917243003845215\n",
      "Total Inv Consistency Loss: 849763.375\n",
      "Total Temp Consistency Loss: 0.00983771588653326\n",
      "Total Loss: 849641.0625\n",
      "\n",
      "Total Fwd Loss: 7.269980430603027\n",
      "Total Bwd Loss: 8.430644035339355\n",
      "Total Inv Consistency Loss: 849625.375\n",
      "Total Temp Consistency Loss: 0.011130074970424175\n",
      "Total Loss: 849503.6875\n",
      "\n",
      "Total Fwd Loss: 9.071244239807129\n",
      "Total Bwd Loss: 9.127911567687988\n",
      "Total Inv Consistency Loss: 849485.5\n",
      "Total Temp Consistency Loss: 0.010961456224322319\n",
      "Total Loss: 849356.125\n",
      "\n",
      "Total Fwd Loss: 8.715725898742676\n",
      "Total Bwd Loss: 7.292138576507568\n",
      "Total Inv Consistency Loss: 849340.125\n",
      "Total Temp Consistency Loss: 0.009905029088258743\n",
      "----------Training epoch--------\n",
      "----------------178---------------\n",
      "\n",
      "Total Loss: 849215.1875\n",
      "\n",
      "Total Fwd Loss: 8.152443885803223\n",
      "Total Bwd Loss: 8.332501411437988\n",
      "Total Inv Consistency Loss: 849198.6875\n",
      "Total Temp Consistency Loss: 0.011622125282883644\n",
      "Total Loss: 849080.9375\n",
      "\n",
      "Total Fwd Loss: 7.865057468414307\n",
      "Total Bwd Loss: 8.17158317565918\n",
      "Total Inv Consistency Loss: 849064.875\n",
      "Total Temp Consistency Loss: 0.010942433960735798\n",
      "Total Loss: 848937.4375\n",
      "\n",
      "Total Fwd Loss: 7.022101402282715\n",
      "Total Bwd Loss: 7.554537773132324\n",
      "Total Inv Consistency Loss: 848922.875\n",
      "Total Temp Consistency Loss: 0.010331934317946434\n",
      "Total Loss: 848799.1875\n",
      "\n",
      "Total Fwd Loss: 9.422334671020508\n",
      "Total Bwd Loss: 8.79471492767334\n",
      "Total Inv Consistency Loss: 848781.0\n",
      "Total Temp Consistency Loss: 0.010432935319840908\n",
      "Total Loss: 848661.6875\n",
      "\n",
      "Total Fwd Loss: 6.830437660217285\n",
      "Total Bwd Loss: 8.671568870544434\n",
      "Total Inv Consistency Loss: 848646.1875\n",
      "Total Temp Consistency Loss: 0.011376026086509228\n",
      "Total Loss: 848520.0625\n",
      "\n",
      "Total Fwd Loss: 7.742737770080566\n",
      "Total Bwd Loss: 7.161436557769775\n",
      "Total Inv Consistency Loss: 848505.1875\n",
      "Total Temp Consistency Loss: 0.009725292213261127\n",
      "----------Training epoch--------\n",
      "----------------179---------------\n",
      "\n",
      "Total Loss: 848380.0625\n",
      "\n",
      "Total Fwd Loss: 6.398869514465332\n",
      "Total Bwd Loss: 8.283960342407227\n",
      "Total Inv Consistency Loss: 848365.375\n",
      "Total Temp Consistency Loss: 0.011962578631937504\n",
      "Total Loss: 848238.9375\n",
      "\n",
      "Total Fwd Loss: 8.001379013061523\n",
      "Total Bwd Loss: 8.050691604614258\n",
      "Total Inv Consistency Loss: 848222.875\n",
      "Total Temp Consistency Loss: 0.01046872790902853\n",
      "Total Loss: 848103.125\n",
      "\n",
      "Total Fwd Loss: 9.154216766357422\n",
      "Total Bwd Loss: 8.620516777038574\n",
      "Total Inv Consistency Loss: 848085.375\n",
      "Total Temp Consistency Loss: 0.010986601002514362\n",
      "Total Loss: 847962.25\n",
      "\n",
      "Total Fwd Loss: 7.95354700088501\n",
      "Total Bwd Loss: 8.286516189575195\n",
      "Total Inv Consistency Loss: 847946.0\n",
      "Total Temp Consistency Loss: 0.011191129684448242\n",
      "Total Loss: 847820.6875\n",
      "\n",
      "Total Fwd Loss: 8.292062759399414\n",
      "Total Bwd Loss: 7.405478477478027\n",
      "Total Inv Consistency Loss: 847805.0\n",
      "Total Temp Consistency Loss: 0.009858975186944008\n",
      "Total Loss: 847679.4375\n",
      "\n",
      "Total Fwd Loss: 7.202261924743652\n",
      "Total Bwd Loss: 8.122838020324707\n",
      "Total Inv Consistency Loss: 847664.125\n",
      "Total Temp Consistency Loss: 0.01082543283700943\n",
      "----------Training epoch--------\n",
      "----------------180---------------\n",
      "\n",
      "Total Loss: 847546.0\n",
      "\n",
      "Total Fwd Loss: 8.30067253112793\n",
      "Total Bwd Loss: 7.387495994567871\n",
      "Total Inv Consistency Loss: 847530.3125\n",
      "Total Temp Consistency Loss: 0.010559363290667534\n",
      "Total Loss: 847402.1875\n",
      "\n",
      "Total Fwd Loss: 6.55944299697876\n",
      "Total Bwd Loss: 8.320192337036133\n",
      "Total Inv Consistency Loss: 847387.3125\n",
      "Total Temp Consistency Loss: 0.010748328641057014\n",
      "Total Loss: 847262.875\n",
      "\n",
      "Total Fwd Loss: 7.310046195983887\n",
      "Total Bwd Loss: 8.533866882324219\n",
      "Total Inv Consistency Loss: 847247.0\n",
      "Total Temp Consistency Loss: 0.011157610453665257\n",
      "Total Loss: 847118.375\n",
      "\n",
      "Total Fwd Loss: 9.217379570007324\n",
      "Total Bwd Loss: 8.670815467834473\n",
      "Total Inv Consistency Loss: 847100.5\n",
      "Total Temp Consistency Loss: 0.01046191155910492\n",
      "Total Loss: 846988.25\n",
      "\n",
      "Total Fwd Loss: 8.101786613464355\n",
      "Total Bwd Loss: 8.3276948928833\n",
      "Total Inv Consistency Loss: 846971.8125\n",
      "Total Temp Consistency Loss: 0.010666823014616966\n",
      "Total Loss: 846840.1875\n",
      "\n",
      "Total Fwd Loss: 7.368380546569824\n",
      "Total Bwd Loss: 7.624234199523926\n",
      "Total Inv Consistency Loss: 846825.1875\n",
      "Total Temp Consistency Loss: 0.010981641709804535\n",
      "----------Training epoch--------\n",
      "----------------181---------------\n",
      "\n",
      "Total Loss: 846710.0625\n",
      "\n",
      "Total Fwd Loss: 8.529352188110352\n",
      "Total Bwd Loss: 8.37153434753418\n",
      "Total Inv Consistency Loss: 846693.1875\n",
      "Total Temp Consistency Loss: 0.01100595761090517\n",
      "Total Loss: 846563.125\n",
      "\n",
      "Total Fwd Loss: 6.291092872619629\n",
      "Total Bwd Loss: 7.9355316162109375\n",
      "Total Inv Consistency Loss: 846548.875\n",
      "Total Temp Consistency Loss: 0.01073586568236351\n",
      "Total Loss: 846428.625\n",
      "\n",
      "Total Fwd Loss: 7.84652853012085\n",
      "Total Bwd Loss: 7.912856101989746\n",
      "Total Inv Consistency Loss: 846412.875\n",
      "Total Temp Consistency Loss: 0.009947624988853931\n",
      "Total Loss: 846281.3125\n",
      "\n",
      "Total Fwd Loss: 7.499354362487793\n",
      "Total Bwd Loss: 8.529701232910156\n",
      "Total Inv Consistency Loss: 846265.3125\n",
      "Total Temp Consistency Loss: 0.011220136657357216\n",
      "Total Loss: 846150.75\n",
      "\n",
      "Total Fwd Loss: 9.014399528503418\n",
      "Total Bwd Loss: 7.058327674865723\n",
      "Total Inv Consistency Loss: 846134.6875\n",
      "Total Temp Consistency Loss: 0.009845530614256859\n",
      "Total Loss: 846009.1875\n",
      "\n",
      "Total Fwd Loss: 7.824091911315918\n",
      "Total Bwd Loss: 9.027052879333496\n",
      "Total Inv Consistency Loss: 845992.3125\n",
      "Total Temp Consistency Loss: 0.01162015926092863\n",
      "----------Training epoch--------\n",
      "----------------182---------------\n",
      "\n",
      "Total Loss: 845870.25\n",
      "\n",
      "Total Fwd Loss: 6.540787696838379\n",
      "Total Bwd Loss: 7.7256035804748535\n",
      "Total Inv Consistency Loss: 845856.0\n",
      "Total Temp Consistency Loss: 0.010192254558205605\n",
      "Total Loss: 845729.5\n",
      "\n",
      "Total Fwd Loss: 7.625572204589844\n",
      "Total Bwd Loss: 7.581387996673584\n",
      "Total Inv Consistency Loss: 845714.3125\n",
      "Total Temp Consistency Loss: 0.010140547528862953\n",
      "Total Loss: 845590.875\n",
      "\n",
      "Total Fwd Loss: 7.906075954437256\n",
      "Total Bwd Loss: 8.596506118774414\n",
      "Total Inv Consistency Loss: 845574.375\n",
      "Total Temp Consistency Loss: 0.012182680889964104\n",
      "Total Loss: 845451.375\n",
      "\n",
      "Total Fwd Loss: 8.390413284301758\n",
      "Total Bwd Loss: 8.291727066040039\n",
      "Total Inv Consistency Loss: 845434.6875\n",
      "Total Temp Consistency Loss: 0.010664751753211021\n",
      "Total Loss: 845314.25\n",
      "\n",
      "Total Fwd Loss: 9.254570007324219\n",
      "Total Bwd Loss: 8.666940689086914\n",
      "Total Inv Consistency Loss: 845296.3125\n",
      "Total Temp Consistency Loss: 0.010864903219044209\n",
      "Total Loss: 845177.1875\n",
      "\n",
      "Total Fwd Loss: 7.273782253265381\n",
      "Total Bwd Loss: 7.9326982498168945\n",
      "Total Inv Consistency Loss: 845162.0\n",
      "Total Temp Consistency Loss: 0.010397905483841896\n",
      "----------Training epoch--------\n",
      "----------------183---------------\n",
      "\n",
      "Total Loss: 845038.6875\n",
      "\n",
      "Total Fwd Loss: 7.891695499420166\n",
      "Total Bwd Loss: 7.590052604675293\n",
      "Total Inv Consistency Loss: 845023.1875\n",
      "Total Temp Consistency Loss: 0.010845527052879333\n",
      "Total Loss: 844900.375\n",
      "\n",
      "Total Fwd Loss: 7.1852288246154785\n",
      "Total Bwd Loss: 7.329033851623535\n",
      "Total Inv Consistency Loss: 844885.875\n",
      "Total Temp Consistency Loss: 0.010017093271017075\n",
      "Total Loss: 844750.3125\n",
      "\n",
      "Total Fwd Loss: 7.7348761558532715\n",
      "Total Bwd Loss: 7.908393859863281\n",
      "Total Inv Consistency Loss: 844734.6875\n",
      "Total Temp Consistency Loss: 0.009682941250503063\n",
      "Total Loss: 844618.9375\n",
      "\n",
      "Total Fwd Loss: 7.296962738037109\n",
      "Total Bwd Loss: 9.138744354248047\n",
      "Total Inv Consistency Loss: 844602.5\n",
      "Total Temp Consistency Loss: 0.012148615904152393\n",
      "Total Loss: 844475.875\n",
      "\n",
      "Total Fwd Loss: 7.857048988342285\n",
      "Total Bwd Loss: 7.6214919090271\n",
      "Total Inv Consistency Loss: 844460.375\n",
      "Total Temp Consistency Loss: 0.011502551846206188\n",
      "Total Loss: 844341.125\n",
      "\n",
      "Total Fwd Loss: 9.176015853881836\n",
      "Total Bwd Loss: 9.166929244995117\n",
      "Total Inv Consistency Loss: 844322.8125\n",
      "Total Temp Consistency Loss: 0.011514907702803612\n",
      "----------Training epoch--------\n",
      "----------------184---------------\n",
      "\n",
      "Total Loss: 844198.8125\n",
      "\n",
      "Total Fwd Loss: 7.074410438537598\n",
      "Total Bwd Loss: 7.253381252288818\n",
      "Total Inv Consistency Loss: 844184.5\n",
      "Total Temp Consistency Loss: 0.010219495743513107\n",
      "Total Loss: 844060.125\n",
      "\n",
      "Total Fwd Loss: 6.704456329345703\n",
      "Total Bwd Loss: 8.429648399353027\n",
      "Total Inv Consistency Loss: 844045.0\n",
      "Total Temp Consistency Loss: 0.013021565973758698\n",
      "Total Loss: 843926.0\n",
      "\n",
      "Total Fwd Loss: 9.382181167602539\n",
      "Total Bwd Loss: 7.738236427307129\n",
      "Total Inv Consistency Loss: 843908.875\n",
      "Total Temp Consistency Loss: 0.009250035509467125\n",
      "Total Loss: 843782.125\n",
      "\n",
      "Total Fwd Loss: 7.523504734039307\n",
      "Total Bwd Loss: 8.082880020141602\n",
      "Total Inv Consistency Loss: 843766.5\n",
      "Total Temp Consistency Loss: 0.010511195287108421\n",
      "Total Loss: 843648.375\n",
      "\n",
      "Total Fwd Loss: 8.325227737426758\n",
      "Total Bwd Loss: 9.16282844543457\n",
      "Total Inv Consistency Loss: 843630.875\n",
      "Total Temp Consistency Loss: 0.010978549718856812\n",
      "Total Loss: 843510.75\n",
      "\n",
      "Total Fwd Loss: 7.895752906799316\n",
      "Total Bwd Loss: 8.162813186645508\n",
      "Total Inv Consistency Loss: 843494.6875\n",
      "Total Temp Consistency Loss: 0.010981944389641285\n",
      "----------Training epoch--------\n",
      "----------------185---------------\n",
      "\n",
      "Total Loss: 843365.0625\n",
      "\n",
      "Total Fwd Loss: 7.367725372314453\n",
      "Total Bwd Loss: 8.329737663269043\n",
      "Total Inv Consistency Loss: 843349.375\n",
      "Total Temp Consistency Loss: 0.010308093391358852\n",
      "Total Loss: 843225.9375\n",
      "\n",
      "Total Fwd Loss: 7.657742500305176\n",
      "Total Bwd Loss: 7.0636115074157715\n",
      "Total Inv Consistency Loss: 843211.1875\n",
      "Total Temp Consistency Loss: 0.0093959029763937\n",
      "Total Loss: 843091.3125\n",
      "\n",
      "Total Fwd Loss: 8.389535903930664\n",
      "Total Bwd Loss: 8.30243968963623\n",
      "Total Inv Consistency Loss: 843074.625\n",
      "Total Temp Consistency Loss: 0.010385415516793728\n",
      "Total Loss: 842952.5625\n",
      "\n",
      "Total Fwd Loss: 9.198623657226562\n",
      "Total Bwd Loss: 7.47843074798584\n",
      "Total Inv Consistency Loss: 842935.875\n",
      "Total Temp Consistency Loss: 0.009166550822556019\n",
      "Total Loss: 842809.8125\n",
      "\n",
      "Total Fwd Loss: 7.458204746246338\n",
      "Total Bwd Loss: 8.45142650604248\n",
      "Total Inv Consistency Loss: 842793.875\n",
      "Total Temp Consistency Loss: 0.010012736544013023\n",
      "Total Loss: 842674.375\n",
      "\n",
      "Total Fwd Loss: 6.816020965576172\n",
      "Total Bwd Loss: 9.449163436889648\n",
      "Total Inv Consistency Loss: 842658.125\n",
      "Total Temp Consistency Loss: 0.012464137747883797\n",
      "----------Training epoch--------\n",
      "----------------186---------------\n",
      "\n",
      "Total Loss: 842544.5625\n",
      "\n",
      "Total Fwd Loss: 7.943002223968506\n",
      "Total Bwd Loss: 8.273239135742188\n",
      "Total Inv Consistency Loss: 842528.375\n",
      "Total Temp Consistency Loss: 0.00971989519894123\n",
      "Total Loss: 842396.6875\n",
      "\n",
      "Total Fwd Loss: 8.606755256652832\n",
      "Total Bwd Loss: 8.893950462341309\n",
      "Total Inv Consistency Loss: 842379.1875\n",
      "Total Temp Consistency Loss: 0.010235460475087166\n",
      "Total Loss: 842261.5\n",
      "\n",
      "Total Fwd Loss: 7.317310333251953\n",
      "Total Bwd Loss: 8.038956642150879\n",
      "Total Inv Consistency Loss: 842246.125\n",
      "Total Temp Consistency Loss: 0.009593834169209003\n",
      "Total Loss: 842117.25\n",
      "\n",
      "Total Fwd Loss: 7.355006217956543\n",
      "Total Bwd Loss: 9.032064437866211\n",
      "Total Inv Consistency Loss: 842100.875\n",
      "Total Temp Consistency Loss: 0.010969776660203934\n",
      "Total Loss: 841980.6875\n",
      "\n",
      "Total Fwd Loss: 8.164053916931152\n",
      "Total Bwd Loss: 7.902140140533447\n",
      "Total Inv Consistency Loss: 841964.625\n",
      "Total Temp Consistency Loss: 0.010565495118498802\n",
      "Total Loss: 841836.375\n",
      "\n",
      "Total Fwd Loss: 7.475884914398193\n",
      "Total Bwd Loss: 6.750400543212891\n",
      "Total Inv Consistency Loss: 841822.125\n",
      "Total Temp Consistency Loss: 0.009235171601176262\n",
      "----------Training epoch--------\n",
      "----------------187---------------\n",
      "\n",
      "Total Loss: 841705.3125\n",
      "\n",
      "Total Fwd Loss: 8.220987319946289\n",
      "Total Bwd Loss: 8.602867126464844\n",
      "Total Inv Consistency Loss: 841688.5\n",
      "Total Temp Consistency Loss: 0.010106462985277176\n",
      "Total Loss: 841560.875\n",
      "\n",
      "Total Fwd Loss: 7.0775861740112305\n",
      "Total Bwd Loss: 7.614595890045166\n",
      "Total Inv Consistency Loss: 841546.1875\n",
      "Total Temp Consistency Loss: 0.010062484070658684\n",
      "Total Loss: 841429.0625\n",
      "\n",
      "Total Fwd Loss: 7.570603847503662\n",
      "Total Bwd Loss: 7.615303993225098\n",
      "Total Inv Consistency Loss: 841413.875\n",
      "Total Temp Consistency Loss: 0.0094438586384058\n",
      "Total Loss: 841286.625\n",
      "\n",
      "Total Fwd Loss: 8.675718307495117\n",
      "Total Bwd Loss: 8.350142478942871\n",
      "Total Inv Consistency Loss: 841269.625\n",
      "Total Temp Consistency Loss: 0.009535352699458599\n",
      "Total Loss: 841150.3125\n",
      "\n",
      "Total Fwd Loss: 7.8420000076293945\n",
      "Total Bwd Loss: 8.313846588134766\n",
      "Total Inv Consistency Loss: 841134.1875\n",
      "Total Temp Consistency Loss: 0.010368715040385723\n",
      "Total Loss: 841012.1875\n",
      "\n",
      "Total Fwd Loss: 7.548550605773926\n",
      "Total Bwd Loss: 8.273533821105957\n",
      "Total Inv Consistency Loss: 840996.375\n",
      "Total Temp Consistency Loss: 0.01162184588611126\n",
      "----------Training epoch--------\n",
      "----------------188---------------\n",
      "\n",
      "Total Loss: 840874.0625\n",
      "\n",
      "Total Fwd Loss: 7.627176761627197\n",
      "Total Bwd Loss: 7.06514835357666\n",
      "Total Inv Consistency Loss: 840859.375\n",
      "Total Temp Consistency Loss: 0.009228003211319447\n",
      "Total Loss: 840732.0625\n",
      "\n",
      "Total Fwd Loss: 8.512395858764648\n",
      "Total Bwd Loss: 9.894411087036133\n",
      "Total Inv Consistency Loss: 840713.625\n",
      "Total Temp Consistency Loss: 0.01111598126590252\n",
      "Total Loss: 840601.3125\n",
      "\n",
      "Total Fwd Loss: 6.954721927642822\n",
      "Total Bwd Loss: 6.884829044342041\n",
      "Total Inv Consistency Loss: 840587.5\n",
      "Total Temp Consistency Loss: 0.010241280309855938\n",
      "Total Loss: 840457.625\n",
      "\n",
      "Total Fwd Loss: 7.759199619293213\n",
      "Total Bwd Loss: 8.696676254272461\n",
      "Total Inv Consistency Loss: 840441.1875\n",
      "Total Temp Consistency Loss: 0.01298290491104126\n",
      "Total Loss: 840319.0625\n",
      "\n",
      "Total Fwd Loss: 8.060295104980469\n",
      "Total Bwd Loss: 8.141026496887207\n",
      "Total Inv Consistency Loss: 840302.875\n",
      "Total Temp Consistency Loss: 0.010565128177404404\n",
      "Total Loss: 840174.9375\n",
      "\n",
      "Total Fwd Loss: 7.976614952087402\n",
      "Total Bwd Loss: 8.127145767211914\n",
      "Total Inv Consistency Loss: 840158.8125\n",
      "Total Temp Consistency Loss: 0.01023509819060564\n",
      "----------Training epoch--------\n",
      "----------------189---------------\n",
      "\n",
      "Total Loss: 840040.125\n",
      "\n",
      "Total Fwd Loss: 7.322916507720947\n",
      "Total Bwd Loss: 7.640397071838379\n",
      "Total Inv Consistency Loss: 840025.1875\n",
      "Total Temp Consistency Loss: 0.009839643724262714\n",
      "Total Loss: 839902.8125\n",
      "\n",
      "Total Fwd Loss: 7.063102722167969\n",
      "Total Bwd Loss: 7.964075565338135\n",
      "Total Inv Consistency Loss: 839887.8125\n",
      "Total Temp Consistency Loss: 0.009311122819781303\n",
      "Total Loss: 839769.75\n",
      "\n",
      "Total Fwd Loss: 10.098782539367676\n",
      "Total Bwd Loss: 8.7753267288208\n",
      "Total Inv Consistency Loss: 839750.875\n",
      "Total Temp Consistency Loss: 0.009119098074734211\n",
      "Total Loss: 839629.0\n",
      "\n",
      "Total Fwd Loss: 7.51540994644165\n",
      "Total Bwd Loss: 7.3378005027771\n",
      "Total Inv Consistency Loss: 839614.125\n",
      "Total Temp Consistency Loss: 0.009625115431845188\n",
      "Total Loss: 839486.1875\n",
      "\n",
      "Total Fwd Loss: 7.288097381591797\n",
      "Total Bwd Loss: 8.226861953735352\n",
      "Total Inv Consistency Loss: 839470.6875\n",
      "Total Temp Consistency Loss: 0.010641568340361118\n",
      "Total Loss: 839356.4375\n",
      "\n",
      "Total Fwd Loss: 7.6620306968688965\n",
      "Total Bwd Loss: 8.952611923217773\n",
      "Total Inv Consistency Loss: 839339.8125\n",
      "Total Temp Consistency Loss: 0.010065114125609398\n",
      "----------Training epoch--------\n",
      "----------------190---------------\n",
      "\n",
      "Total Loss: 839209.25\n",
      "\n",
      "Total Fwd Loss: 7.523542881011963\n",
      "Total Bwd Loss: 7.602397918701172\n",
      "Total Inv Consistency Loss: 839194.125\n",
      "Total Temp Consistency Loss: 0.009714009240269661\n",
      "Total Loss: 839075.9375\n",
      "\n",
      "Total Fwd Loss: 7.671393394470215\n",
      "Total Bwd Loss: 7.3620100021362305\n",
      "Total Inv Consistency Loss: 839060.875\n",
      "Total Temp Consistency Loss: 0.008892863057553768\n",
      "Total Loss: 838939.4375\n",
      "\n",
      "Total Fwd Loss: 8.463080406188965\n",
      "Total Bwd Loss: 7.459208011627197\n",
      "Total Inv Consistency Loss: 838923.5\n",
      "Total Temp Consistency Loss: 0.010112450458109379\n",
      "Total Loss: 838801.1875\n",
      "\n",
      "Total Fwd Loss: 7.465239524841309\n",
      "Total Bwd Loss: 8.109370231628418\n",
      "Total Inv Consistency Loss: 838785.625\n",
      "Total Temp Consistency Loss: 0.009706078097224236\n",
      "Total Loss: 838663.0625\n",
      "\n",
      "Total Fwd Loss: 6.87111759185791\n",
      "Total Bwd Loss: 9.33125114440918\n",
      "Total Inv Consistency Loss: 838646.875\n",
      "Total Temp Consistency Loss: 0.011102096177637577\n",
      "Total Loss: 838521.0\n",
      "\n",
      "Total Fwd Loss: 8.99305248260498\n",
      "Total Bwd Loss: 8.865938186645508\n",
      "Total Inv Consistency Loss: 838503.125\n",
      "Total Temp Consistency Loss: 0.009196866303682327\n",
      "----------Training epoch--------\n",
      "----------------191---------------\n",
      "\n",
      "Total Loss: 838386.875\n",
      "\n",
      "Total Fwd Loss: 8.848092079162598\n",
      "Total Bwd Loss: 8.00396728515625\n",
      "Total Inv Consistency Loss: 838370.0\n",
      "Total Temp Consistency Loss: 0.009464450180530548\n",
      "Total Loss: 838242.875\n",
      "\n",
      "Total Fwd Loss: 9.517412185668945\n",
      "Total Bwd Loss: 8.986698150634766\n",
      "Total Inv Consistency Loss: 838224.375\n",
      "Total Temp Consistency Loss: 0.00985630415380001\n",
      "Total Loss: 838111.25\n",
      "\n",
      "Total Fwd Loss: 7.802223205566406\n",
      "Total Bwd Loss: 7.592114448547363\n",
      "Total Inv Consistency Loss: 838095.875\n",
      "Total Temp Consistency Loss: 0.009788526222109795\n",
      "Total Loss: 837968.125\n",
      "\n",
      "Total Fwd Loss: 6.648388862609863\n",
      "Total Bwd Loss: 7.404571533203125\n",
      "Total Inv Consistency Loss: 837954.0625\n",
      "Total Temp Consistency Loss: 0.00873518455773592\n",
      "Total Loss: 837830.0\n",
      "\n",
      "Total Fwd Loss: 6.301756381988525\n",
      "Total Bwd Loss: 8.490748405456543\n",
      "Total Inv Consistency Loss: 837815.1875\n",
      "Total Temp Consistency Loss: 0.010856778360903263\n",
      "Total Loss: 837696.875\n",
      "\n",
      "Total Fwd Loss: 7.864604949951172\n",
      "Total Bwd Loss: 8.35806655883789\n",
      "Total Inv Consistency Loss: 837680.625\n",
      "Total Temp Consistency Loss: 0.01116327103227377\n",
      "----------Training epoch--------\n",
      "----------------192---------------\n",
      "\n",
      "Total Loss: 837562.1875\n",
      "\n",
      "Total Fwd Loss: 8.914097785949707\n",
      "Total Bwd Loss: 8.869112968444824\n",
      "Total Inv Consistency Loss: 837544.375\n",
      "Total Temp Consistency Loss: 0.009192117489874363\n",
      "Total Loss: 837419.375\n",
      "\n",
      "Total Fwd Loss: 7.158111572265625\n",
      "Total Bwd Loss: 7.555159568786621\n",
      "Total Inv Consistency Loss: 837404.6875\n",
      "Total Temp Consistency Loss: 0.009295263327658176\n",
      "Total Loss: 837274.0\n",
      "\n",
      "Total Fwd Loss: 7.84130859375\n",
      "Total Bwd Loss: 7.686460971832275\n",
      "Total Inv Consistency Loss: 837258.5\n",
      "Total Temp Consistency Loss: 0.009754055179655552\n",
      "Total Loss: 837143.5625\n",
      "\n",
      "Total Fwd Loss: 7.298164367675781\n",
      "Total Bwd Loss: 8.18853759765625\n",
      "Total Inv Consistency Loss: 837128.0625\n",
      "Total Temp Consistency Loss: 0.010986587032675743\n",
      "Total Loss: 837009.375\n",
      "\n",
      "Total Fwd Loss: 8.663622856140137\n",
      "Total Bwd Loss: 7.9641313552856445\n",
      "Total Inv Consistency Loss: 836992.75\n",
      "Total Temp Consistency Loss: 0.00913389865309\n",
      "Total Loss: 836872.75\n",
      "\n",
      "Total Fwd Loss: 6.9411187171936035\n",
      "Total Bwd Loss: 8.661473274230957\n",
      "Total Inv Consistency Loss: 836857.125\n",
      "Total Temp Consistency Loss: 0.010262259282171726\n",
      "----------Training epoch--------\n",
      "----------------193---------------\n",
      "\n",
      "Total Loss: 836732.75\n",
      "\n",
      "Total Fwd Loss: 8.16047477722168\n",
      "Total Bwd Loss: 7.9940667152404785\n",
      "Total Inv Consistency Loss: 836716.625\n",
      "Total Temp Consistency Loss: 0.009428082033991814\n",
      "Total Loss: 836591.875\n",
      "\n",
      "Total Fwd Loss: 7.566204071044922\n",
      "Total Bwd Loss: 8.200040817260742\n",
      "Total Inv Consistency Loss: 836576.125\n",
      "Total Temp Consistency Loss: 0.010359524749219418\n",
      "Total Loss: 836457.6875\n",
      "\n",
      "Total Fwd Loss: 7.274849891662598\n",
      "Total Bwd Loss: 7.934235572814941\n",
      "Total Inv Consistency Loss: 836442.5\n",
      "Total Temp Consistency Loss: 0.009767675772309303\n",
      "Total Loss: 836312.875\n",
      "\n",
      "Total Fwd Loss: 7.8926544189453125\n",
      "Total Bwd Loss: 8.109493255615234\n",
      "Total Inv Consistency Loss: 836296.875\n",
      "Total Temp Consistency Loss: 0.00867692194879055\n",
      "Total Loss: 836176.875\n",
      "\n",
      "Total Fwd Loss: 8.465652465820312\n",
      "Total Bwd Loss: 7.802668571472168\n",
      "Total Inv Consistency Loss: 836160.625\n",
      "Total Temp Consistency Loss: 0.008669661357998848\n",
      "Total Loss: 836042.5\n",
      "\n",
      "Total Fwd Loss: 7.596606254577637\n",
      "Total Bwd Loss: 8.738821983337402\n",
      "Total Inv Consistency Loss: 836026.1875\n",
      "Total Temp Consistency Loss: 0.010101689025759697\n",
      "----------Training epoch--------\n",
      "----------------194---------------\n",
      "\n",
      "Total Loss: 835907.5\n",
      "\n",
      "Total Fwd Loss: 8.115447998046875\n",
      "Total Bwd Loss: 8.454298973083496\n",
      "Total Inv Consistency Loss: 835890.9375\n",
      "Total Temp Consistency Loss: 0.009903548285365105\n",
      "Total Loss: 835766.1875\n",
      "\n",
      "Total Fwd Loss: 7.275215148925781\n",
      "Total Bwd Loss: 8.879594802856445\n",
      "Total Inv Consistency Loss: 835750.0625\n",
      "Total Temp Consistency Loss: 0.010860915295779705\n",
      "Total Loss: 835625.375\n",
      "\n",
      "Total Fwd Loss: 9.025257110595703\n",
      "Total Bwd Loss: 7.785114288330078\n",
      "Total Inv Consistency Loss: 835608.5625\n",
      "Total Temp Consistency Loss: 0.008621588349342346\n",
      "Total Loss: 835490.75\n",
      "\n",
      "Total Fwd Loss: 7.63433313369751\n",
      "Total Bwd Loss: 7.586869239807129\n",
      "Total Inv Consistency Loss: 835475.5\n",
      "Total Temp Consistency Loss: 0.008875648491084576\n",
      "Total Loss: 835352.1875\n",
      "\n",
      "Total Fwd Loss: 7.046438694000244\n",
      "Total Bwd Loss: 7.358713626861572\n",
      "Total Inv Consistency Loss: 835337.8125\n",
      "Total Temp Consistency Loss: 0.008990305475890636\n",
      "Total Loss: 835220.875\n",
      "\n",
      "Total Fwd Loss: 7.979889869689941\n",
      "Total Bwd Loss: 8.59123420715332\n",
      "Total Inv Consistency Loss: 835204.3125\n",
      "Total Temp Consistency Loss: 0.010140608064830303\n",
      "----------Training epoch--------\n",
      "----------------195---------------\n",
      "\n",
      "Total Loss: 835078.125\n",
      "\n",
      "Total Fwd Loss: 8.091520309448242\n",
      "Total Bwd Loss: 7.417400360107422\n",
      "Total Inv Consistency Loss: 835062.625\n",
      "Total Temp Consistency Loss: 0.009037842974066734\n",
      "Total Loss: 834940.5\n",
      "\n",
      "Total Fwd Loss: 7.457017421722412\n",
      "Total Bwd Loss: 8.257416725158691\n",
      "Total Inv Consistency Loss: 834924.8125\n",
      "Total Temp Consistency Loss: 0.01050120685249567\n",
      "Total Loss: 834802.625\n",
      "\n",
      "Total Fwd Loss: 9.12971019744873\n",
      "Total Bwd Loss: 9.714998245239258\n",
      "Total Inv Consistency Loss: 834783.75\n",
      "Total Temp Consistency Loss: 0.01085708849132061\n",
      "Total Loss: 834667.1875\n",
      "\n",
      "Total Fwd Loss: 7.333721160888672\n",
      "Total Bwd Loss: 7.947157859802246\n",
      "Total Inv Consistency Loss: 834651.9375\n",
      "Total Temp Consistency Loss: 0.010731379501521587\n",
      "Total Loss: 834519.125\n",
      "\n",
      "Total Fwd Loss: 7.7577362060546875\n",
      "Total Bwd Loss: 7.894157409667969\n",
      "Total Inv Consistency Loss: 834503.5\n",
      "Total Temp Consistency Loss: 0.008912771008908749\n",
      "Total Loss: 834396.625\n",
      "\n",
      "Total Fwd Loss: 7.304097652435303\n",
      "Total Bwd Loss: 7.52963399887085\n",
      "Total Inv Consistency Loss: 834381.8125\n",
      "Total Temp Consistency Loss: 0.008638088591396809\n",
      "----------Training epoch--------\n",
      "----------------196---------------\n",
      "\n",
      "Total Loss: 834250.875\n",
      "\n",
      "Total Fwd Loss: 7.59067440032959\n",
      "Total Bwd Loss: 8.018326759338379\n",
      "Total Inv Consistency Loss: 834235.25\n",
      "Total Temp Consistency Loss: 0.009046209044754505\n",
      "Total Loss: 834119.375\n",
      "\n",
      "Total Fwd Loss: 7.5557403564453125\n",
      "Total Bwd Loss: 8.9491605758667\n",
      "Total Inv Consistency Loss: 834102.875\n",
      "Total Temp Consistency Loss: 0.009987981989979744\n",
      "Total Loss: 833977.375\n",
      "\n",
      "Total Fwd Loss: 7.589637756347656\n",
      "Total Bwd Loss: 7.112388610839844\n",
      "Total Inv Consistency Loss: 833962.6875\n",
      "Total Temp Consistency Loss: 0.009326284751296043\n",
      "Total Loss: 833847.375\n",
      "\n",
      "Total Fwd Loss: 9.287300109863281\n",
      "Total Bwd Loss: 8.690533638000488\n",
      "Total Inv Consistency Loss: 833829.375\n",
      "Total Temp Consistency Loss: 0.009699629619717598\n",
      "Total Loss: 833704.9375\n",
      "\n",
      "Total Fwd Loss: 7.499297142028809\n",
      "Total Bwd Loss: 8.068230628967285\n",
      "Total Inv Consistency Loss: 833689.375\n",
      "Total Temp Consistency Loss: 0.009119040332734585\n",
      "Total Loss: 833569.375\n",
      "\n",
      "Total Fwd Loss: 7.39776086807251\n",
      "Total Bwd Loss: 7.958334922790527\n",
      "Total Inv Consistency Loss: 833554.0\n",
      "Total Temp Consistency Loss: 0.009431609883904457\n",
      "----------Training epoch--------\n",
      "----------------197---------------\n",
      "\n",
      "Total Loss: 833435.125\n",
      "\n",
      "Total Fwd Loss: 8.264238357543945\n",
      "Total Bwd Loss: 7.703651428222656\n",
      "Total Inv Consistency Loss: 833419.1875\n",
      "Total Temp Consistency Loss: 0.009537678211927414\n",
      "Total Loss: 833291.625\n",
      "\n",
      "Total Fwd Loss: 6.981178283691406\n",
      "Total Bwd Loss: 7.78125\n",
      "Total Inv Consistency Loss: 833276.875\n",
      "Total Temp Consistency Loss: 0.009420308284461498\n",
      "Total Loss: 833156.6875\n",
      "\n",
      "Total Fwd Loss: 7.309100151062012\n",
      "Total Bwd Loss: 7.5449957847595215\n",
      "Total Inv Consistency Loss: 833141.8125\n",
      "Total Temp Consistency Loss: 0.009469191543757915\n",
      "Total Loss: 833021.375\n",
      "\n",
      "Total Fwd Loss: 9.025838851928711\n",
      "Total Bwd Loss: 9.766751289367676\n",
      "Total Inv Consistency Loss: 833002.5625\n",
      "Total Temp Consistency Loss: 0.009318527765572071\n",
      "Total Loss: 832885.875\n",
      "\n",
      "Total Fwd Loss: 7.306558132171631\n",
      "Total Bwd Loss: 8.295310020446777\n",
      "Total Inv Consistency Loss: 832870.25\n",
      "Total Temp Consistency Loss: 0.010159009136259556\n",
      "Total Loss: 832739.875\n",
      "\n",
      "Total Fwd Loss: 7.952182769775391\n",
      "Total Bwd Loss: 7.829049110412598\n",
      "Total Inv Consistency Loss: 832724.125\n",
      "Total Temp Consistency Loss: 0.009278921410441399\n",
      "----------Training epoch--------\n",
      "----------------198---------------\n",
      "\n",
      "Total Loss: 832604.9375\n",
      "\n",
      "Total Fwd Loss: 7.668174743652344\n",
      "Total Bwd Loss: 8.521378517150879\n",
      "Total Inv Consistency Loss: 832588.75\n",
      "Total Temp Consistency Loss: 0.009987503290176392\n",
      "Total Loss: 832466.1875\n",
      "\n",
      "Total Fwd Loss: 7.073371887207031\n",
      "Total Bwd Loss: 7.658673286437988\n",
      "Total Inv Consistency Loss: 832451.4375\n",
      "Total Temp Consistency Loss: 0.009804188273847103\n",
      "Total Loss: 832330.8125\n",
      "\n",
      "Total Fwd Loss: 9.330913543701172\n",
      "Total Bwd Loss: 7.828984260559082\n",
      "Total Inv Consistency Loss: 832313.625\n",
      "Total Temp Consistency Loss: 0.008586700074374676\n",
      "Total Loss: 832199.375\n",
      "\n",
      "Total Fwd Loss: 7.774931907653809\n",
      "Total Bwd Loss: 8.32231330871582\n",
      "Total Inv Consistency Loss: 832183.25\n",
      "Total Temp Consistency Loss: 0.008996531367301941\n",
      "Total Loss: 832065.125\n",
      "\n",
      "Total Fwd Loss: 7.152822971343994\n",
      "Total Bwd Loss: 8.008578300476074\n",
      "Total Inv Consistency Loss: 832049.9375\n",
      "Total Temp Consistency Loss: 0.008785010315477848\n",
      "Total Loss: 831920.375\n",
      "\n",
      "Total Fwd Loss: 7.918519020080566\n",
      "Total Bwd Loss: 8.487279891967773\n",
      "Total Inv Consistency Loss: 831904.0\n",
      "Total Temp Consistency Loss: 0.009522652253508568\n",
      "----------Training epoch--------\n",
      "----------------199---------------\n",
      "\n",
      "Total Loss: 831788.9375\n",
      "\n",
      "Total Fwd Loss: 8.191393852233887\n",
      "Total Bwd Loss: 8.943347930908203\n",
      "Total Inv Consistency Loss: 831771.8125\n",
      "Total Temp Consistency Loss: 0.010596249252557755\n",
      "Total Loss: 831647.5625\n",
      "\n",
      "Total Fwd Loss: 8.037134170532227\n",
      "Total Bwd Loss: 7.968786716461182\n",
      "Total Inv Consistency Loss: 831631.5625\n",
      "Total Temp Consistency Loss: 0.009249767288565636\n",
      "Total Loss: 831503.3125\n",
      "\n",
      "Total Fwd Loss: 7.896550178527832\n",
      "Total Bwd Loss: 7.78678035736084\n",
      "Total Inv Consistency Loss: 831487.625\n",
      "Total Temp Consistency Loss: 0.008825021795928478\n",
      "Total Loss: 831373.6875\n",
      "\n",
      "Total Fwd Loss: 7.529394626617432\n",
      "Total Bwd Loss: 8.17949104309082\n",
      "Total Inv Consistency Loss: 831358.0\n",
      "Total Temp Consistency Loss: 0.009624918922781944\n",
      "Total Loss: 831235.8125\n",
      "\n",
      "Total Fwd Loss: 7.71142053604126\n",
      "Total Bwd Loss: 7.754834175109863\n",
      "Total Inv Consistency Loss: 831220.375\n",
      "Total Temp Consistency Loss: 0.008687147870659828\n",
      "Total Loss: 831100.4375\n",
      "\n",
      "Total Fwd Loss: 7.64034366607666\n",
      "Total Bwd Loss: 8.091055870056152\n",
      "Total Inv Consistency Loss: 831084.6875\n",
      "Total Temp Consistency Loss: 0.00884001050144434\n",
      "----------Training epoch--------\n",
      "----------------200---------------\n",
      "\n",
      "Total Loss: 830961.6875\n",
      "\n",
      "Total Fwd Loss: 7.565164089202881\n",
      "Total Bwd Loss: 8.494738578796387\n",
      "Total Inv Consistency Loss: 830945.625\n",
      "Total Temp Consistency Loss: 0.010494669899344444\n",
      "Total Loss: 830829.5625\n",
      "\n",
      "Total Fwd Loss: 7.592691898345947\n",
      "Total Bwd Loss: 7.116721153259277\n",
      "Total Inv Consistency Loss: 830814.875\n",
      "Total Temp Consistency Loss: 0.008395388722419739\n",
      "Total Loss: 830692.9375\n",
      "\n",
      "Total Fwd Loss: 8.150384902954102\n",
      "Total Bwd Loss: 8.04114818572998\n",
      "Total Inv Consistency Loss: 830676.75\n",
      "Total Temp Consistency Loss: 0.009009240195155144\n",
      "Total Loss: 830554.8125\n",
      "\n",
      "Total Fwd Loss: 7.665335655212402\n",
      "Total Bwd Loss: 8.033079147338867\n",
      "Total Inv Consistency Loss: 830539.125\n",
      "Total Temp Consistency Loss: 0.0090864272788167\n",
      "Total Loss: 830423.4375\n",
      "\n",
      "Total Fwd Loss: 8.729206085205078\n",
      "Total Bwd Loss: 8.770946502685547\n",
      "Total Inv Consistency Loss: 830405.9375\n",
      "Total Temp Consistency Loss: 0.008897052146494389\n",
      "Total Loss: 830277.0\n",
      "\n",
      "Total Fwd Loss: 7.3121843338012695\n",
      "Total Bwd Loss: 8.28209114074707\n",
      "Total Inv Consistency Loss: 830261.375\n",
      "Total Temp Consistency Loss: 0.00939289852976799\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embedding_model = em.FF_AE([264,100,100,20], [20,100,100,264])\n",
    "operator_model = op.LinearizingKoop(linearizer=op.FFLinearizer([20,30,40], [40,30,20]), koop=op.InvKoop(latent_dim=40))\n",
    "\n",
    "LinKoopAE_model = ko.KoopmanModel(embedding=embedding_model, operator=operator_model)\n",
    "\n",
    "# Run training loop\n",
    "train(LinKoopAE_model, dataloader, lr= 0.01, learning_rate_change=0.8, num_epochs=200, max_Kstep=15, weight_decay=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9427d05a-d5d0-4077-a9db-98bdb912f646",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(LinKoopAE_model.state_dict(), '/Users/daviddornig/Documents/Master_Thesis/Bioinf/Code/philipp-trinh/KOOPOMICS/model_states/TestingKOOP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0088b94b-100a-49b6-a9ed-2dd94c6032b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bwdM, fwdM = LinKoopAE_model.Kmatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "826a8f17-a716-405e-aa34-2c422948ca39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3853,  0.2107,  0.5257,  ...,  0.9559,  0.2401,  0.1008],\n",
       "        [ 0.5296, -0.0270,  0.4838,  ...,  0.1532,  0.7025,  0.9606],\n",
       "        [ 0.8651, -0.0149,  0.3882,  ...,  0.7340, -0.0347,  0.8373],\n",
       "        ...,\n",
       "        [ 0.9392,  0.7810,  0.8306,  ...,  0.5010,  0.2362,  0.4086],\n",
       "        [ 0.7713,  0.8100,  0.3198,  ...,  0.5779,  0.2157,  0.6683],\n",
       "        [ 0.1899,  0.5303,  0.4013,  ...,  0.1001,  0.4425,  0.1219]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fwdM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3cb380b2-bfd6-40dc-9911-5292a16fec59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix:\n",
      "tensor([[ 0.3853,  0.2107,  0.5257,  ...,  0.9559,  0.2401,  0.1008],\n",
      "        [ 0.5296, -0.0270,  0.4838,  ...,  0.1532,  0.7025,  0.9606],\n",
      "        [ 0.8651, -0.0149,  0.3882,  ...,  0.7340, -0.0347,  0.8373],\n",
      "        ...,\n",
      "        [ 0.9392,  0.7810,  0.8306,  ...,  0.5010,  0.2362,  0.4086],\n",
      "        [ 0.7713,  0.8100,  0.3198,  ...,  0.5779,  0.2157,  0.6683],\n",
      "        [ 0.1899,  0.5303,  0.4013,  ...,  0.1001,  0.4425,  0.1219]])\n",
      "\n",
      "Eigenvalues:\n",
      "[ 1.8043921e+01+0.j          1.5780556e+00+0.j\n",
      "  1.5496212e+00+0.15203416j  1.5496212e+00-0.15203416j\n",
      "  1.1265293e+00+1.0908j      1.1265293e+00-1.0908j\n",
      "  5.1111466e-01+1.5807363j   5.1111466e-01-1.5807363j\n",
      " -1.7156861e+00+0.j         -1.5574535e+00+0.49929166j\n",
      " -1.5574535e+00-0.49929166j -7.9199940e-01+1.3327897j\n",
      " -7.9199940e-01-1.3327897j  -1.3066077e+00+0.4852741j\n",
      " -1.3066077e+00-0.4852741j  -8.1233221e-01+1.0872129j\n",
      " -8.1233221e-01-1.0872129j   8.3763981e-01+0.888135j\n",
      "  8.3763981e-01-0.888135j   -1.5454414e-01+1.260968j\n",
      " -1.5454414e-01-1.260968j    3.1075841e-01+1.1604011j\n",
      "  3.1075841e-01-1.1604011j  -7.9406714e-03+1.1555425j\n",
      " -7.9406714e-03-1.1555425j  -4.4086134e-01+0.9157824j\n",
      " -4.4086134e-01-0.9157824j  -9.5874375e-01+0.j\n",
      "  8.5444385e-01+0.3628862j   8.5444385e-01-0.3628862j\n",
      "  9.9887383e-01+0.j         -6.8923825e-01+0.j\n",
      " -3.3682507e-01+0.5150274j  -3.3682507e-01-0.5150274j\n",
      "  5.8690643e-01+0.j          1.9686066e-01+0.38394734j\n",
      "  1.9686066e-01-0.38394734j  1.5809260e-01+0.j\n",
      " -2.8213719e-01+0.11902727j -2.8213719e-01-0.11902727j]\n",
      "\n",
      "Eigenvectors:\n",
      "[[ 0.13601285+0.j         -0.01594173+0.j          0.31350803-0.05870511j\n",
      "  ...  0.12688498+0.j          0.06868673-0.0610616j\n",
      "   0.06868673+0.0610616j ]\n",
      " [ 0.14129834+0.j         -0.22499867+0.j         -0.05077313+0.01889968j\n",
      "  ... -0.11496764+0.j          0.01877277+0.0105385j\n",
      "   0.01877277-0.0105385j ]\n",
      " [ 0.14831567+0.j         -0.29071215+0.j          0.37131977+0.j\n",
      "  ...  0.17872894+0.j         -0.1646825 +0.03131454j\n",
      "  -0.1646825 -0.03131454j]\n",
      " ...\n",
      " [ 0.1407949 +0.j         -0.19881955+0.j          0.370161  -0.00129964j\n",
      "  ...  0.03393131+0.j          0.15559262-0.07120156j\n",
      "   0.15559262+0.07120156j]\n",
      " [ 0.15314466+0.j          0.01312907+0.j          0.03496956-0.07790717j\n",
      "  ... -0.2725651 +0.j          0.2490261 -0.02118946j\n",
      "   0.2490261 +0.02118946j]\n",
      " [ 0.17141217+0.j         -0.16882053+0.j         -0.0478207 +0.06796748j\n",
      "  ... -0.15294656+0.j          0.207066  -0.02723609j\n",
      "   0.207066  +0.02723609j]]\n"
     ]
    }
   ],
   "source": [
    "eigenvalues, eigenvectors = np.linalg.eig(fwdM)\n",
    "print(\"Matrix:\")\n",
    "print(fwdM)\n",
    "print(\"\\nEigenvalues:\")\n",
    "print(eigenvalues)\n",
    "print(\"\\nEigenvectors:\")\n",
    "print(eigenvectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6c4f3bc0-1420-4cd6-8c2b-02c84c121f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix:\n",
      "tensor([[0.3611, 0.8033, 0.3150,  ..., 0.5948, 0.1467, 0.7168],\n",
      "        [0.5875, 0.4504, 0.6585,  ..., 0.5301, 0.0211, 0.3379],\n",
      "        [0.6481, 0.6321, 0.8504,  ..., 0.8729, 0.0842, 0.5294],\n",
      "        ...,\n",
      "        [0.7699, 0.7367, 0.7524,  ..., 0.5761, 0.4344, 0.5292],\n",
      "        [0.8553, 0.7579, 0.8779,  ..., 0.3917, 0.3738, 0.5136],\n",
      "        [0.2666, 0.4551, 0.2044,  ..., 0.4498, 0.9479, 0.1683]])\n",
      "\n",
      "Eigenvalues:\n",
      "[18.028337  +0.j         -0.5232404 +1.4457221j  -0.5232404 -1.4457221j\n",
      " -0.24709545+1.5045081j  -0.24709545-1.5045081j  -1.2419859 +0.83394414j\n",
      " -1.2419859 -0.83394414j -1.4208968 +0.36477897j -1.4208968 -0.36477897j\n",
      " -0.7774306 +0.9439604j  -0.7774306 -0.9439604j  -1.1014322 +0.5072074j\n",
      " -1.1014322 -0.5072074j   1.4020922 +0.6772282j   1.4020922 -0.6772282j\n",
      "  1.4900359 +0.j          1.2103435 +0.81177014j  1.2103435 -0.81177014j\n",
      "  0.51833946+1.1735047j   0.51833946-1.1735047j   0.92013323+0.7221648j\n",
      "  0.92013323-0.7221648j   0.29973814+0.8952062j   0.29973814-0.8952062j\n",
      "  1.0231293 +0.29813558j  1.0231293 -0.29813558j  0.6622453 +0.6340976j\n",
      "  0.6622453 -0.6340976j   0.8428481 +0.j         -1.1049781 +0.j\n",
      " -0.3247512 +0.7229038j  -0.3247512 -0.7229038j   0.2883432 +0.25434017j\n",
      "  0.2883432 -0.25434017j -0.8008912 +0.j         -0.44652504+0.395923j\n",
      " -0.44652504-0.395923j   -0.22497308+0.09964599j -0.22497308-0.09964599j\n",
      " -0.41433072+0.j        ]\n",
      "\n",
      "Eigenvectors:\n",
      "[[-0.15272576+0.j          0.02621106-0.05633742j  0.02621106+0.05633742j\n",
      "  ...  0.04691976-0.02328j     0.04691976+0.02328j\n",
      "  -0.1087504 +0.j        ]\n",
      " [-0.13207011+0.j          0.34762457+0.j          0.34762457-0.j\n",
      "  ... -0.02179503+0.01086418j -0.02179503-0.01086418j\n",
      "   0.19698253+0.j        ]\n",
      " [-0.13363111+0.j         -0.04216085-0.03932985j -0.04216085+0.03932985j\n",
      "  ...  0.13433208-0.06105481j  0.13433208+0.06105481j\n",
      "  -0.25952008+0.j        ]\n",
      " ...\n",
      " [-0.1648489 +0.j         -0.09654196-0.01437708j -0.09654196+0.01437708j\n",
      "  ... -0.21908562+0.11985081j -0.21908562-0.11985081j\n",
      "   0.26813352+0.j        ]\n",
      " [-0.15626293+0.j          0.09916317-0.12455734j  0.09916317+0.12455734j\n",
      "  ...  0.02596885+0.05931405j  0.02596885-0.05931405j\n",
      "   0.13075562+0.j        ]\n",
      " [-0.17125641+0.j          0.01909551+0.23548295j  0.01909551-0.23548295j\n",
      "  ...  0.1656033 +0.10857318j  0.1656033 -0.10857318j\n",
      "  -0.04170286+0.j        ]]\n"
     ]
    }
   ],
   "source": [
    "eigenvalues, eigenvectors = np.linalg.eig(bwdM)\n",
    "print(\"Matrix:\")\n",
    "print(bwdM)\n",
    "print(\"\\nEigenvalues:\")\n",
    "print(eigenvalues)\n",
    "print(\"\\nEigenvectors:\")\n",
    "print(eigenvectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b87214b4-cf55-4d10-b472-e7acb7c48573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdAAAAGDCAYAAACbR0FZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAAsTAAALEwEAmpwYAABOlElEQVR4nO3debxV4/4H8M+neZ5HlQYNGqhIJFeZIxIuMmXOWCLCvbi4uETFz1woc4gIyVRJhhJK0qB5nue5zvn+/tjrsDs667vPPlb77Pq8vfbLbq/PftY6a++zn7PWXs/zpZlBREREcqdAqjdAREQkHakDFRERSYI6UBERkSSoAxUREUmCOlAREZEkqAMVERFJgjpQkWxIXkdyOclNJCsmkL+M5Li9sW3B+v5BcsbeWl8qkLyI5Gep3g6RMOpABQBAch7JE7M99rd1DCSNZP2/o60okSwMoB+Ak82slJmtzra8TvCzFIpwGxqQHEJyJckNJH8n+STJmgBgZl+bWaOo1h+yXfNI7iBZKdvjPwf7pE4CbSS0/8zsdTM7OY+bLBIpdaAiu6sKoBiAqalYefBHxngASwC0NLMyANoCmA3gmFRsUzZzAVyQ9Q+ShwAo8XeuIMo/TkT+TupAJWEkDyD5bnBkNJdkj7hlrUl+R3IdyaUknyJZJFg2NohNDk6Lnk+yPclFJHuTXBE8pzPJ00jOJLmG5L8SaT9YbiR7kJxDchXJR0nu8f1NsijJx0kuCW6PB481BJB1anQdyVF7ePrYuOWbSLaJa/cxkmuDfXNq3ONlSb4YbPdikg+QLJjDbr4XwDdmdouZLQIAM1thZo+b2ZCgvfYkFwX3byc5NNvP9wTJ//PWnXWGIaftzsGrALrG/ftSAK9kW3/H4Kh0A8mFJO8N23/BdnxDsj/J1QDujT/7QfLo4DWtFfy7ebC9BzvbKhItM9NNNwCYB+DEbI9dBmBccL8AgB8B3AOgCIB6AOYAOCVYfjiAowAUAlAHwDQAPePaMgD14/7dHsCuoL3CAK4GsBLAGwBKA2gKYCuAurlofzSACgAOBDATwFU5/Kz3A/geQBUAlQF8C+C/wbI6QVuFcnjuX5YH+2ln8DMUBHAdYkeQDJYPA/A8gJLBOicAuCaH9pcBuMx5rdoDWBTcrw1gC4DSwb8LAlgK4Chv3d525/QeQeyPjMbBcxYF22AA6sRt3yHBe+ZQAMsBdHb23y4A3YPXtzji3ntB5kEAo4JlUwDcmOrfGd10S/kG6JY/bsGH4yYA6+JuW/BnB3okgAXZnnMngEE5tNcTwLC4f++pA90KoGDw79JB5si4zI9ZH7wJtt8h7t/XA/gyh+fOBnBa3L9PATAvuP+XD/hsz82pA5gV9+8SQaYaYqeEtwMoHrf8AgCjc2h/V7af48bgtdgEYGDcvlsUlxkHoGtw/yQAs4P7oesO2+6Q98iJAO4C8D8AHQB8jlin90cHuofnPQ6gv7P/sr+3LsPuHWjh4P0wBcBI5NDJ66bb3rzpuwaJ19nMvsj6B8nLAFwV/LM2gANIrovLFwTwdZBtiNjFN60Q+yAuhNgHXpjVZpYR3N8a/H953PKtAErlov2FcffnAzggh/UeECxPJJuoZVl3zGwLSSC27RUQ+/BfGjwGxI7MFmZvILAaQPW4tp4C8BTJBwDUzOE5byDWMb4C4MLg30DsNfPWndN2h3kVsVOxdZHt9C0AkDwSwMMAmiF2tqIogHecNnPaH1nbtpPkYAD/B+AWM1MVDEk5fQcqiVoIYK6ZlYu7lTaz04LlzwKYDqCBxS58+RcA5tRYEhJpv1bc/QMROx25J0sQ61wSyWaX2w/uhYgdBVaK229lzKxpDvkvAZydy3W8A6B9cJXuWfizA83tuhNiZvMRu5joNADv7SHyBoDhAGqZWVkAz+HP1yqn/Re6X0nWAPAfAIMA9CVZNIlNF/lbqQOVRE0AsDG4aKU4yYIkm5E8IlheGsAGAJuCizuuy/b85Yh9b5osr30AuI1k+eBik5sAvJVDW28CuItkZcaGZNwD4LUEt2MlgEwk+LOY2VIAnyH2oV+GZAGSB5Fsl8NT7gXwD5L9gk4DwTY2DlnHSgBjEOtc5prZtCTXnRtXAjjezDbvYVlpAGvMbBvJ1ogdFWfJ1f4DAMYOiwcDeDFY71IA/01yu0X+NupAJSHBqdbTAbRA7OhjFYAXAJQNIrci9kG5EcBA/LXzuhfAy8FVtOclsQle+wDwAWKndScB+BixD9w9eQDARAC/IPad2k/BYy4z24LYBS3fBD/LUQk8rStipzJ/A7AWwFDEnabN1v5MxL5vronYVcsbAXyD2BHy3SHreAOx7yffyPZ4wuvODTObbWYTc1h8PYD7g22/B8Dbcc9LZv/1QOwCqLuDU7eXA7ic5D/y9EOI5FHWVYIiaY2kIXZ6d1aqt0VE9g86AhUREUmCOlAREZEk6BSuiIhIEnQEKiIikgR1oCIiIknItzMR1evfzz23fNA7W9x2mJnpZrZXKOZm5p/nn+pu/O9FbmZT69puZmHnDDdTeUxhN1Pn6t9Dl59XdYLbxu3jz3Ez0457wc3cveJwN/PeZ23cTMFt/twMTY73L8TdcFdOk/r8aenR/vti6LWPuZmpO6q5mTvev8jN1Os93s3MezB8VEj9Z+aHLgeAzS1ruJlm/5nsZqoV2eBmzi+b00iYP11+ay83s/wI/31R9nc/k3HGGjezeWoFN1Nv2EY3s6G+N+ETUHaG387qQ0uHLr/tzjfdNr7fdJCb6d9iyN85McpuMpc1zNN3iQWqzYxs28Lk2w5URET2D5nwD3TCpOpUapRFgQ8GcCaArD9nFwMYnjVLioiICABkWN460FQdCUbScZO8HcAQxOa/nBDcCOBNkndEsU4REUlPmbA83VIlqo77SgBNzWxn/IMk+wGYililhr8g2Q1ANwCoeO4/UaaN/52YiIhIKkR16jgTey4PVT1YtkdmNsDMWplZK3WeIiL7h8w8/pcqUR2B9gTwJcnf8WedvwMB1EesQLCIiAgAICNNJ/SJpAM1s5FBAeTW2P0ioh/iCiiLiIik9HvMvMi3U/k9M+M4d8MWbK/otvN97yPcTMFtfp/+6CvPuZmMBM6IVyyw3c1c0tMf97bsnzvcTP3Lwy94nvHEoW4bB/8rfCwpAFidPZ2t392aQ8u4mW2d17uZjPHl3MyW+jvdzBGN5rqZgXU+cjNdOlzqZrbWLutmvhz4vJtp+egNbqbthT+FLm9Vep7bxldrG7mZXea/1+eu938/l831Mx92fNzNPLH8RDcze4O/rkENs1eD+6sKBfwx2Il0CM1HdnczhVf566rbekHo8iZll7ltfPh1Kzczp0evyMZarltSK08dUbkDFmocqIiI7H8y0vQIVB2oiIikVLqewlUHKiIiKaWLiERERJKQuoEoeaNqLCIiIknQEaiIiKSULiISERFJQkZ69p/5twN9alo7N1PrQX/oz4bDi7iZqmNXupn/LeroZn6Y7NfUO/g5f6xjtWfnuJkrK/3iZv77Qvg2V66w1m1jel//Z6r2uT9WreTSXW7m4KqL3cy4Jn6NzoZP+etqOnCJm3lrYz03s6KtP7Zw04mb3Ey/NQ3cTM1P/PfpyGaHhC7/7UC/NumOXf7HwrL5fk3M/x73rpt549LD3Ezjzv5rXqmIv49HTfHHPXf6qLebqTrBr0O86tASbmbm3U+6mcvm++NbV2wNrys6fJw/xrNh84VuJkrp+h1ovu1ARURk/5CBlMyDkGe6iEhERCQJkXWgJA8meQLJUtke7xDVOkVEJP1kWt5uqRJVQe0eAD4A0B3AryTPjFv8UMjzupGcSHLi2s9+jGLTREQkn8kA83RLlai+A70awOFmtolkHQBDSdYxsyeAnH9aMxsAYAAANHn/3jS9LktERHIjXb8DjaoDLWBmmwDAzOaRbI9YJ1obIR2oiIhIuojqO9DlJFtk/SPoTE8HUAlA+HX2IiKyX8k05umWKlEdgXYFsNtgPDPbBaArSb/wIYD+zd92M/c0vsrNFNrqnwleclJlN7N5kT+uq93h4fU3AWA5/NqZXat962b6zj3JzRQuGj4eckdGQbeNQsX92pqFN/l/h53Z7ws388mh/tjCp2e+7mbuHH+lm2lYzK+R+Mj/dXEz1V6d5GaqX+DXA/1mjT/edvbF/vu06UHhdU4zepRz21jQy3+vc7v/mr9y+RluplCpdW7mvpX+39zDPmjrZl7r6o+7vPwVv0bnrOv8n719Q3+cdqfTL3Ezs7v47532x4Wva+fn1d02Xj7b/7wFHk8gkxydwo1jZotCln0TxTpFRCQ9ZaTpiEpNpCAiIimVytOweZGe3b6IiEiK6QhURERSSt+BioiIJCHD0vNkqDpQERFJqcw0/TZRHaiIiKRUup7CpVn+nDGv6Z393Q17+OpBbju3vna5m6k5aqub+XzIYDdzwdzj3czxFfyxon0+OtPN1B+y0c0sa1smvI3zfnfbqFpsg5upU2y1mxmxtKmbKX6x/zrsquuPaVtyu18PFN/54+t2lfSb2VnG//0ZdlZ/N/PppmZu5uVXTnYzPS5/P3T53O3+WNKHqvhjGC+a197NfDfTr6daYkZRN5PI61D2d/91yDh3jZup0nWFm1k6uKq/QZ/7Y5qLrfW3edmxGW6m8R3hv8dLL2rstlGkg19r9odTH4qslxs1r1GeOqLj68xISQ+sI1AREUkpfQcqIiKShMw0PYW717p9kq/srXWJiEj6yECBPN1SJZIjUJLDsz8E4DiS5QDAzDrl8LxuALoBQPUO56J8izZRbJ6IiEieRXUKtyaA3wC8AMAQ60BbAegb9qT4eqCJXEQkIiLpL12/A41qq1sB+BHAvwGsN7MxALaa2Vdm9lVE6xQRkTSUiQJ5uqVKVNVYMgH0J/lO8P/lUa1LRETSW0aaTiYfaacWlDU7l2RHAP6AwjhTbnrWzczetcnNFElgrRnF/d3Q5Pnr3ExxfwgZdlzo1+C0BF6VWb0Ku5nMlZmhy39/p4HbxpLl4W0AwPel/b8A1xzmj2c7qLFfe3T7v9a6mV3f+WNFi/tvHdT6xB83OHjEi26mUkG/vmbnUSe6mUsvHuNmHvslvE5s9Vf9cZcdf67hZlYfX9vN/Pi/fm7miHU3u5nyDfzXofy94XVQAQDTDnYjO1r6Y1fxmb8PN7bxxzRXuXiym7n1P7PczL1XXhy6vEDbdW4bm771xwfjVD+SLJUzC2FmHwP4eG+sS0REZG/QaVUREUmpzDS9iEgdqIiIpJRO4YqIiCQhXS8iSs9uX0REJMV0BCoiIimleqAiIiJJSNeZiNSBiohISqVrNZZ824E2n3CBm9m0ILxgNACceuGPbmb2lQe5maYn+QVnt1zub0+za5e4mUkF/MHct7b83M30G35G6PKC290m8NFjfjHozjf0dDOrC/sTMsy+zP8lqv6iX8z4276hUy4DAEZt9dt5aaRfIP2DTf5kFNO3+hM7nH/sd27msnIT3Mw3jx8RurzA9s1uGzN7+pMkXHOq//47vk8vN/Ov699zM693P93NZHxazc1sfrGUmxny8KNu5sQ3eruZ+1plr6exh3WNbu1mbvumpZsp57yku8aVc9s4sON8NxOldD0CTc+tFhERSbGoypkdCWCamW0gWRzAHQAOQ6xCy0Nmtj6K9YqISPpJ13GgUW31SwC2BPefAFAWwCPBY4MiWqeIiKShTGOebqkS1XegBcxsV3C/lZkdFtwfR3JSTk+KL6h9wPUdUeGUwyPaPBERyS90BLq7X0leHtyfTLIVAJBsCCDHkhtmNsDMWplZK3WeIiL7h0wrkKdbqkS15qsAtCM5G0ATAN+RnANgYLBMREQkrUVVUHs9gMtIlgFQN1jPIjNbHsX6REQkfWVoHOhfmdkGAH7V2D14q+ULbubCgle4mS8/8k8F77jCH6PY6HR//CYOLOJGhr16rJu54fKRbubtmzq4mQYLVoUuX9HXf9P+42l/LN81//NLvRamX1C7z1enuZmr7xvmZr7a6hcHPr2EX6T50bb++M33Ox/tZioN9scQ1yi+zs0UTOAzptDC8Nd8+Wn+GM86hy10Mx/++wQ389XTfkHtQesbu5miSze6mSMq+QW1J473P+7aj77JzRww0f+8aNrF/7xY85z/Wgx80P8cvLZoeEHto+rMc9uY87hfbBzt/EiyVM5MREQkCel6BJqe3b6IiEiK6QhURERSKl1P4abnVouIyD4jwwrk6ZYIkh1IziA5i+Qde1h+IMnRJH8m+QtJ96IMHYGKiEhKRV2NhWRBAE8DOAnAIgA/kBxuZr/Fxe4C8LaZPUuyCYARAOqEtasOVEREUmovVGNpDWCWmc0BAJJDAJyJ2PzsWQxAVkmtsgDcS6nVgYqIyL6uBoD48VmLAByZLXMvgM9IdgdQEsCJXqP5tgO9oO+tbqbKD35tw6tefsPN3DH+bDfD4sXczI5yfqbIBjeCl1/0x3g2+c8MNzPxm4ahy885wK9BOeVVfx/3b+S+z1BkUVE303jwMjfzvzXnuJnMulvczEOfF3cz60/32zn9tqluZtStbd3MwX2/djMnDfTrUGZeG768efuZbhvFCuY42+YfLuj3mZ85+p9uZuaNtdxM+fASpwCAUcvC3+sA8MXYt93M8M2V3MyLz3RyM8PW++PPy03w3+/Xv9XNzZRyjpN+mdDEbWNzG39sa5TyOiF8/DzqgQFmNiCXzVwAYLCZ9SXZBsCrJJuZWY47J992oCIisn/I62TyQWcZ1mEuBhD/11rN4LF4VwLoELT3HcliACoBWJFTo7oKV0REUmovlDP7AUADknVJFgHQBcDwbJkFAE4AAJKNARQDEDqNWFQFtbM2cImZfUHyQgBHA5iG2KG1f45IRET2C5kRH8uZ2S6SNwL4FEBBAC+Z2VSS9wOYaGbDAfQCMJDkzYhdUHSZmVlYu1Gdwh0UtF2C5KUASgF4D7HevTWAS/f0pPjz2DWOPxcVD2kT0eaJiMj+xMxGIDY0Jf6xe+Lu/wbAv2AhTlQd6CFmdijJQoidZz7AzDJIvoaQyeXjz2M379k/tOcXEZF9Q0YeLyJKlag60ALBadySAEogNqZmDYCiAApHtE4REUlDeb0KN1Wi6kBfBDAdsXPN/wbwTlBQ+ygAQyJap4iIpKF0nQs3qoLa/Um+FdxfQvIVxAalDjSzCYm08VVvv5bg5B3+WL57rr/KzVTqvs7NTO9zgJupMtIf61jlm/B6jQDAXf6YrAd6Zr+A7K9uvDu8ZuPku/1xq21/mONmMi8p62aWt/X3zdOjXnEzdy48081MH9LIzdz/7xfdzA0fX+5mTmk9xc282fo4NzOuw0Fupszx/rcaK44Jr7var/b7bhtn/M8fb9qm+2w3M+uaA91Mj9P9WrIjXve/lsqcXtrNbH9nl5t5qYU/ZvL35/2PzZ29jnIzO571a9LW7fiDm1l7cavQ5Ud399uY3i2BeqA9/Mj+JrJxoGa2JO7+OgBDo1qXiIikr3StB6qJFEREJKX0HaiIiEgS9B2oiIhIEqIuZxaV9Oz2RUREUkxHoCIiklKaSEFERCQJ6fodKJ25clOmQ/O73Q3j+k1uO5ta+OM3N9Xw/46oNNmvDVngx+l+pnJFNzPr+tpuptr3/pi2DVeFFx+t3tWvR7jyHH9cXLnZ29xMkTk5VgT601a/nZWnN3Az28v7f81ubLbDzczqMNDN3LfyEDfz6qTsdXv3YIM/QVfZ3/0PmbJnhBeHXLHeHy9ZbLSf6dXjLTdTkP5ny9ztld1MpUIb3czQxlXcTKF6ddzM79dUdzOfXPCYm7nkDr+e8Y5S/vs0kXHjcIaNc806t4lbvh3lZk6qOy2yw8RLxl+Vp47o1SNfSMkhrI5ARUQkpXQRURySZUk+THI6yTUkV5OcFjxWLop1ioiI7E1RnXh+G8BaAO3NrIKZVQRwXPDY2xGtU0RE0tBeKKgdiag60Dpm9oiZ/fElm5ktM7NHAOT4BR/JbiQnkpy4cPVPEW2aiIjkJ5lWIE+3VIlqzfNJ9iZZNesBklVJ3g5gYU5PMrMBZtbKzFrVqnhYRJsmIiL5iY5Ad3c+gIoAvgq+A10DYAyACgDOjWidIiIie01U5czWArg9uO2G5OUABkWxXhERST/pehXuXh8HSnKBmbmFAo+6qK+7YWW75Xg2+A+1S651M+NfaeFmdpZyI6j+7VY3s+72zW6mwgN+ndOZVxRxMwU3hv99lFnBHwvZ8Bk/s+nAkm6mYvd5bmbBu/XczOYa/vu17O8JjAOt40ZQ7BD/vVO0kD8et/z9/uuZUbSgm9l0oF+/dcUp20OX137VP+lUcFt4TVEAKLzWf6//3rW8m8ko5r+exar7vzOc5I9dbdtpsps5uORSNzNriz/mdOy7/ldQtYf647BXtKvmZkbf1z90+Wm/dXHbWPuFP/516sM3R9bLnfXNDXnqiIa1fXrfGQdK8pecFgGomsMyERHZD6mc2e6qAjgFsWEr8Qjg24jWKSIiaUgd6O4+AlDKzCZlX0ByTETrFBER2WuiuojoypBlF0axThERSU86AhUREUlCul6Fqw5URERSSkegIiIiSVAH+jfbXM0frza64QdupvmgHm6m3ldr3MzRr/tjyL4b1tTNrF5Z1s1U3OmPvfTGeALAJ+eE1y087R2/ZuHvF/l1Kief84SbKVHAH7da/7Cr3EzhJUXdzE5/WCo+7vqom3lzXSs3891p/tjVa8eMcDNPNWjkZpYPaulmbFv4eNLjHxvntnFdhR/dTMc7e7mZhg/PdDNrT/bru95wr7//Hp5wvps5qsxsN/PusX5919WDy7mZGmP9+sFIYAz+yPv82qOHfnBz6PIDRvudU/Xl/rhe+at824GKiMj+QUegIiIiSUjXDjSqgtplSP6P5KskL8y27Jko1ikiIunJjHm6pUpU1VgGITbr0LsAupB8l2TWl1dHRbROERGRvSaqDvQgM7vDzN43s04AfgIwimTFsCfFF9Re88t3EW2aiIjkJ5lgnm6pElUHWpTkH22b2YMABgIYi1id0D2KL6hd4dA2EW2aiIjkJyqovbsPARwf/4CZDQbQC4A/RkNERPYb6fodaFRz4fbO4fGRJB+KYp0iIpKe0vUq3FQMY7kPsYuMQmX44+XRe+nRf8PmAItOruBmzizzs5sZXautmymywJ9QYPCw/3MzV8/+p5vpPPC20OX9LnvZbeNfz1/qZs471c9cN+xDN3NQrRVuZnaGX2C4xhh/gHqPk/xttmUr3cyK1/1K60/V9ycLuGeOP3nBY4squ5kpCw4IXf7Fcn/Chm/b+/u4fO31bmZOd39dB37qF8t+tak/WUXZszLdzGVl/GLZL77i/w7vGua/Dt1efs3NPNH7AjdzxKc3uZnC68Inz+h872duGwM+OtnNyF+poLaIiKRUKk/D5oUKaouISErpFO7uVFBbREQSksC0wPmSCmqLiEhKpWs90KiGsYiIiOzTNJm8iIiklC4iEhERSUK6XkREy6ff3p5a6yZ3w96f4I8tbPL6jW6mzBz/xcv0h29ifeMMN1NoffiYLQDYWWmnmznoTX/cW2bB8J+r6Cq/iO6MK/xxjhUm+z/TppM2uZleh3zhZt7v2NrNYNt2N3Lt2LFu5rVl/nSSE+cc6GZsq/936oH+Wxkl5vljL1e1Dh/TvKplAr/vBfxMoxf913PG5aXdzEsdB7iZ2++/xs3sLJHA77BfGx5du410M9eVm+5mBq0/yM0MnOWPOS1dzH8vly8a/nu846Q1bhto5o9V/nTivZH1cs0/ujtPHdHk0/+bkh54r30HSrLK3lqXiIhI1KKaSCH7n8EEMIFkS8SOehP4k0hERPYH+g50d6sAzM/2WA3EypoZAH9uLhER2S+kawca1Snc2wDMANDJzOqaWV0Ai4L7OXae8fVAF276NaJNExGR/ETlzOKYWV8AVwG4h2Q/kqURO/L0nvdHPdBapZpFsWkiIpLPmOXtliqRXURkZovM7FwAYwB8DqBEVOsSERHZ2yK/CtfMhgM4DsCJAEDy8qjXKSIi6UMFtUOY2VYAWV9qJlQP9Iox4/6WdVee5GfKDc+p+tqfNpzqn1LOLOiPh3zoBvdHxx0D/b8xMor4Y0UXXbIrdHnJCWXdNhr3WeBmFj/pj/cr8GN5N9Nn/pluptqzy91MmeuLuZnnjj3Wzcy5pq6baXhs9mvl/mrG9JpuZsnF29xM2S8qupmi68PHB4/o1M9tY1mGP/Z32Unl3My6DP+k0wNXJvD3tD/UFj16vOtm3jrMH5tZ6wZ/gMBZB/rjgzPbHOJmqk+a5WZWXHSomyk97PfQ5WxQx23jiEGT3UyU0vUiItUDFRGRlMqf0/n4VA9URERSSkegu1M9UBER2aepHqiIiKRWmp7DVT1QERFJqb1xFS7JDiRnkJxF8o4cMueR/I3kVJJveG2qnJmIiKRU1JMhkCwI4GkAJwFYBOAHksPN7Le4TAMAdwJoa2ZrEymAoiNQERHZ17UGMMvM5pjZDgBDAGQfN3c1gKfNbC0AmNkKr9F8ewT6zPz2buZf4/3xdXVn+3ULl17hj7V6qufTbqZB4S1u5oQnb3MzxdqtcjNF+vhjyIo3OSp0eY3XZ7ptLLq4oZvZuMSvg/reFf3dzKtrjnYzw75v5WYW9/b/nL2sjT/OeMaUam6mwEV+XdYCvf1TTIUL+/tw9TE73EyREuGZix+61W2j6udL3MyOgf72Dmzwppvpc/bpbqbiZH//vdPZH9e78pLKbub2b5u4mQKP+YVFK090I3jqtdFu5rr7/PGka18OH89drJBfU/TNaYe7mf/6m5K0vF6FS7IbgG5xDw0ws/hiszUALIz79yIAR2ZrpmHQ1jcACgK418xCC8TutQ6UZEUzW7231iciImkijx1o0Fn61dnDFQLQAEB7ADUBjCV5iJmty+kJkZzCJfkwyUrB/VYk5wAYT3I+yXZRrFNERNLTXphMfjGAWnH/rhk8Fm8RgOFmttPM5gKYiViHmqOovgPtaGZZ5yEfBXC+mdVH7AvcvhGtU0RE0pHl8eb7AUADknVJFgHQBcDwbJn3ETv6RHAA2BDAnLBGo+pAC5HMOj1c3Mx+AAAzmwmgaE5Piq8HuuQjf35aERERj5ntAnAjgE8BTAPwtplNJXk/yU5B7FMAq0n+BmA0gNu8rx2j+g70GQAjSD4MYCTJJwC8B+B4AJNyelL8eezjRvVK06G1IiKSG3tjKj8zGwFgRLbH7om7bwBuCW4JiWomoidJTgFwHWKHwVlfzr4P4L9RrFNERNJUmh4uRXYVrpmNQayY9m6CeqB+TS8REdkvaDL5xCVUD3TOHH8MXsES/p8ty1v7tQ03tfHHb94241w3816zl91MImM8N4+v5Ga29fYzT17zXOjyK+pf7bZxSJPQ79ABADtvLOdmLp15s5u54qpP3Eztj/3X/IT/fe1mvjuhhps5oG0RN3POqJ/dzDst17mZQTO/cDO3LeroZkoXCq8r+tPWlm4btYa448fx+Xf+2OlvatZ2M8+f9qKb6V7tAjez+rAKbgbwx67e1+YDPzPin27mpv+85Wbu7tTVzVRaNN3N9Lor/P3ec0C30OUAUOtHf4wxzvMj+xvVAxURkdTSKdzdqB6oiIgkSKdw46keqIiIJEZHoH9SPVAREUlYmnagqsYiIiKShHxbjUVERPYTGsYiIiKSe1EX1I4KLZ9u+an1bnU3zIrnOK3uHxY86Nfu2z6rjJs5of1kNzPv+oPcTK+3hriZPldc4mYWd9/pZjJmlg5dzl1uE7CCfqbe0fPdzLMH+ePiStD/K/TK2f4YvIwz/Bqw0x8/2M3Urb3czfB/fo3J2g/NcDNXV/nKzfz3yA5u5pBPV4YuHzqqjdtG3Q/Cx5ICQKFN/rjBuXf6b57iX/vjtLcdu9HNFPk2/L0OAPXP8mvo/vK9/zucUcz/zGz8lD/eG2vXuxHbstXNzL01fExuqSP8bfmsuT+GveIBiyI7TKz9Yp88dUTzr0yg6G4EdAQqIiKplaancKOqB9qK5GiSr5GsRfJzkutJ/kDSnwpFREQkn4uyGst/AJRDbOKEm83sJJInBMv880giIrJfYP78JtEV1TCWwmb2iZm9iViVmKGI3fkSQLGcnhRfD3ThBtUDFRHZL0RfUDsSUXWg20ieTPJcAEayMwCQbIeQGZ3NbICZtTKzVrXK+JNVi4jIPsCYt1uK5PoULsnyAGqZWdgh4rUA+gDIRGxO3OtIDgawGIBfAkRERCSfS+gIlOQYkmVIVgDwE4CBJPvllDezyWZ2ipmdambTzewmMytnZk0BNPqbtl1ERPYF+/gp3LJmtgHA2QBeMbMjAZyY5DrvS/J5IiKyL0rTDjTRU7iFSFZHrKTqv73w31EPdF5ff2D0zll+pmpJfzD8HWe/52ae7OIP4C+webubSUSPF/1JB24e7hfjtZKZ4YHy/mB4FvTfnY/VG+pmLr3xFjez8By/4HHJMv4g/zofuhFgij/Iv0EZfwD6z7f4k3nMfqCJm7lvSR03s6qTP+HH29/VC11+zSl+4e6B1Y5xMwdW8SerKDDOL1qeyNdX44563s0cM/VWN/Pz7APdTMmV/gZVnOa/T2u8utTN3FTVfy0u7tvLzWyvGr49rzV93W3j8I9ucjPz/LrcyUvTq3AT7UDvB/ApgHFm9gPJegB+D8mrHqiIiCQmTSdSSLQD/dDM3sn6h5nNAXBOSF71QEVEZJ+WaAf6K8nlAL4ObuPMLMeJHFUPVEREErVPT6RgZvUBXABgCoCOACaTnBThdomIyP5iX76IiGRNAG0B/ANAcwBTAYyLcLtERETytURP4S4A8AOAh8zs2gi3R0RE9jP79ClcAC0BvALgQpLfkXyFZI7fc4qIiOzrEjoCNbPJJGcDmI3YadyLAbQD8GJUG5Yx1R/zVn2iXxHavvcLHh/15Go38+NLP7qZN2a0cjMnFPfHXp5w5WVupsEKv8jwxrrhxYqXnOb//VTu2yJu5szl/hjPs+8b72Y2vnCUm9le3h93uXBjWTfT6Gu/mPH8weFjKgFgRxv/fVpu1WY3c9Ir/uiukde2czNrG+dYqwEAsGlX+HIAyNzkF6FfU6qEm9lW1x8XXe0Nf/hC20H+GM96j01xM29P88ddnn/X2W7mlq8+dTMj1jd3Mxf188d47vJ3M65oOzZ8+f/5YzwP/iL7iMM9iHQcaHoOY0l0Kr+JAL4DcBaAaQCONbPaIfmyJB8mOZ3kGpKrSU4LHiv3t2y5iIjsG/bli4gAnGpmK3PR7tsARgFob2bLAIBkNQCXBstOztVWiojIvmsf/w50B8l+WbU6SfYlGXaerI6ZPZLVeQKAmS0zs0cA5HjkKiIiki4S7UBfArARsblwzwOwAcCgkPx8kr1J/jHvLcmqJG8HsDCnJ8UX1F7/w3cJbpqIiKQzWt5uqZJoB3qQmf3HzOYEt/sAhF1hcT6AigC+IrmW5BoAYwBUQKwD3qP4gtplj2iT4KaJiEha28e/A91K8hgzGwcAJNsC2JpT2MzWkhwE4HMA35vZH6UbSHYAMDIP2ywiIvuSffw70OsAPE1yHsn5AJ4CcE1OYZI9AHwA4EbE5tE9M27xQ8lurIiI7HvS9RRuouNAJwFoTjJr0NtmAF0A5FT382oAh5vZJpJ1AAwlWcfMnkCspJnrxYufdjPjzm7oZsaeXN/NnH19TzfzxbPPupmPP/bH6RVo6//NUnyxX2uRS/2LostuLh+6fGMtf4xsi8tzeon/NHV1dTczf3MFN3PfzS+7medO9Ou4LzzHr0N519DX3Mzlb93gZj6+8FE3szjDr1v78gq/Bueq3jme9PnDfxp+FLr8lWvOcNsofJ0/fnP9cv9nanHwfDezqldJNzOh6RtupmXZnm7moy1T3Uyrj/1t/u9NV7iZxRf5470rd1jhZrZ/UMXNjL6lbejyEYP6uG2cu9Qfayt/FfppTrIMyTtJPkXyJMQuJOoKYBZCvssEUCDrtK2ZzQPQHsCpJPshwQ5URET2E8a83VLEOxx6FUAjxKqwXA1gNIBzAZxlZmeGPG85yRZZ/wg609MBVAJwSF42WERE9jH76EVE9czsEAAg+QKApQAONLNtzvO6Athtnj0z2wWgK8nnk91YERHZ96TrZPJeB7oz646ZZZBclEDnCTNbFLLsm1xsn4iI7Ov20Q60OckNwX0CKB78mwDMzPyZtEVERPZBoR2omRXcWxsiIiL7p331FK6IiEi00rQDpVn+3PIT2z3kbtj9r7zgtnPtLxe7mcIfho+XBIByM/0xeGsbFXczVUctdTPYudONLDr7QL+d9utCF29c7I/le6fjU26mGP26rL3bdXEz217w34tzF/ljV09r9qubmX57MzdTdL5fI7HNe9PcTLEC/uv56ot+gaJSpyx3M0ULhr8WS7/xx8ge1/EnN3Nqeb/+5mFF/e2dsqOim+k16Eo30+I0/3VY8Jg/bnzF+e4lHij+rT92tdo3fr3eAtPnupn5Nx3qZrZXDP+9qfFVptvGkvP9cauzu/wrsvEiDR/on6eOaOZdN6dkLEuiMxHlSjB+9H8kXyV5YbZlz0SxThERkb0pkg4UsUotBPAugC4k3yVZNFh2VETrFBGRNJSuU/lF1YEeZGZ3mNn7ZtYJwE8ARpH0z9eIiIikgag60KIk/2jbzB4EMBDAWMTKnO1RfD3QxUsnRLRpIiKSr6TpTERRdaAfAjg+/gEzGwygF4Acv62Orwdao3rriDZNRETyE53CjWNmvQEsInkCyVJxj48E0COKdYqIiOxNUV2F2x2xeqDd8dd6oA9GsU4REUlTaXoKN6qJFLohj/VAM4v4fft/LvTr8pW7f72bObHHJDfz6dLGbqbqTX4dzxk3VHMzj5zxupu5beSFbqZKkfDxh5t2+S9Fjzu6u5nlCVxX3bDkGjdTpOMCN9Puq3VupnyhLW5m+Y3+eL8Db/DH/l5QbqKb6XHM+W7mm2/7u5nDX7nZzdR/Ylbo8mqH+WNSv21Z183MvKOpmyl6tz/meelGfzbQjOb+79V3Uxq4mcKt/InVrmn2tZs54yh/DOz9XTq6mVrF/bGis0a6EZx93PjQ5UOLHeG2cfA14e8bALEK0FHJn9MRuKLqQHerB0qyPWKdaG2oHqiIiMRJ16n8orqISPVARUQkMWl6CjeqDrQrgGXxD5jZLjPrCuDYiNYpIiKy10RyClf1QEVEJFE6hSsiIpKMvXAKl2QHkjNIziJ5R0juHJJGspXXpjpQERFJrYg7UJIFATwN4FQATQBcQLLJHnKlAdwEIPzS5oA6UBER2de1BjDLzOaY2Q4AQwCcuYfcfwE8AsAf54a9WFCbZBUzW5Fo/s4BL7uZCVsOcjMLt/m1Pt+d19zNVH2wiJuZdltRN1PnLX8cXvvzlrmZ0rP9v31WWvjc/YU2+W189FhfN3PFnM5uptFrfm3IneaP0/vhfv+1OuKhEW6m6lPF3My0f/s1V1dm+O2sO7qmmznv0A5upn4F/32Bt8J/pedP9V/z8p/4vzPL3ZNbwObZfu1RFslwM41unO1m5tzqj0s9aLA/LvV581+HLx713xc7WvqfTcsSqMVc9Eg3gl+OCP+9OeP7n9027pk2xl9RhPL6HSjJbojNP5BlgJkNiPt3DQAL4/69CMBue5fkYQBqmdnHJG9LZL2RdKAkK2R/CMAEki0RK+Ltj6oXEZH9Qx470KCzHOAGcxAUP+kH4LLcPC+qI9BVAOZne6wGYmXNDEC9iNYrIiLpJvqrcBcDqBX375rBY1lKA2gGYAxJAKgGYDjJTmaW43RjUXWgtwE4CcBtZjYFAEjONTN/jjAREdmv7IVhLD8AaECyLmIdZxcAf8yHambrEZvoJ7Y95BgAt4Z1nkB01Vj6ArgKwD0k+wVXNrm7KL4e6Mdvroti00REZD9jZrsA3AjgUwDTALxtZlNJ3k+yU7LtRnYRUTCZwrnBxn0OoEQCz/njPPbncxun6dBaERHJlb3waW9mIwCMyPbYPTlk2yfSZmQdKMmDEfvecxRiHehBweMdgrqgIiIimokoHskeiKsHCuBkM/s1WPxQFOsUEZE0laaTyUd1BHo18lgP9Oqxl7mZylX8Wp+vNRvsZh6u7o8/POvB89xM427+3yMZ8xe6mZX+0DhU7LjYzRQZEj4Ob/R9fg3KFm/f4mbatvnNzUxdX93NvFL/PTfzz00t/XYePt3NFCvh7+Siq/z3xR3dr3MzW7qtczMllvnX13V65ks38/IT4XUon+890G3jkfe6uplE6qmeXGOumxn9RQs3s+n4g91Mvb7+e/CYr/1xoPNmlnQzQ371T6Bd0PosN7O0s/+aV5rsjxtfdkN4vc/l/pB6nLn0MDfz7dt+O0lL0yNQ1QMVERFJguqBiohISjGPt1RRPVAREUktfQf6J9UDFRGRROkqXBERkf3IXqvGIiIiskdpegSqDlRERFJLHaiIiEjupet3oLQEirr+LSsiK5rZ6kTzp9a71d2wRZ39gr27/HHRqHCsX6h400fV3MyEO550M+1uvcHNlJm92c1c8qpfNPreD8MnfyhcZ5PbxrEH+sWMW5eZ42beOdQvQtzwWzeCRiX8wfBPvnOGm6nbb6qbsa1b3cyce/3K0pV/ynQzJa/P8bq7P1Qo6m/P+msqhS7nan/ykdUvlnYzFbvvcjMnDJ/sZgYP8gtYf3bTo27muJf8+sd1HgwtrAEAuGv6eDdz5Wv+73CBJhvdzLZNRdxMzff9Y5zSX88KXX7MGP+99faAE9zM5CdujmzESIsb++epI5r0VHTbFiaqqfweJlkpuN+K5BwA40nOJ9kuinWKiIjsTVFdhdvRzFYF9x8FcL6Z1UesRmjfiNYpIiJpiJa3W6pE9R1oIZKFghpsxc3sBwAws5kki0a0ThERSUdp+h1oVEegzwAYQfJ4ACNJPkGyHcn7AEzK6UnxBbUXbvglok0TEZH8REegcczsSZJTAFwHoGGwngYA3gfwQMjz/iionchFRCIiIqkS5TCWZYh1huOzKrMAsYLaAFRQW0REYtL0cGmvFNQmeWbcYhXUFhGRP2ky+d3kuaD2tHsru5kCq/09d9/pfhXYYgX8orXjr6jvZtpP8Ytubz5/g5spc84MN3PvcH9d93YK/9mnbq3ptvHzFc3czJSGh7qZcrVXupkPvw8fwwgAIyo2cTMFm/jjW5e+6hf4PrLaAjdzYdl33MzrH5zqZjpXn+Rm/lHidzdzwSm9Qpdvr1jRbWNok35u5spjb3YzHy3xx79W+XG7mzl+gD/Gs6A/LBWDZo92M52nXOZmKhyxws0sn+m/l0vP9Y9fVhzuRtC9z4+hyz9f19Rto8pT3/sresKPJCtdJ1JQQW0REUmtNO1AVVBbREQkCVEdgXYFsNtJlWBMaFeSz0e0ThERSUPcS1PK/t1UUFtERFIrPftPVWMREZHU0kVEIiIiyUjTDjSqi4hERET2aXutHmhu1X2yr7th1RKoH7mzhD9qpsutn7qZ88v4c/OeM+VyN/N9i6Fu5qxZJ7uZHedkuJlFFzUMXX7AV+vcNl4YPsDNdOl5q5vZcKD/t9rXvfzxh3N2+a9n70uudTMZxQq6mWr3+bVQF/+vgZspsN0fD5lZxN8/iy/a4a9rbonQ5UXWuU0kNNBscy3//Vd9nN9QsVX+AM4FJxd2Mw3un+Jmllzlj1feVMd/rU482v8sWHClX//2nuGvu5np2w9wM28eHD6mmQX993qnKf7Y1hsPHhXZEMTWl/XLU0c0YfAt+1Q90FYkR5N8jWQtkp+TXE/yB5Ito1iniIikKc1EtJtnAPwHQDkA3wK42cxOInlCsKxNROsVEZE0k64XEUX1HWhhM/vEzN4EYGY2FLE7XwIoFtE6RURE9pqoOtBtJE8meS4AI9kZAEi2A5Djlyfx9UA3fpPA3IwiIpL+0vQUblQd6LUAegG4AsApAI4juQ6x07c9cnqSmQ0ws1Zm1qp026Mi2jQREclPVFA7jplNJtkTwAEAFpnZTQBuAv6oByoiIhKTT0eDeKKsBzoMqgcqIiIOHYHu7moArfJSD7TnSZ+4mf6lT3IzxeYVcTOfX+yfLp76nD8ea1em//fIBXOPdzNz1vg1GwsP8sfhZYxzAtPnum1c3ayjm+k24V0302/GCW7m6bXN3czAse3dzKjX+7qZqy/u7mZmPX+wm6k49jc3Y1u3upkqXxV3M+uG+HUdN9QPH8f43kX93Tbm7PRrWfa5/RI3M7i//zqc8YJf67PEMjeCVef5RZ5Ou8yfhnvSMaXdzGd9/Rq5bwx/1s3cMescN9Os/FI3U6hKmdDlOxv5dX8/PnWVm7nR/7jY76geqIiIpFZ6nsFVPVAREUktZubtlipRdaBdAex24sXMdplZVwDHRrROERFJR2k6jEX1QEVEJKU0E5GIiMh+RPVARUQktdJ0HKg6UBERSal0PYWbbzvQekX8+nQ1PvY3/54+A93MiovCx1EBQNOiS9zMkCJHuplLKnznZqZXqepmBjZu5Gbm3x0+npR1arltZJYs6maGnOzvvwN2rHMzNcescTONXtjoZk4s54/xHPaKP06vakF/rO1RrW9xM0XW+PUYZ/zsr6v5OX590rV9aocu7/VqN7eNoe+94Ga+vmuCm/looz9esvZH693M5oe3uJkRTYa4mQ49b3IzpZr668JOfyTeBV/6NWlrfuK/L4r+e6GbefOnD0OXtxx1vdvGxc1TPMhTHeifSBYCcCWAsxCbzg8AFgP4AMCLZrYzivWKiIjsLVEdgb4KYB2AewFkXZFbE8ClAF4DcH5E6xURkTSjU7i7O9zMGmZ7bBGA70nOjGidIiKSjtL0IqKohrGsIXkuyT/aJ1mA5PkA1ub0pPh6oCPf9L8PExGR9KfJ5HfXBcAjAJ4O6oACQDkAo4Nle2RmAwAMAIAP5xyann+SiIhI7qTpp31UMxHNI9kPQF8AswEcDKANgN/MTHP6i4hI2ovqKtz/ADg1aP9zAK0BjAFwB8mWZvZgFOsVEZH0k64XEdEi+PKW5BQALQAURWxS+ZpmtoFkcQDjzexQr406Ax91N6xE5c3utvxy1Gtu5qFVTdzMmWV+djOdP+jpZqygv7+vaT/Kzbwx6wg3c0/Tj0KXP7egnduGPVTFzRzb3x/bOmye+5Jjw/oSbqbY9GJu5vWr/ZqXlzxzs5upNHWXmynysT8ecmtnf3xw9z5vuZl/FF/sZo7+IHxc6sF3THXbmNbP/3249ujRbqYw/bGt7UrOcDP3tD3TzSx4qryb2fVLWTdTcJsbwWUXfeZmRrWt4Wam9c1+neVfla/qj3uu/FD4WO05Z/u/V990eczNVKuxJLJSlO069slTR/TVx71TUiYzqu9Ad5lZBoAtJGeb2QYAMLOtZCqLz4iISL6TpkegUXWgO0iWMLMtAA7PepBkWQDqQEVE5A/pego3qg70WDPbDgBmFt9hFkZsMgUREZG0FtVVuNtzeHwVgFVRrFNERNKUJlIQERHJvb0xkQLJDiRnkJxF8o49LL+F5G8kfyH5JcnwygxQByoiIqlmebw5SBYE8DRiwyubALiAZPbLzX8G0CoYJTIUQB+vXXWgIiKyr2sNYJaZzTGzHQCGANhtfJSZjQ4ufAWA7xErgBIq39YDFRGR/QOj/w60BoD44qqLAIQN0L4SwCdeo/m2Az2uxW9u5qfl7h8IaNH3BjdToYNfLPu70+q5mYZrfnEzV0zyf67bR5/nZq4++is3c9uoHKcdBgBUHesX9C27fpObGX9OYzez/hZ/EPvVx4xxM+9/cbybuXCQP0lCo9P94tQP3PC+mymSwKisvsv9geyDzzjRzVT+5D03U2B7+EmljOHl3DZ6Vv3czTz5c3s3c3bTyW7m4nf8Itc7e/kfrrbQzxx9sv+7N3+jPyHDR3f678EFT/iTSDS68ic3U6BxfTcz8/Lw91ft5v4EHKfdf5ub+el5N5K8PA5uJNkNQHy1+AHB3OrJtHUxgFYA3JlmoprKryCAqxA7BB5pZt/ELbvLzB6IYr0iIpJ+8noEGl+IJAeLAdSK+3fN4LHdt4M8EcC/AbTLaTRJvKi+A30esd57NYD/CyaWz3J2ROsUEZF0FPFFRAB+ANCAZF2SRRCrCjY8PkCyJWJ9VyczW5FIo1F1oK3N7EIzexyx88ylSL5HsiiAlMxZKCIi+ycz2wXgRgCfApgG4G0zm0ryfpKdgtijAEoBeIfkJJLDc2juD1F9B1ok606w4d2CCi2jgg3co/jz2G3ubINGZzWKaPNERCTf2AsTKZjZCAAjsj12T9x9/0KEbKI6Ap1IskP8A2Z2H4BBAOrk9CQzG2BmrcyslTpPEZH9w96YSCEKkXSgZnYxgDUkjwAAkk1I3gJgiZkVjmKdIiKSpszydkuRyAtqk/wcse9BR0MFtUVEJJt0LXIZ1Xeg/8SeC2o/BmA8ALcDnfCeX4B56yFb3UzhMm4EBRN49W4b646pxaMndnIzj9/t/1w1t/t/UVVvt87NFFsU/vIWyPB/7s0H+mMYv3hykJs58Vd/bGvNImvczPaz17mZ0sPKuZnfvj7Izdxy+7Vu5qyXvnQzC7v4RcmtpP+reNXwbm6mwRvhBZgXLznQbePtxf746jIV/ZNXYyv4+7jMXP+9nsgpui2V/e159uxP3cwpvfxxqQV3+mM8T2wy3c18/ZZfuPy2Zv6Y3HeXHRYeOG+n20ahepvdjPyVCmqLiEhqpWk1FhXUFhGR1ErP/lMFtUVEJLX2wly4kVBBbRERkSTk28nkRURkP6EjUBERkSSk6ZUx6kBFRCSl9B3o32xXcT9TtdIGN9O99QduZuz6hm6mTiF/XRd+8rWbeWDKaW6mell/XYcXm+9mrrvg49DlT//qlrtLyGmX+uMlV3bb4Wb6vHCumynoN4OK7/t1H0f+6o/rvb796W5m0kZ/XOXG5lXdzJZKfm3Wk9v69SN7n/1F6PInVrZ32/hw+iFupush493MuOtbu5kl//BrS9QeMMPNVCxezM20aOm/Tz/o09/NdHm6l5uZP7GZm2lwsF+n8+Xe/tjyUjPCx0+vOamy20a5oT+7mUilaQca1Vy4f0Fy5t5al4iISNSimspvI/4c2ZP1J2aJrMfNLIH5gUREZL+QpkegUZ3CHQSgHIDbzGw5AJCca2Z1I1qfiIikqzS9iCiqaiw9ADwB4E2SPUgWQAJzTZDsRnIiyYnrJn4XxaaJiEg+Q7M83VIlsu9AzexHAFkFSr8C4H7LH18PtFyrNlFtmoiI5CcqZ7Y7kq0R+77z/0j+DOA4kqcFVcFFRETS2t6qB9oawBioHqiIiGSni4h2k+d6oDXHbHNXUqz9FjdTuaA/pnLKwy3czA0/+mP5Njzvj+X7Z/1JbmbYkH+4mRuf7OFmnnnyidDlz//c0W3j9PO+dTP/Gvy9mzm/4xVuZtsBft3CBR38fXzqt3PdzNCNfq3KLbsKu5n+Bw53M11X+uOMF3Uo4mY+/baFm2nTYXbo8m+eOsJto0QFf2zmqDePcTM771rtZkpl+nUo5xVo5GYK+R8FaNBtkpvp1cyvuXrpS5+5mQEfn+Rmetce6WbeucsfS/vdGy1Cl995/RC3jTvb/dPNREod6G5UD1RERBKTpr1CVBcR7SBZIriveqAiIrLPUT1QERFJKc2FG0f1QEVEJGHqQEVERJKQqQ5UREQk99L0CHSvVWMRERHZl9Dyac9/eLf+7oZV+mm9286Mq0u5mRLz/bGFP3d/0s0cM6mLmynX8Xc3s+XsI93MhgP9ba4xOLwu5pxbm7htZCZwjqL2SH/M7qH9JruZ39ZXczO7Mv2/+RZ+V8vN7Kztb3Pmdn8fF1/gjxUt7A9Fxq5/+KGKpf0xk1vfqh66/NV7HnPbeGWtP43m1/cf5WZKfjDRzSwf1sDNrF/iF28qvNp/o/54qV/r8+j+N7uZLTX8z8xGjy90M0uf9j+bHmoyzM3c8NHlocur+sO0UX7iSjfzyYyH/QHCSTq1Qe88dUSf/N4nsm0LE8kRKMkbSVYK7tcnOZbkOpLjSfrVekVEZP+RpnPhRnUK97rgilsgVpWlv5mVA3A7gOciWqeIiKSjTMvbLUWiuogovt0qZjYMAMxsDMnSEa1TRETSkaXn/DpRHYEOJTmYZD0Aw0j2JFmb5OUAFuT0pPh6oKumqR6oiIjkX1EV1P43YtVX3gRwC4D/AvgEQAMAF4U87496oJUaqx6oiMh+IU2/A41yHOhvAG40sx9INgXQAcA0M/MvnRURkf2HJlL4k+qBiohIwvLpcEpPvq0HuvqIXe5K1jX0x4cVSmAM3jVdP3YzBw+9wc30POkTN/Pu5y3dzIbNG91MpzpT3Mz7RcLrilZstdxto+yFa9xMxY/9N/+w71q5GZTyX/OSvxZ1M9/c5I91bDfhajdTeUhJN1PiZ7/26K46fi3ZxcX99/LqBHbhAb9vDV1eoYD/Wk26pLGbKbPGH+e4sZO/wetzvCLiT02b+aGmZZe6mRZfXetmyiXweVF14h6n+t7N7zf4Y5GLf+kPXSzW1K+RW2/YjtDli9oXc9sY13eomwEeTiCzf1E9UBERSS0dge5mB8kSZrYFqgcqIiJh1IHuRvVARUQkMZnpeVyleqAiIpJaaXoEqmosIiIiSVA9UBERSa00PQJVByoiIqmVphMp5Nt6oLVf6uNuWMk5fi3G2m/548N2zfbH8l01c56bGdzen35wVv8qbiZjUQk3U7j2JjezY3H4OMYSi/0z+OVnZriZArv899C2Cn5tzdWHuhGUmeWPnVvX2L8goUgNv7ZmkW/8ugfbKrkRdDl9rJv5vrn/t+zmf/o1ODOdX4lSi/wxjGsbFnczpZb44xPnn+a/5tXHuREM7tPXzfRfcYKbmd2jkZvJLOpvc8Fx/hjsTZ0PdzOPPfq0m7n/9AvczLxzwt+ETU/2axCvfKSumxn7wW2R1dzsUKlbnjqikasG7FP1QOuRfInkAyRLkRxI8leS75CsE8U6RURE9qaoLiIaDOAHAJsAfA9gOmJT+40E8FJE6xQRkXSUpvVAo+pAS5vZs2b2MIAyZtbXzBaa2YsAyke0ThERSUeqxrKbTJINAZQDUIJkKzObSLI+AP9LBhER2X+k6UQKUR2B9gbwIYCXAXQGcCfJ3wF8C+DunJ4UX1B701fjI9o0ERHJV3QE+icz+5JkVwCZQT3QtYh9B/qbmY0Ied4AAAOAxK7CFRERSRXVAxURkZSyND2Fm2/rgYqIyH4in85H4Mm39UBfOG6Qmyl2vD+Y+8aNfiFsZlR3MwOvaOFmuo99x80Upl80+u7Pr3Qz66yUm6n/Xnhx5d+v8l/+bVX8ySr+2+ktNzO4yUFuBuYXYK58pT/pxcFFtrmZzpV+djN9PrnQzWw6zF/Xm58c62Z2vexPcFB5lD9WfEXb8IkvOh021W1j5pX13Uzzwb+5mdFVJ7uZeqWvcDOnfNHTzbRt4k8WUHCj/1ptruEPEijWLoEZPxJwxeDubqbuRr+YeIZTY/6nuX5x79JXbXQzkUrTmYhUD1RERFLL0rNbUD1QERGRJKgeqIiIpJTpFK6IiEgSdApXREQk99L1CDSqmYhERET2aToCFRGR1ErTU7gws7S5AeiWX9rJT9uidvSaqx295qloZ3+/pdsp3G75qJ38tC1qZ++0k5+2Re3snXby07bkx3b2a+nWgYqIiOQL6kBFRESSkG4d6IB81E5+2ha1s3fayU/bonb2Tjv5aVvyYzv7NQZfKIuIiEgupNsRqIiISL6QFh0oyQ4kZ5CcRfKOJNuoRXI0yd9ITiV5Ux63qSDJn0l+lIc2ypEcSnI6yWkk2yTZzs3Bz/QryTdJFkvweS+RXEHy17jHKpD8nOTvwf/d+k45tPNo8HP9QnIYyXK5bSNuWS+SRrJSMtsSPN492J6pJPsk+TO1IPk9yUkkJ5JsnUA7e3zf5WY/h7SR230c+juQ6H4Oayc3+znk58rVfiZZjOQEkpODdu4LHq9LcnzwufEWySJJtvN68Pnza/C+CK3xl1M7ccv/j+SmJLeFJB8kOZOxz4weSbZzAsmfgn08jqRfw07+KtXjaBIYr1QQwGwA9QAUATAZQJMk2qkO4LDgfmkAM5NpJ669WwC8AeCjPLTxMoCrgvtFAJRLoo0aAOYCKB78+20AlyX43GMBHAbg17jH+gC4I7h/B4BHkmznZACFgvuPeO3sqY3g8VoAPgUwH0ClJLflOABfACga/LtKku18BuDU4P5pAMYk+77LzX4OaSO3+zjH34Hc7OeQ7cnVfg5pJ1f7GQABlAruFwYwHsBRwe9Cl+Dx5wBcl2Q7pwXLCODNZNsJ/t0KwKsANiW5LZcDeAVAgQT3cU7tzATQOHj8egCDvfeybn+9pcMRaGsAs8xsjpntADAEwJm5bcTMlprZT8H9jQCmIdb55BrJmgA6AnghmecHbZRF7EP6xWCbdpjZuiSbKwSgOMlCAEoAWJLIk8xsLIA12R4+E7GOHcH/OyfTjpl9ZmZZ1cO/B1AziW0BgP4AegNI6Mv6HNq5DsDD9meJvRVJtmMAygT3yyKB/Rzyvkt4P+fURhL7OOx3IOH9HNJOrvZzSDu52s8Wk3VEVzi4GYDjAQwNHnffyzm1Y2YjgmUGYAL8/bzHdkgWBPAoYvs5VMjPdB2A+y0oE5nAPs6pnVy/l+Wv0qEDrQFgYdy/FyHJji8LyToAWiL211gyHkfslyAv80/VBbASwCDGTgW/QLJkbhsxs8UAHgOwAMBSAOvN7LM8bFdVM1sa3F8GoGoe2spyBYBPcvskkmcCWGxmk/O4/oYA/hGczvuK5BFJttMTwKMkFyK2z+/MzZOzve+S2s8h791c7eP4dvKyn7NtT9L7OVs7PZHL/czYVyqTAKwA8DliZ63Wxf2BkdDnRvZ2zGx83LLCAC4BMDLJdm4EMDzudU+mjYMAnB+c2v6EZIMk27kKwAiSi4Kf6eFEtkl2lw4d6N+KZCkA7wLoaWYbknj+6QBWmNmPedyUQoidInzWzFoC2IzYqbzcbk95xI5m6gI4AEBJkhfncdsAxP56RYJHfjkh+W8AuwC8nsvnlQDwLwD35GX9gUIAKiB26uo2AG+TZBLtXAfgZjOrBeBmBGcPEhH2vkt0P+fURm73cXw7wfOS2s972J6k9vMe2sn1fjazDDNrgdjRYWsAB+f259lTOySbxS1+BsBYM/s6iXaOBXAugCfzuC1FAWwzs1YABgJ4Kcl2bgZwmpnVBDAIQL9Et0v+lA4d6GLEvp/JUjN4LNeCvyDfBfC6mb2X5Pa0BdCJ5DzETicfT/K1JNpZBGBR3F+4QxHrUHPrRABzzWylme0E8B6Ao5NoJ8tyktUBIPi/e7ozJyQvA3A6gIuCTiI3DkLsj4LJwb6uCeAnktWS2JRFAN4LTmdNQOzMgXtB0h5citj+BYB3EPugduXwvsvVfs7pvZvbfbyHdpLazzlsT673cw7tJLWfASD4GmQ0gDYAygVfawC5/NyIa6dDsJ3/AVAZsWsfEhbXznEA6gOYFeznEiRnJbEti/DnvhkG4NAktuVUAM3jPnveQt4+M/Zb6dCB/gCgAWNX1BUB0AXA8Nw2Evwl/CKAaWaW9F9bZnanmdU0szrBtowys1wf8ZnZMgALSTYKHjoBwG9JbNICAEeRLBH8jCcg9l1SsoYj9gGG4P8fJNMIyQ6InebuZGZbcvt8M5tiZlXMrE6wrxchdsHJsiQ2533EPsBAsiFiF2ytSqKdJQDaBfePB/C794SQ913C+zmnNnK7j/fUTjL7OeRneh+52M8h7eRqP5OszOAKZJLFAZyE2O/AaAD/DGLuezmHdqaTvArAKQAuyPruMYl2fjSzanH7eYuZ5Xjla07bgrh9jNg+mpnEtkwDUDZ4jRD3mOSW5YMrmbwbYlfBzUTse41/J9nGMYidJvsFwKTgdloet6s98nYVbgsAE4Nteh9A+STbuQ+xX65fEbvCr2iCz3sTse9NdyL2wXklgIoAvkTsQ+sLABWSbGcWYt9dZ+3r53LbRrbl85DYVbh72pYiAF4L9s9PAI5Psp1jAPyI2JXg4wEcnuz7Ljf7OaSN3O5j93cgkf0csj252s8h7eRqPyN2FPZz0M6vAO4JHq+H2EU/sxA7kg39vQhpZxdinz1Z23hPMu1ky3hX4ea0LeUAfAxgCoDvEDuSTKads4I2JgMYA6BeIp8Zuu1+00xEIiIiSUiHU7giIiL5jjpQERGRJKgDFRERSYI6UBERkSSoAxUREUmCOlCRbEhWIzmE5GySP5IcETdmLnu2DvdQQUZE9n2F/IjI/iMY3D8MwMtm1iV4rDlic9WGDloXkf2LjkBFdnccgJ1m9lzWAxabZH0cY/U3fyU5heT52Z9I8jKST8X9+yOS7YP7m4LnTyX5BcnWJMeQnEOyU9zz3yM5krE6oX2CxwuSHBy37puj3QUikggdgYrsrhlis+BkdzZiM0c1R2x+1x9Ijs1FuyURm/bxNpLDADyA2BRqTRArtZU1PWULxKqSbAcwg+STAKogVr6sGRArxJ67H0lEoqAjUJHEHAPgTYtVtlgO4CsAuSmLtgN/lsGaAuAri03+PwVAnbjcl2a23sy2ITY3cm0AcwDUI/lkMP9trqsIicjfTx2oyO6mAjg8yefuwu6/U8Xi7u+0P+fNzETsCBMWm5w8/kzQ9rj7GQAKmdlaxI58xwC4Fnko5C4ifx91oCK7GwWgKMluWQ+QPBTAOsQKGRckWRnAsYhNVB5vHoAWJAuQrIVclOEKQ7ISgAJm9i6Au5Bc2TsR+ZvpO1CROGZmJM8C8DjJ2wFsQ6xj7AmgFGLVKwxAbzNbRrJO3NO/ATAXsVOv0xCrSPJ3qAFgEMmsP3jv/JvaFZE8UDUWERGRJOgUroiISBLUgYqIiCRBHaiIiEgS1IGKiIgkQR2oiIhIEtSBioiIJEEdqIiISBLUgYqIiCTh/wGxD0+yboe0tAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 576x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create a sample matrix\n",
    "matrix = np.random.rand(40, 40)  # 10x10 matrix with random values\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Create a heatmap using seaborn\n",
    "sns.heatmap(matrix, annot=False, fmt=\".2f\", cmap='viridis')\n",
    "\n",
    "# Add labels and title\n",
    "plt.title('Heatmap of the Given Matrix')\n",
    "plt.xlabel('Columns')\n",
    "plt.ylabel('Rows')\n",
    "\n",
    "# Show the heatmap\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a442edd5-28b8-4d29-903b-fa990aa88851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdAAAAGDCAYAAACbR0FZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAAsTAAALEwEAmpwYAABPMUlEQVR4nO3dd3gU5doG8PsmdCmhdwGl21ARe8OGWMAKNsTGOfbePdg9qEfQT0XFhl1BRVGxC4gNKaL0DhKaVAHpyfP9sRNdIjvPZuOwWXL/vPYy7Nz7zmS2vJnZed+HZgYREREpnFLp3gAREZFMpA5UREQkBepARUREUqAOVEREJAXqQEVERFKgDlRERCQF6kBFCiB5KcklJNeSrJFEvgfJb7bHtgXrO5TktO21vnQgeQ7Jz9K9HSJh1IEKAIDkXJJHF7jvH+sYSBrJZv9EW1EiWQZAHwDHmlklM1teYHmT4HcpHeE2NCf5JsmlJFeTnEHycZINAcDMRppZy6jWH7Jdc0luIlmzwP0/BfukSRJtJLX/zOw1Mzu2iJssEil1oCJbqwOgPIBJ6Vh58EfGKAALAextZlUAHAxgFoBD0rFNBcwBcFb+P0juAaDiP7mCKP84EfknqQOVpJGsT/Kd4MhoDsmr4pa1J/k9yVUkF5F8gmTZYNnXQezn4LRoV5JHkMwheRPJ34LHdCHZieR0kitI3pZM+8FyI3kVydkkl5F8mOQ2X98ky5F8lOTC4PZocF8LAPmnRleR/GobD/86bvlakgfGtfs/kiuDfXN83P1VST4fbPcCkveRzEqwm+8C8K2ZXWdmOQBgZr+Z2aNm9mbQ3hEkc4Kfbyb5doHf7zGS/+etO/8MQ6LtTuAVAN3j/n0+gJcLrP+E4Kh0Ncn5JO8K23/BdnxLsi/J5QDuij/7QfKg4DltFPx7r2B7WznbKhItM9NNNwCYC+DoAvf1APBN8HMpAGMB9AJQFsAuAGYDOC5Yvi+AAwCUBtAEwBQA18S1ZQCaxf37CABbgvbKALgEwFIArwOoDGA3AOsBNC1E+8MAVAewM4DpAC5O8LveA+AHALUB1ALwHYB7g2VNgrZKJ3js35YH+2lz8DtkAbgUsSNIBssHA3gGwE7BOn8E8K8E7S8G0MN5ro4AkBP83BjAOgCVg39nAVgE4ABv3d52J3qNIPZHRuvgMTnBNhiAJnHbt0fwmtkTwBIAXZz9twXAlcHzWwFxr70gcz+Ar4JlEwBcke73jG66pX0DdCset+DDcS2AVXG3dfirA90fwK8FHnMrgBcTtHcNgMFx/95WB7oeQFbw78pBZv+4zNj8D94k2+8Y9+/LAHyZ4LGzAHSK+/dxAOYGP//tA77AYxN1ADPj/l0xyNRF7JTwRgAV4pafBWBYgva3FPg9rgiei7UAno3bdzlxmW8AdA9+PgbArODn0HWHbXfIa+RoAHcA+C+AjgA+R6zT+7MD3cbjHgXQ19l/BV9bPbB1B1omeD1MAPAJEnTyuum2PW/6rkHidTGzL/L/QbIHgIuDfzYGUJ/kqrh8FoCRQbYFYhfftEPsg7g0Yh94YZabWW7w8/rg/0vilq8HUKkQ7c+P+3kegPoJ1ls/WJ5MNlmL838ws3Ukgdi2V0fsw39RcB8QOzKbX7CBwHIA9eLaegLAEyTvA9AwwWNeR6xjfBnA2cG/gdhz5q070XaHeQWxU7FNUeD0LQCQ3B9AbwC7I3a2ohyAQU6bifZH/rZtJjkAwP8BuM7MVAVD0k7fgUqy5gOYY2bZcbfKZtYpWP4UgKkAmlvswpfbADBRYylIpv1GcT/vjNjpyG1ZiFjnkky2oMJ+cM9H7CiwZtx+q2JmuyXIfwng1EKuYxCAI4KrdE/BXx1oYdedFDObh9jFRJ0AvLuNyOsAhgBoZGZVATyNv56rRPsvdL+SbADgTgAvAniEZLkUNl3kH6UOVJL1I4A1wUUrFUhmkdyd5H7B8soAVgNYG1zccWmBxy9B7HvTVHntA8CNJKsFF5tcDeCtBG29AeAOkrUYG5LRC8CrSW7HUgB5SPJ3MbNFAD5D7EO/CslSJHcleXiCh9wF4FCSfYJOA8E2tg5Zx1IAwxHrXOaY2ZQU110YFwHoYGZ/bGNZZQArzGwDyfaIHRXnK9T+AwDGDosHAHg+WO8iAPemuN0i/xh1oJKU4FTriQDaInb0sQzAcwCqBpEbEPugXAPgWfy987oLwEvBVbRnprAJXvsA8D5ip3XHA/gIsQ/cbbkPwBgAvyD2ndq44D6Xma1D7IKWb4Pf5YAkHtYdsVOZkwGsBPA24k7TFmh/OmLfNzdE7KrlNQC+RewI+T8h63gdse8nXy9wf9LrLgwzm2VmYxIsvgzAPcG29wIwMO5xqey/qxC7AOo/wanbCwBcQPLQIv0SIkWUf5WgSEYjaYid3p2Z7m0RkZJBR6AiIiIpUAcqIiKSAp3CFRERSYGOQEVERFKgDlRERCQFxXYmolZ39XXPLdeYlOe2s+DELW6mzKKybqbVwbPdzMbra7mZ6Rf6hSvu6PC+m3nnuP3czPQrE01cE9P4o01uG0vb+uPVcw/93c28tM8AN9PjqavdzL0X/23im7+Zt8l/Hl6d6++/1eNrupmdP13vZmad77/NmjZe4mbsodpuZu7Z4csrTvNf63ll3AiavjDPzaxpF/76A4Dyyza6mf2f8Ca0AsYdU8fNrD3QH3paadRcN5O3cpWb2XjkXm5m7omJ6gn8pUxt//U19uD+ocuPueUat42qU9e6mc9+7PVPToyylbzFLYr0XWKputMj27YwxbYDFRGRkiEP/sFQmHSdSo2yKHArAJ0BNAjuWgBgSP4sKSIiIgCQa0XrQNN1JBhJx03yZgBvIjb/5Y/BjQDeIHlLFOsUEZHMlAcr0i1douq4LwKwm5ltjr+TZB8AkxCr1PA3JHsC6AkAdU48A9n7HritmIiISNpFdeo4D9suD1UvWLZNZtbfzNqZWTt1niIiJUNeEf9Ll6iOQK8B8CXJGfirzt/OAJohViBYREQEAJCboRP6RNKBmtknQQHk9tj6IqLRcQWURURE0vo9ZlEU26n8mrz8oLthra6Y6rYz9UG/dnDWH/6Z7GdPf8bN9LruEjdTeUyOm6k+aFslFrc28bU2buaYi34IXT7+yj3dNmZe4A8KrPW1/3dYXhl/mNb6Tv540k0zqriZ5o/4BVkqveu/7pff2djNzLvIP32Uu9Ife7nrwM1uZvapfjt1vwtf3uq6iW4bX/3sv7aavepvb+k1/hjPfw0c4maePeIwN5NzZhM3s/sZ/gCAXwYnLLv6pz+a+scArfssdTNr2/jjjOcf779vmr4Tvj0175rrtjGg6VA3s1O9eZGNtVy1sFGROqLs+vM1DlREREqe3Aw9AlUHKiIiaZWpp3DVgYqISFrpIiIREZEUpG8gStGoGouIiEgKdAQqIiJppYuIREREUpCbmf1n8e1Anzn0JTfTt/TBbqb8b37NvfPP+NzNXPn0v91MOb8kIbCfXyOxVbkxbqZ6F3886T47zQ1dPq93dbeNrF8buZnleyYxDrScP3au9Ca/nZo/uxHM7lfXzeSOrOxmqjX039XNHvbHrpZasczNzLrY38+l/ZKNqPrJpNDlo7r6Y1vrDfO/2VlXz68TiyQyzy7wx3gu7+Bv82kXDHczA8Ye5GYaT/DHt9b6xY1gek//w6DZf35yM/c+PMPNjNy3ZejyeYf7r+P3f/bfM06p2SLJ1O9Ai20HKiIiJUMu0jIPQpHpIiIREZEURNaBkmxF8iiSlQrc3zGqdYqISObJs6Ld0iWqgtpXAXgfwJUAJpLsHLf4gZDH9SQ5huSYoW+sjGLTRESkmMkFi3RLl6i+A70EwL5mtpZkEwBvk2xiZo8BiX9bM+sPoD8AfDqnTYZelyUiIoWRqd+BRtWBljKztQBgZnNJHoFYJ9oYIR2oiIhIpojqO9AlJNvm/yPoTE8EUBPAHhGtU0REMlCesUi3dInqCLQ7gC3xd5jZFgDdSfqFNQFsMn/Tdh+22s3Me9tf13O/HOJmmj81wc10Hxs+Bi9ZD/Tv5mZO7z7czfSeclzo8l2rL3fbqF7VH3y4bnwFN1N1jv8i31KhopsptTmJM/uT/DGetfZf4mZOOckfdDp3Qw0388XQfd1M+T397/wPqDfPzUwbGV7/tnnNX902Ng33XxezHqvtZjauKu9mSvf1x792vu8rN/PsN0e4mZo/+scLc0/f4mba3O4/Dy3GuxHM+s8+bmZATi03s2pD+PuvZu5ct41nLz/VzZz9qRtJmU7hxjGzhKP8zezbKNYpIiKZKTdDR1RqIgUREUmrdJ6GLYrM7PZFRETSTEegIiKSVvoOVEREJAW5lpknQ9WBiohIWuVl6LeJ6kBFRCStMvUULs2K54x5h538sLthK1r5/f/aXf06lGVrrnczu9y90c1MvaKqm2nyrl/5rsLsFW7m7I9GuJm7B3cNXZ5X1n/uy6zxX9gb6vs1FMcc/5ib6X74OW5myn/8cZen7OnXWXxv0l5uxtb5r6/9d5/lZkb/0MLN5JX3Xxctn/fH5HJ6+BjFy8ePddu4/z/nu5nFx/jjJbOWlXEz5Zf7r6/cJEqPVp7nv5Yf6PWsm7nyp7PczJZZldxM8z7+62LNgU3dzILT/PdW1oLw8bYVF7lN4LXrHnEzezTKiayX+2puyyJ1RB2aTEtLD6wjUBERSSt9ByoiIpKCvAw9hbvdun2SL2+vdYmISObIRaki3dIlkiNQkkMK3gXgSJLZAGBmJyd4XE8APQGg2Z6nol7jA6LYPBERkSKL6hRuQwCTATwHwBDrQNsBCP2mOr4eaDIXEYmISObL1O9Ao9rqdgDGArgdwO9mNhzAejMbYWb+5aMiIlJi5KFUkW7pElU1ljwAfUkOCv6/JKp1iYhIZsvN0MnkI+3UgrJmZ5A8AYBfvDNOuWX+uMs/Gvl/eVT8NcvNNHw+ibPFOX79yF2br3Mzpz3mj1H8dGl4TUcAuPunE91M88dmhy5f0cEfh1ZxySY3s6Cnnxm0xh8Luei4em6mzhf+czV4o19/E+X88cGNC36Tvw0vnPixmzngnZZuZk0L//daeo8/9rLOJeFjFG996gK3jY43/eBmhj++v5u5+dbX3Mw3q/3Xxefz/P130unj3MyaPL8+aW6u/5nS/LG5bsZqV3czlYZPczOtRvqfX6tfDx9/zlF+TdHTR/d0M9P80q0pUzmzEGb2EYCPtse6REREtgedVhURkbTKy9CLiNSBiohIWukUroiISAoy9SKizOz2RURE0kxHoCIiklaqByoiIpKCTJ2JSB2oiIikVaZWYym2HWje/X5R6UprKruZTauy3cw5r33iZu787DQ3s8t9fvHbV2v4EyD83tT/a+y6C/1R/m2/Cy+ufOdZF/rb0ryim9myyZ+U4OExx7qZGuv9yQSyB/kTUaxuso+bObLLz26m3QFz3cxu713pZo7u5q9r2Nd7upm16/3K0mserRO63Ob6+3js8oZupsocf/KMJy8/082888LjbuarV9u7mWFPHexmXjzJ/7h7/sT+buY/L3ZxM5VvcCN4dsKHbuaMm/yG9qsRPonEh0fUdttodas/CQxO9SOpytQj0MzcahERkTSLqpzZ/gCmmNlqkhUA3AJgH8QqtDxgZr9HsV4REck8mToONKqtfgFA/jmBxwBUBfBgcN+LEa1TREQyUJ6xSLdkkOxIchrJmSRv2cbynUkOI/kTyV9IdvLajOo70FJmlj/zdTszy/9S6huS4xM9KL6gdsvrjkKDE/3vhUREJLNFfQRKMgvAkwCOAZADYDTJIWY2OS52B4CBZvYUyTYAhgJoEtZuVFs9kWR+2YefSbYDAJItACS80sbM+ptZOzNrp85TRKRkyLNSRboloT2AmWY228w2AXgTQOcCGQNQJfi5KoCFXqNRdaAXAzic5CwAbQB8T3I2gGeDZSIiIttLAwDz4/6dE9wX7y4A55LMQezo073EPqqC2r8D6EGyCoCmwXpyzMwvqikiIiVKbhHHgcZ//Rfob2b+mKStnQVggJk9QvJAAK+Q3N3M8hI9IOqC2qsB+IPgtuG3D/3qrQ0+X+5mBn7S1810Payrm+k1dLCb+WH/Xd3MZ5PbuBms94voDrqqo5t56JTwdlqMHuO2UfUBv8h11iZ/fGIp+uMPK7/rP5/Te+/tZpq9udbNjEii6Pacd+u7mQb9lrqZYTP9otG9O/vFp+9/9Fw3U3fkytDl3PiH28Y1p/tFwq+6vJub2eUafyz38b90dzMVlvmvnRMe/crNvPys/565cKRfcLzFk/4Y2NzK/nv4rCnnuZlKORvdTO9634Qu/3R1O7eNvMp+sfEoFbWcWdBZhnWYCwDEdyoNg/viXQSgY9De9yTLA6gJ4LdEjWbmtcMiIrLDyAWLdEvCaADNSTYlWRZANwAFZ6P5FcBRAECyNYDyAEL/QlYHKiIiO7RgVMgVAD4FMAWxq20nkbyH5MlB7HoAl5D8GcAbAHqYWejpj2I7lZ+IiJQMRT2FmwwzG4rYxUHx9/WK+3kyAH9OyDjqQEVEJK0ydS5cdaAiIpJWqsYiIiKSgkw9As3MrRYREUmzYnsEuvngNW7m+Et+cDMH97nWzey0f8Jxsn8a1CGJGpOfzXAzY2r641vrVVrtZlo/5M9JcRDD65N+eOmhbhul1y9zM92bjnIzj43r4GZ6/jjSzYz8fYubGbFuLzdzWRd/rONH3x/uZhbM8MfAll3hjwl87gJ/fPDq59e7mXqDwp+v1Yf6Y5UfuKKHmym1bxk3c9Sn37qZJ0Yc42ayjt/gZl4c4I/xPPuiL93MzHW13MyUZnu4mUrz/W3es7o7UxzmzPc/m9oMvTx0edkkDpOWHFjVD0Uo2Qnhi5ti24GKiEjJkKnlzNSBiohIWukINE7cTA8LzewLkmcDOAixAaz9zSz83KKIiJQYeRl6BBrVVr8I4AQAV5N8BcAZAEYB2A/Ac4keRLInyTEkx6z6zJ+nVUREJF2iOoW7h5ntSbI0YhP21jezXJKvImRy+fgJgVu9e48/g7SIiGS8XJ3C3Uqp4DTuTgAqIlacdAWAcgD8y/dERKTE0HegW3sewFQAWQBuBzAoKKh9AGKVwEVERABsn7lwo0BnsvnUGybrA4CZLSSZDeBoAL+a2Y/JPP741re6G2al/J2+6EE/U/3xSm5mXW3/wHnog33czIWzu7iZfbN/dTMDJhzgZrJHVAhdXm61P8as6qRVbmZDH3984ikNxruZoUt2dzPT5tV1M7OPfcHNnDbLH3/47/rD3MyjhxzlZnqM8Mcr97n7LDeTtdF/r+5+0y+hy/esNN9tY9iKVm7myOpT3Uz10n7t0X5z/bG2lS/338PT7qriZhq96h8v3PzEy27m2zV+fdc3hvtzkjcd7NcVXXSwX6ez0aPjQ5cvuaCt24ZXRxYAPhl/T2SHiVeOO6dIHdHj+7yWlkPYyIaxmNnCuJ9XAXg7qnWJiEjmSrKmZ7GjcaAiIpJW+g5UREQkBZn6Hag6UBERSatMLWeWmd2+iIhImukIVERE0koTKYiIiKQgU78DjWwcaFFdNu5cd8NGDNzXbafed+vcTKnN/njIOo/OczMjx/vj5yzL39/Vx/l/19T92B/Pt3nnmqHLW/Sd7LZxSBW/xultX57hZrDF/wuz0Rf+vllyjj/mtMI3/rje1e38eo1tm+a4meUbKrqZi3b262JWLLXRzQz7vbWbGf14eN3aKvP89fy+i1/jtHy3xW4G/Wq7kZyTc91MmcVl3UyTD/wxp+vu8msMX950uJt56axObmbNLv5rcMsFy93M0hk13MzlR38WunzCmoZuG8l4qf3zkR0mnjfq4iJ1RK/s/9yONQ5UREQkGbqIKA7JqiR7k5xKcgXJ5SSnBPdlR7FOERGR7SmqE88DAawEcISZVTezGgCODO4bGNE6RUQkA+UZi3RLl6g60CZm9qCZ/flFiZktNrMHATRO9KD4eqCT3/W/exMRkcyXZ6WKdEuXqNY8j+RNJOvk30GyDsmbASS8+sXM+ptZOzNr1+bU5hFtmoiIFCc6At1aVwA1AIwIvgNdAWA4gOoAkrhkU0REpHiL5CpcM1sJ4ObgthWSFwB4MYr1iohI5snUq3DTMYzlbiTRgf7wjD/G89hLR7mZwW3aupk7DvjEzbx008lu5uFH3nAzG/L8uqIPTe3qZtY/5588qHDl2tDl0373x+lNuXUPN9O4tD+O9rlnHnUz531/g5tpXnupm9n8gz+k7Kub+ruZk6651s2sOtsfW/ho33/mpEvFU/yxl+trhX8QVTpvmdtGzdN+dzPT9vLHpLa9frabWfplMzeTPcN/PpfvuZObqXOh/9rp9chJbqZxpSw3c0dv/xjh+hcucjPZK9wIup4aXgP2y6X++PQpc+v7K2rvR1KlaixxSCZ6RgmgToJlIiJSAqkD3VodAMchNmwlHgF8F9E6RUQkA6kD3dqHACqZ2fiCC0gOj2idIiIi201UFxElPLlvZmdHsU4REclMOgIVERFJga7CFRERSYGOQEVERFKgDvQfdvF1Q9zMujy/TmCNWv44vX59TnUzpav6Y9HmbPTHVb790DFu5v37HnIzl3bu6WZWtKsaunzJFH/8ZuOy/u999v8+dDMj1u3qZla29N9EpR/b2c0svHqTm+l25qV+O939/XNJ8zFuplzLzW7moIr+3M833Hi5m2k04bfQ5VN2r+W2UfFyv35ks7f8OrvZB/iZLP+pwuIjtriZls/49V0bDPbHt/au/a6bufCL69zMbX38MZ7P3/C4m7nlSv912vPI7qHLD3tvgtvG1J/995X8XbHtQEVEpGTQEaiIiEgKMrUDjaqgdhWS/yX5CsmzCyzrF8U6RUQkM5mxSLd0iaoay4uIzTr0DoBuJN8hWS5YdkBE6xQREdluoupAdzWzW8zsPTM7GcA4AF+RrBH2oPiC2t+8tTCiTRMRkeIkDyzSLV2i+g60HMlSZpYHAGZ2P8kFAL4GUCnRg8ysP4D+ANBv2pH+5Z8iIpLx9B3o1j4A0CH+DjMbAOB6AElcuC4iIiVFpn4HGtVcuDcluP8Tkg9EsU4REclMmXoEWmwLav/fm34Ba+b6K7vlvEFuZuFV2W7mgqrj3cxxD9/oZupOWu1m6mT5E0RsqlHRzez87/DB+UvnNnLbWLZ7BTfz7EOd3Uy7y35yM9Wm+Wft971jrJup1H0XNzP7rJpupsZoN4LvWvnrWvxqUzdz+d3T3cwZd3/qZp57+fjQ5bs2mu+2kdvAPzF1aDd/e79d7u+b6lOSeBObX4S+2mN+8e4b63zmZm6a50+qUutbvyj55joJv6n601X3XOFmNrTyO5aKI8ILhTcs41flrt8qfAIO2TYV1BYRkbRK52nYolBBbRERSSudwt2aCmqLiEhSLEPHXKigtoiIpFWm1gONahiLiIjIDk2TyYuISFrpIiIREZEU6CKifxj9GroYdWlfN9N135PczAc/+ePr9nrUH+NZ/8c/3MzCw6u4mSNvvdrNrG3vv+BallsbunzKkc+5bXS5+0w3s6CTPzLpl4faupn3+vzPzRwz9hI389ZQ//fq/INfqHjdhp3czNyPkhjjeb1fHP7ttf4+HLO6sZvZWC18eXY5v/D0lKHN3czzTf3i8VUn+eM3s5IY1Lahln+FSb/GflH3Y+7238Pr6vvb0wT+ONCmD01zMwvOq+dm9nnLb2f8ieEF0F8+/0S3jSqLV7kZ+ENtU5apFxFtt+9ASfrvOBERkQiQ7EhyGsmZJG9JkDmT5GSSk0i+7rUZ1UQK1QveBeBHknsDoJn5U2OIiEiJEPV3oCSzADwJ4BgAOQBGkxxiZpPjMs0B3ArgYDNbmcxBX1SncJcBmFfgvgaIlTUzAP4cXyIiUiJsh4uI2gOYaWazAYDkmwA6A5gcl7kEwJNmtjK2TebObxjVKdwbAUwDcLKZNTWzpgBygp8Tdp7x9UBXjfs+ok0TEZHiJM9YpFt83xHcehZYRQMA8RNB5wT3xWsBoAXJb0n+QLKjt91RTaTwCMm3APQlOR/AnYgdeXqP+7MeaOv/9M3Qr5VFRKQwinoRUXzfUQSlATQHcASAhgC+JrmHma1K9IDILiIysxwzOwPAcACfA/DLh4iIiPzzFgCILz/VMLgvXg6AIWa22czmAJiOWIeaUORX4ZrZEABHAjgaAEheEPU6RUQkc2yHgtqjATQn2ZRkWQDdABQcX/YeYkefIFkTsVO6oYN3tss4UDNbD2Bi8M+k6oGW2uS3e9bMLm5mt0+WuJm9fzzHzQy63B+j2KXyDW6m2lT/XMWWs5e7me/2etnNVGD4OLwh65xBgwByq/knDj664SE3c+GZl7uZAz68zs20bu3Xs+z00bV+Ow8udDNTbi7vZpq/7I+r7L/Or21bd2TBwkV/d9wbP7iZX5a0CV3epdY4t43lY/zxpl9d8bybOWpgwimx/7SqmV/7ttwK/wPy7M7++ODKtTe7mbyT17iZpQv8WrLr7852M6cO9sefV88KH8sNAGVKhddU/aZMXbeNpR2SGAAboagvIjKzLSSvAPApgCwAL5jZJJL3ABgTHOh9CuBYkpMB5AK40cxCP4xVD1RERNJqe1zwYmZDAQwtcF+vuJ8NwHXBLSmqByoiImmluXC3pnqgIiKyQ1M9UBERSa8MHbRYbCeTFxGRkkGncEVERFKgaiwiIiIlSLE9Aq3cwZ3HF2seaeRmvmzo12tkOX97xrb215VX1v8zqvT6PDdzfKOJbqb9dwWnevy7xo86fx/l+dvb87XBbqbTWH9bSrXza2s+fPSrbub+vue6mZajVruZX7v5z2dFf8gpltzqD1huXTPHzfzQ1q+vsOemqm5mn67hr53H7/fruy7r5L8udvnMH+OZdZxfD/SqTh/57cB/zwxY6Ne8/KOef5pwzD7++Oq9f7rGzbz+H79W8TXH9XAz7335ppvpMPfw0OULz/A/4KpOTe8pVJ3CdZCs4Q1KFRGREihDO9BITuGS7B1MhQSS7UjOBjCK5DyS4X8uiYhIiWJWtFu6RPUd6Almtiz4+WEAXc2sGWLFTB+JaJ0iIpKJrIi3NImqAy1NMv/0cAUzGw0AZjYdQMIT8vE13X4bOj6iTRMRESm6qDrQfgCGkuwA4BOSj5E8nOTdAMYnepCZ9TezdmbWrnanthFtmoiIFCfboRpLJKKaiehxkhMAXIpYSZj8QqXvAbg3inWKiEiGytBxoJFdhWtmwxErpr2VoB6oW85MRERKhkwdxkLbzpcwkfzVzHb2cgd8equ7YUum+3X5rjrar7m3T4W5bmZ5rj+O8ZOVe7qZz3/wM6yx0c3U+tivVbm0Y3ityr2b+OMTp34QWpAdAJB1kF/Lcu0cfwxj85v8WpWbDvf339F9RrqZr5c1czM5n/h1MV+/tI+bGbDiYDfz5Uv7u5k6j/mFjI6ZGF4/8qLsCW4b57Q6xs3MvdZ/HjY292ulJqPpi/6H67ye4TUxAWCnUX5t27WN/M/DBiOSWNfs390M16xzM7Me9mv2NnwmfLztLv+d6rZRr5y/vffuMTiyXq7JK72L1BHNPe+WtPTAqgcqIiLppVO4W1E9UBERSVJmnsJVPVAREUkvHYH+RfVARUQkaRnagaoai4iISAqKbTUWEREpITJ0GIs6UBERSatMLahdbDvQ1tUXu5k/Gpd1M6/9r6Ob6X/iGjeTm+uf7b6r7YduZt7N/itlVq+93cyxN37tZmavCx8nW7Ns+JhBAFj7+Hg3U+rVbDfz2/H+eLbqw/2xtt+Py3Iza3L9MbILP/DHeFoSdWJvPqmHmzlz0DA3M2yzPw509bkH+u10mBm6vOc4v9bs1EfauJmKtfxxg+1qL3Ezv3zZws1sqOnXA92zoV+8dUM9/+PuwgbfuJk39/efq7VXJzFab94qN1LuO79u7eadtoQuX3BOXbeNGc385xx+6dbUqQMVERFJQYaewo2qHmg7ksNIvkqyEcnPSf5OcjRJ//BKRESkmIvqCLQfgDsBZCM2ccK1ZnYMyaOCZf65KBERKRGYoadwoxrGUsbMPjazNwCYmb2N2A9fAkj4BVV8PdBpg6dFtGkiIlKsqKD2VjaQPJbkGQCMZBcAIHk4gIQzMcfXA215SsuINk1ERIoVY9FuaVLoU7gkqwFoZGaJJowHgH8DeAhAHmJz4l5KcgCABQAuSWE7RUREipWkjkBJDidZhWR1AOMAPEsyYR0nM/vZzI4zs+PNbKqZXW1m2Wa2GwAdWoqIyF928FO4Vc1sNYBTAbxsZvsDODrFdd6d4uNERGRHlKEdaLKncEuTrAfgTAC3e+F/oh7ovpXnuZkuNX5yM/9XuYObWbsy281UqugXuR5w9gluZno/f3R+tR/dCA6sNMPNXF59VOjys6ad5bax4sKGbqbeiBVuJvssf6B7+6pz3cy4Za3dzJd9D3IzFfL8d92Wrv7vta6D/7p48K3T3MyLNz7uZi7uf6WbKX3ILqHL235wtdtG1lr/7+r3j+/vZk788d9uZp9jpriZ0WX953zV0lpuZuPG8MLTAHDjBL/WRatn/UkkOr7xrZt5+Ynj3cxOv/mv00UHhX+MV+mZzMf86iQyEcrQq3CT7UDvAfApgG/MbDTJXQCEfYKrHqiIiCQnQydSSLYD/cDMBuX/w8xmAwj7s1r1QEVEZIeWbAc6keQSACOD2zdmlvA8huqBiohIsnboiRTMrBmAswBMAHACgJ9Jjo9wu0REpKTYkS8iItkQwMEADgWwF4BJAPyyBSIiIjuoZE/h/gpgNIAHzMy/tE5ERCRJO/QpXAB7A3gZwNkkvyf5MsmE33OKiIjs6JI6AjWzn0nOAjALsdO45wI4HMDzUW3YL2v9QrKD7vSLZZdducnNdHl6vJsZMvBgN3P7wKfczH9POMPN7P56eFFkALjuJf/vl/LLwpfn+vXIsdNyv5jx1OsruZnsdf66nnvJHxe3ceeEUyn/qeMp/hi8VZsrupkZPZq6mQ5v+gWq63Ub6WbO/e5iN1N6b78Aepmx4U/qXrv543E3XeQ/n6M7++/PVrV/czOX1B3hZqbtUdvN1LzLfzEvusP/LKj2md/OiW/63171/fBEN3PpFZ+4mS9P3svNZE8MLyC/3yuT3DZOrzrGzQD3JZFJUYYOY0l2Kr8xAL4HcAqAKQAOM7PGIfmqJHuTnEpyBcnlJKcE92X/I1suIiI7hh35IiIAx5vZ0kK0OxDAVwCOMLPFAECyLoDzg2XHFmorRURkx7WDfwe6iWSf/FqdJB8hWTUk38TMHszvPAHAzBab2YMAEh65ioiIZIpkO9AXAKxBbC7cMxGbOPHFkPw8kjeR/HPeW5J1SN4MIOGXMPEFtae8Oz3JTRMRkUxGK9otXZLtQHc1szvNbHZwuxtA2KzVXQHUADCC5EqSKwAMB1AdsQ54m+ILarc+tUWSmyYiIhltB/8OdD3JQ8zsGwAgeTCA9YnCZraS5IsAPgfwg5n9efkgyY4A/MvPRESkZNjBvwO9FMCTJOeSnAfgCQD/ShQmeRWA9wFcgdg8up3jFj+Q6saKiMiOJ1NP4SY7DnQ8gL1IVgnu+gNANwCJ6n5eAmBfM1tLsgmAt0k2MbPHECtp5jqvpj+Wb1rf2W7m8+W7uZmfO+/sZhrbr25m7vk13cys8/zMzGkV3Eytaf74zIqLnXFv5r/yyk7xxw3+Ucc/3b7T/Gw3s7K5G0GzN/36m4OWHOZmmrzn1/rc0KCymxmc44/TW/hrDTfDzUm8LWolPOnzpw01wt/S8+Y2cNso0z18XCEAfLx8DzczY5n/Wn/4gs5uJntnf1zqukb+R1ndKgvcTNa3i9zMq/f6dX+bf+zXOX3v+2PczLJzs9xMk3fDX8vvvua/H94p7WcmRzgMNFOFHoGSrELyVpJPkDwGsQuJugOYiZDvMgGUyj9ta2ZzARwB4HiSfZBkByoiIiWEsWi3NPFO4b4CoCViVVguATAMwBkATjGzsD8dl5Bsm/+PoDM9EUBNAP6friIiUnLsoBcR7WJmewAAyecALAKws5ltcB7XHcCW+DvMbAuA7iSfSXVjRURkx5Opk8l7Hejm/B/MLJdkThKdJ8wsJ2SZ/+WmiIiUHDtoB7oXydXBzwRQIfg3AZiZVUn8UBERkR1XaAdqZv4lYCIiIkWQqadwkx0HKiIiEo3tcBERyY4kp5GcSfKWkNxpJI1kO6/NZGci2u4eOPY0P/T7ajcy5a5d3cwlHw53M28/eZSbeenfddxMtRv9ojbLJ/vj5xYdt9nNZNcKL8JZ5y7/6c9tXM/NlPnDjWBjFf9S8wYj/IZKbfB/76Zv+K+L84YOdzO1svx2Ln+9p5tpdpA/lrZsN3+M58xr/fG2fzgvwaxF5dw2Nu3sj7Vd6pfixc70x12ierYbKTfdH5u56IImbmZDjl9XtOUmfw7uTZX8447dh/mvnfemuZeTwHL8urULjq4eurzWz/57Ju+GwhTbikDER6AkswA8CeAYADkARpMcYmaTC+QqA7gawKhk2o3kCDQYP/pfkq+QPLvAsn5RrFNERCSB9gBmBnO5bwLwJoBtDcW8F8CDAPy/bhDdKdwXEbvQ6B0A3Ui+QzL/T98DIlqniIhkoKJO5RdfySu4FTw11ABbVwLLCe77axvIfQA0MrOPkt3uqE7h7mpm+edg3yN5O4CvSJ4c0fpERKSEMrP+APqn+niSpQD0AdCjMI+L6gi0XLBBAAAzux/AswC+RqzM2TbF/xUx//fxEW2aiIgUK9FfRLQAQKO4fzcM7stXGcDuAIaTnIvYmdIh3oVEUXWgHwDoEH+HmQ0AcD2AhDOcx9cDbVS1bUSbJiIixcl2qMYyGkBzkk1JlkWsGMqQ/IVm9ruZ1TSzJmbWBMAPAE42szFhjUbSgZrZTQBySB5FslLc/Z8AuCqKdYqIiGxLMJXsFQA+BTAFwEAzm0TynqJ8tRjJd6Akr0RsY6cAeJ7k1Wb2frD4fgAfR7FeERHJQNthIgUzGwpgaIH7eiXIHpFMm1FdRNQTRawHOvXObDfT7Dy/HmiT95u4mfd+7OBmWv3Lr+/XstISN1OvzCo389xbfo3ELjeMdDPDLjsodLmNT1TO9S8z+u7nZlr18cf7rWlb183Mvsp/afDXqm5mc42d3MyA8050M5e8+p6baXmY/xqcMLmxm2mdvczNbK6W62Y6HTc2dPmkq3d327C7l7uZ6Xe3djM7zfVPcFVa7Ne1XdzJqWsLIG+jP9ax/Gy/zumCS/Z0M6W2uBF8MMQfbNC830w3c97I0DOIAICXuoYPyr1s0PuhywHgiXNPdzPwh8KnLkNnIoqqA92qHijJIxDrRBtD9UBFRCSOpvLbmuqBiohIcjK0HmhUHWh3AIvj7zCzLWbWHcBhEa1TRERku4nkFK7qgYqISLIy9RRusZ1MXkRESgh1oCIiIinI0A5U9UBFRERSsN2OQEnWNrPfks4v9esWthnrb/6QSf44s7of+X/+rOzkZ8qM9MfpDT79EDeT++AKN/PFkpZu5uh+34Uur1jKH1/XpdSHbuaD3v6F1YsO8v9Wa3b+ZDdz0ni/NuRHR+3mZpactIubufGzs9xMr6PeczP7VZvnZvY+Zq6bWZfnvyduGxK+zeUO80eR7VHer186q7L/Wl/X0F/XWv9pwAHN5vjb83QrN7PhtJVuZvVyfwxx6/v8MbuzetR3M9Ov92sV3z6qoZvZ5/Hw5+uGgee7bVRrkd5DQH0HGodkwQqvBPAjyb0B0Mz8HkJEREoGdaBbWQag4J/dDQCMQ2xXJfF3p4iIlAjqQLdyI4BjANxoZhMAgOQcM2sa0fpERCRDZeop3KiqsTwC4GIAvUj2IVkZSfyNEV8PdPV330exaSIiIv+IyK7CNbMcMzsDwHAAnwOomMRj/qwHWuWgA6PaNBERKU4ydCq/yK7CJdkKse89v0KsA901uL9jUBdUREREp3DjkbwKwPsArgQwEcCxZjYxWPxAFOsUEZEMpSPQrVyCItYDZd0NbuaDEX6tysrz/NVZD394qn3gj5n86sqD3Uylp/zamXW6+tu85TX/qXv+5/B6oNlfV3DbWH/cajdT9tQqbqb2OH88bseflrqZfq+c5GY23OC/o5q/7v9eNcf7z8PjE05zM3WH+a+vz1r7NRYWHJnEa7l6eF3M8q3WuG2My/HHHlapudbN1Gnqr2vxBzu7mVGb/TGe9c7xa/EeXMsfj9uqhT/O+LnD/Hq9uz4xw81MudMfjFB2rl/DdMrk5qHLH7zwZbeN/qf576tIZegRqOqBioiIpED1QEVEJK1YxFu6qB6oiIikl74D/YvqgYqISLJ0Fa6IiEgJonqgIiKSXhl6BKoOVERE0ksdqIiISOFl6neg27Ogdg0zW55svtwv/iD/Rp+ucjOLDs92M+VLhw8+B4AVZ+zlZjZU8y+oLnOZvz0Lz6zhZjrX+NrNtKoSPoC/2T7+4POPu/pzEu/3qr8tw+7yJ5loXNafSCF7hj8hw90XP+dmBhzkFzY/ocYvbuaOj7q6mfJd/E+H+pVmu5kyG/z3xG1Nh4Yuv/mhnm4be57nTwIwdqJfWKn+Hf7vvel0N4K+nf2JAJ4+KHzSEADY7Wt/EpNBi/Z1M9Um+pNIWN2abqblDf7ra/Ele7uZch3D3zfXDz3XbaNh0zT3YBnagUY1lV9vkjWDn9uRnA1gFMl5JA+PYp0iIiLbU1RX4Z5gZsuCnx8G0NXMmiFWI/SRiNYpIiIZiFa0W7pEdQq3NMnSZrYFQAUzGw0AZjadZLmI1ikiIplIp3C30g/AUJIdAHxC8jGSh5O8G8D4RA+KL6i98icV1BYRKQl0BBrHzB4nOQHApQBaBOtpDuA9APeFPK4/gP4A0Oa2vhn6N4mIiJQEUV6FuxixznBUfmUWIFZQG4AKaouISEyGHi5tl4LaJOML6KmgtoiI/EWTyW+lyAW1q8zzx/vt9cJkNzN/oD8+jA/VcjPHPvyNmxl73m5upuELCefZ/9PU0dlu5v25flW4U5v+HLr8yXdOcNuo3SzXzbSusNDNfL3efz7v793dzZS5yC9Ofdmb/ljHJ7o962buvOMiN9PwQr8Ac43y69zM2J/94soNvnQjmHhvo9DlZbv4+2/KR+EFmgGgznz/U2tz9YpuptpU/3Vx37RObqb0y347bx/U2s2sPTJ8/wHA8uP9444TT/3BzXx/735upvqUTW6m9Ojw/Vx97Sq3jRr9F7uZKGkiha2poLaIiCQnQztQFdQWERFJQVRHoN0BbIm/IxgT2p3kMxGtU0REMhAtMw9BVVBbRETSKzP7T1VjERGR9NJFRCIiIqnI0A40qouIREREdmjF9gi0Us5GNzNjbW03U2WuPz7s12PLuJnyXVu6mY07V3Izw4ft6WY6HTXOzXwx29+eof8Lrxx33FVj3Da+3duv+/hw325u5rPn/udmuh9whpvZNLOOm6l291w3c/nos91MnR7+mMlLdh7pZl4/zq89ipv8yNJ9stzMh4t3D12+U+8qbhtt/+e//ka+vY+baXXFTDczYnwrN1NmrT+etM++A93MwI/au5lpP7oRPH2iP4b40Y4nu5nuQz5wM1+vauFmvp3SLHT5Lq/4H/MTX2/jZrC/H0lVpp7CjWomonYkh5F8lWQjkp+T/J3kaJJ+hVgRESk5NBPRVvoBuBNANoDvAFxrZseQPCpYdmBE6xURkQyjI9CtlTGzj83sDQBmZm8j9sOXAMpHtE4REZHtJqoOdAPJY0meAcBIdgEAkocDSDi5anw90AWLkvgyQkREMl+GnsKNqgP9N4DrAVwI4DgAR5Jchdjp26sSPcjM+ptZOzNr16Ce/4W/iIhkPhXUjmNmP5O8BkB9ADlmdjWAq4E/64GKiIjEZOhUflHWAx0M1QMVERFHph6B0iLo+UlOAHBgfD1QAK+Y2WMkfzIzdyjL7Jx67oad8pA/eG51c38caKuH57uZBU/64+fqn+vX+sxr3cTNLD6wspt59Oqn3cwD558fvp72Fdw2Dunmjwk8rOp0N/Ofsf64uNxN/jjH7FHl3Iz9Q+dV1h7o1/HMXV3WzZSu6o9p3reR/9r56Ut/7G+FtitCl2c/47+Ol+7l78Azuw13M68POcLNZK13Iygf/isBAP5o4Gd2eW2Zm7GK/vPJTVvczIxb/fdW06fcCOae4LezpWp4zd5Wt/vvz3mX+bVSp9xzbWSlKA88+5EidUTfv359Wspkqh6oiIikV2aewVU9UBERSS/mFe2WLlF1oN0BLI6/w8y2mFl3AIdFtE4REclEGTqMRfVARUQkrTQTkYiISDFFsiPJaSRnkrxlG8uvIzmZ5C8kvwyu2QmlDlRERNLLrGg3B8ksAE8COB5AGwBnkSxYguYnAO3MbE/ERo485LWrDlRERNJqO4wDbQ9gppnNNrNNAN4EED8/AcxsmJnlj137AUBDr9FiWw/0gp7Xupl1h/rtPHrCK27mrqnh4yUBoP7ty93MHx38mnrra/h/s6zee5Obufz5f7uZdd03hy5vccl3bhuVzvW39/5n/XqgjX4J3xYA6PDwN27m5XL+FI9m/kipE1tMdDPvT2jrZsrV8Acy9t/nVTdz49TT3UzTwavcTPvO4b9Xmf+GjxkEgEEvHOlm3py+r5up86M/XrJtr/Fu5tjsCW7mticvcDNYuMSN1P3U/zQ+rrr/2rnt+1PcTJlZC93MrgOru5nFd4U/p2UG+2On+bEbiVYRvwMl2RNAz7i7+ptZ/7h/NwAQP+A/B+EVTi8C4O6VSDpQkqWDDTgFsen8AGABgPcBPG9m/qepiIhIEoLOsr8bTALJcwG0A3C4l43qCPQVAKsA3IVYTw/EDofPB/AqgK4RrVdERDLMdrgKdwGARnH/bhjct/V2kEcDuB3A4WbmTiEWVQe6r5m1KHBfDoAfSPrzSomISMkR/WTyowE0J9kUsY6zG4Cz4wMk9wbwDICOZvZbMo1GdRHRCpJnkPyzfZKlSHYFsDLRg+LrgS76dVREmyYiIsVJ1BcRmdkWAFcA+BTAFAADzWwSyXtI5k/U/TCASgAGkRxPcojXblRHoN0APAjgyaAOKABkAxgWLNum+PPYh3d6KEOH1oqISKFsh097MxsKYGiB+3rF/Xx0YduMaiaiuST7AHgEwCwArQAcCGCymc2JYp0iIiLbU1RX4d6J2IDV0gA+R2wMznAAt5Dc28zuj2K9IiKSeTJ1Kr+oTuGeDqAtgHKITSrf0MxWk/wfgFEA3A607E2L3JXcWn+0m/nf9ee6mTx3wibglEEj3czLt5/kZpYe6I+Na9zAr1tYvp9fM/SaNwaGLn+09J5uG5NOqu1mjh3if1899YNd3MzqXL/24cAD/CvVT3v/ajfzyTR/PGnVJOpQ3nPVG36mRw83s6Wl/7tzzmQ38/ag8FoN2Yf4YyGTqae6fmV5N7N5J/8Si5nn+m++j+/ya1U2/2Cxm7Fcfwzs1zMKXvv4dzNr13QzLx36gpu55YVT3Uz2xavczNqpO4cub33MVLeNKdWauplI5WVmDxpVB7rFzHIBrCM5y8xWA4CZrSfTWXxGRESKnczsPyPrQDeRrBhMi/TnlCUkqwJQByoiIn/SKdytHZY/CNXM4jvMMohNpiAiIpLRoroKd5szOJjZMgD+F3wiIlJyRD+RQiSK7WTyIiJSMugUroiISCoytANVPVAREZEU6AhURETSihn6HSitmG74+T9e5G7Y6Pd3d9vZWM3//Vr09weX59ao5GbWNKnoZrZ09wtz571by82sOsov5Nysx6TQ5XN6tXPbaHzQr25mxiS3cDtQ1h+9VHmK//dck1Nm+9vzlT9pQ5O3k7iWbZ5f8HjVoLpuptqFa93MsmP9gexVuue4mTJXh78Gl7X3CzTbaf5r9IFWg91M330PdjPznvdfO+vX+AWhW/de7Wawdp0bafbeUjfz7TP++2ZtIzeCU0/+1s188X8HuZnsGRtCl2eNCv8cAIB35/jF7HeqN8+vVJ+iDkf1LlJH9NWXt0S2bWEiOYVLMovkv0jeS/LgAsvuiGKdIiKSmWhWpFu6RPUd6DOIVfNeDuD/gonl8/nzV4mISMlhRbylSVQdaHszO9vMHgWwP4BKJN8lWQ5AWg61RURE/klRdaBl838wsy1m1hPAzwC+Qqxg6TbFF9SePtifAFlERHYAZkW7pUlUHegYkh3j7zCzuwG8CKBJogeZWX8za2dm7Vqc0iqiTRMRkeKEVrRbukTSgZrZuQBWkNwPAEi2IXkdgIVmViaKdYqISIbK0CPQyAtqk/wcse9Bh0EFtUVEpIBMLXJZbAtq/7fBx+5KDq7bxs1UmeEfZF/48Rdu5t2l+7qZ6dN3dTP1XqnhZo67bbibGb6kub89z4WPk61Y+Xe/jdn13Awq+IWKW/Xzx+DN6O4XCd9i/vPZ5F2/Evaqvfzn4drBn7mZF7r5BaEXd/aLkm+q6kawdHp9N1P6vPD9k1tvm3UetsJp/r75o4U/NnND+2Zu5ujGv7iZibf4hd+N/rWJ0x/0n4fJ4xq4mdqb/SOe5ofNdTPz1vljch+64xk3c8k7/wpd/q+nkqgMLylRQW0REUmvYjqhj0cFtUVEJL0ys/9UQW0REUmvTJ0LVwW1RUREUqBqLCIikl46AhUREUlBhl4Zow5URETSSt+B/sO6XnGdm6l/2W9uxkb6Y79yNvnj3nJ6++Mud07iNVDh03FuZsTF/vi5cjf79Umvf/Xz0OV9fjrabaNeg5Vu5rcV/vjN6ef7mVaP+89n9st+HdSxvWq6mXNbDnczj8w8xs38drlfA7Z5U7+mas5XO7uZltdPcDPTHtojdPnoDk+4bZzTJXxcIQBMPNoveFnmszFu5rOD/XqXTX/wx4ouuMQfK7pbQ7+W7KIBfl3WdbX9Madbrq7mZko97deb7Tv/WDdTfmn49szd4H++7fbJ5W5m7gVuJHUZ2oFGNRfu35Ccvr3WJSIiErWopvJbg79G9uT/eVQx/34zqxLFekVEJANl6BFoVKdwXwSQDeBGM1sCACTnmJl/fkREREqWDL2IKKpqLFcBeAzAGySvIlkKScw1EV8PdPHcH6LYNBERKWZoVqRbukT2HaiZjQWQf5XKCADlk3jMn/VA6zY5IKpNExGR4kTlzLZGsj1i33f+H8mfABxJspOZDY1qnSIiItvL9qoH2h7AcKgeqIiIFKSLiLZS5HqgR9070l3JyCv2dzOzzvSfmI8vPNTNVOy9wM1c2OBbN9OnylluZvkyv3bmx+/1czOVneFqpx421W3j0DdvcDMVfvPHxZXyS4Zi1b7+mN1KeavdTONH/O157fgj/Q1K4j3dcLz/i83oXNfNlPaH9aL0UL9oaJ3nwr+V6f5wV7eN6bdWcDOzJ7d3M+Vu3MnNtOkw082Mq9/azZT3h1TihkafuplrTzvTzZT+zB9XOf169xsrLB3g1zNe2ca/umaXURtCl9965ZduG58u38fNREod6FZUD1RERJKTob1CVBcRbSKZP0WL6oGKiMgOR/VARUQkrTQXbhzVAxURkaSpAxUREUlBnjpQERGRwsvQI9DtVo1FRERkR0Irpj3/CV9f5W7YxBkN3Xa40f8boeyqLDez1xF+NbaKpTe5mQnL6rmZ+1q/52aWbvEL2vQacUro8uyJ/gmI4y74zs38fJA/bjBvrxZuZv5x/rjBLH+ILDbs44fylvrj9FrsMd/NzFri1x7NW+zvnxo/+WNXq736o5s5YUL4JQbPTT/YbWPNSr/GKdb6r51SW/zfqewqP7O5pV8DtuHL/vasr+VnVnde62Z67fmRm+ndr5ubufeKl9zMLS/611zm7b0mdHn1t/331SP/fdLNHNR4tv9kpej45jcVqSP6eMZDkW1bmEiOQEleQbJm8HMzkl+TXEVyFMnwir8iIlKyZOhcuFGdwr00uOIWiFVl6Wtm2QBuBvB0ROsUEZFMlGdFu6VJVBcRxbdb28wGA4CZDSdZOaJ1iohIJrLMnF8nqiPQt0kOILkLgMEkryHZmOQFAH5N9KD4eqC/DpkY0aaJiIgUXVQTKdxOsgeANwDsitik8j0BvAfgnJDH9QfQH0juIiIREdkBFNOLWT1RjgOdDOAKMxtNcjcAHQFMMbPfI1yniIhkGk2k8BfVAxURkaTpCHQrRa4H+vyub7sr6V+9nZtpXM6fevfx3qe7mel7+uP9/phU3c083/UpN3Pnvy9yM+XHzHIzreusCl3OPP+L+53/vdzNvPXY8W6m1g/+WNtNzf3xftnZ/hjPhjv5Y/k2vFzfzZS6zx/Xe/RH09zMJxt3czObKpdzM1nVq7mZofuWDV2+8fZst40mX/u/99wu/gfeoftNdjO/nVfLzcw+p46baX3vODcz8YG93Ezj8/z6pF+N8OuTWhJXlzzVbj83s7GXv5/zloePM87L8odI9nj1Sjcz/XY3UuKoHqiIiKRXhh6Bqh6oiIik13aYSIFkR5LTSM4kecs2lpcj+VawfBTJJl6bUXWgh5nZOkD1QEVExJGXV7Sbg2QWgCcRuzanDYCzSLYpELsIwEozawagL4AHvXYj6UDD6oGa2YQo1ikiIhkq+iPQ9gBmmtlsM9sE4E0AnQtkOgPIn6D4bQBHkQz9AlnVWEREZEfXAEB8dYic4L5tZsxsC4DfAdQIa1QdqIiIpFcRj0DjZ7ELbj23x2aroLaIiKRXESdSiJ/FLoEFABrF/bthcN+2MjkkSwOoCiB0HF+x7UAfX36Am6lXdpWbeaTfGW6m1rl+3cc1A/zao3UH+mPR7n/vPDezeq/wsXwAsKS9Pxat0SfhdQI39/Ynhfr4t93dTNYf/hjPLeX9sWiVf/TrZq5o7o+XrPRZVTez00x/fGvtIdv8Kn8r4/u0dTO7Xlzwffp3i6Y18jNn+DVV64xaHbo8d1d/HO213Qa5mVuf868FLLWvf3GHLV/hZg7vtMjNjPi1mZu5pfe7bub+ffzPizmfuREcffZYN1Otu/9cbLxxs5tZ0DN83G7WRr++a+OD/c/AKFn0k8mPBtCcZFPEOspuAM4ukBmC2EWu3yM2l8FX5hTMjqoe6C4kXyB5H8lKJJ8lOZHkoGQuDRYREfmnBN9pXgHgUwBTAAw0s0kk7yF5chB7HkANkjMBXAfgb0NdCorqCHQAYhPJVwXwA4AXAdwD4FgALwDoENF6RUQk02yHuXDNbCiAoQXu6xX38wYA/imIOFFdRFTZzJ4ys94AqpjZI2Y238yeB+DPRyYiIiXHdphIIQpRHYHmkWwBIBtARZLtzGwMyWYA/C/MRESk5EhiMoTiKKoj0JsAfIDYoNQuAG4lOQPAdwD+k+hB8Zci//T2nIg2TUREihUdgf7FzL4k2R1AXlAPdCViUyhNDs5DJ3rcn5ci3/7LqZk5u7CIiJQIqgcqIiJpZRl6CrfY1gMVEZESIkPLmRXbeqBf9zrQzcw/1W+qnF/jGrNm1HMzjX7PdTMfzxnlZtrdtY+b+apXHzfz7trGbubVb08MXX7Jzt+4bfS72b+qu+lKf8KBecf7EyDcc8pbbqbvjKPczJv9BriZf80+zc2MXuhPbtD4X7+6mZwhTdxMw6/DJ70AgN36+wWqhx3fPHR57TJ+0fL6pVe5mfL+/Af46aU93cwJX410M29MbOevbJn/+nrmTf8533isP3FBhfll3Mzk2/ZwM+UX/+FmHhnypJu55fSLQ5fP6upPYrLLHTXdDPyPi9Rth2EsUYiqA91EsmJQ0kz1QEVEJLHoZyKKRFQd6GH5Jc1UD1RERHZEUV2Fm7AeKIBlUaxTREQyk+kUroiISAp0CldERKTwMvUIVAW1RUREUqAjUBERSa8MPYULM8uYG4CexaWd4rQtakfPudrRc56Odkr6LdNO4fYsRu0Up21RO9unneK0LWpn+7RTnLalOLZTomVaByoiIlIsqAMVERFJQaZ1oP2LUTvFaVvUzvZppzhti9rZPu0Up20pju2UaAy+UBYREZFCyLQjUBERkWIhIzpQkh1JTiM5k+QtKbbRiOQwkpNJTiJ5dRG3KYvkTyQ/LEIb2STfJjmV5BSSfg23bbdzbfA7TST5BsnyST7uBZK/kZwYd191kp+TnBH8v1qK7Twc/F6/kBxMMruwbcQtu56kkXRrLiVqh+SVwfZMIvlQir9TW5I/kBxPcgzJ9km0s83XXWH2c0gbhd3Hoe+BZPdzWDuF2c8hv1eh9jPJ8iR/JPlz0M7dwf1NSY4KPjfeIlk2xXZeCz5/Jgavi9B6ZonaiVv+fyTXprgtJHk/yemMfWZclWI7R5EcF+zjb0g2C2tHEkj3OJokxitlAZgFYBcAZQH8DKBNCu3UA7BP8HNlANNTaSeuvesAvA7gwyK08RKAi4OfywLITqGNBgDmAKgQ/HsggB5JPvYwAPsAmBh330MAbgl+vgXAgym2cyyA0sHPD3rtbKuN4P5GAD4FMA9AzRS35UgAXwAoF/y7dortfAbg+ODnTgCGp/q6K8x+DmmjsPs44XugMPs5ZHsKtZ9D2inUfgZAAJWCn8sAGAXggOC90C24/2kAl6bYTqdgGQG8kWo7wb/bAXgFwNoUt+UCAC8DKJXkPk7UznQArYP7LwMwwHst6/b3WyYcgbYHMNPMZpvZJgBvAuhc2EbMbJGZjQt+XgNgCmKdT6GRbAjgBADPpfL4oI2qiH1IPx9s0yYzW5Vic6UBVCBZGkBFAAuTeZCZfQ2gYGnkzoh17Aj+3yWVdszsMzPbEvzzBwANU9gWAOgL4CYASX1Zn6CdSwH0tr9K7P2WYjsGoErwc1UksZ9DXndJ7+dEbaSwj8PeA0nv55B2CrWfQ9op1H62mPwjujLBzQB0APB2cL/7Wk7UjpkNDZYZgB/h7+dttkMyC8DDiO3nUCG/06UA7rGgTGQS+zhRO4V+LcvfZUIH2gDA/Lh/5yDFji8fySYA9kbsr7FUPIrYm6Ao8081BbAUwIuMnQp+juROhW3EzBYA+B+AXwEsAvC7mX1WhO2qY2aLgp8XA6hThLbyXQjg48I+iGRnAAvM7Ocirr8FgEOD03kjSO6XYjvXAHiY5HzE9vmthXlwgdddSvs55LVbqH0c305R9nOB7Ul5Pxdo5xoUcj8z9pXKeAC/AfgcsbNWq+L+wEjqc6NgO2Y2Km5ZGQDnAfgkxXauADAk7nlPpY1dAXQNTm1/TLJ5iu1cDGAoyZzgd+qdzDbJ1jKhA/1HkawE4B0A15jZ6hQefyKA38xsbBE3pTRipwifMrO9AfyB2Km8wm5PNcSOZpoCqA9gJ5LnFnHbAMT+ekWSR36JkLwdwBYArxXycRUB3AagV1HWHygNoDpip65uBDCQJFNo51IA15pZIwDXIjh7kIyw112y+zlRG4Xdx/HtBI9LaT9vY3tS2s/baKfQ+9nMcs2sLWJHh+0BtCrs77OtdkjuHre4H4CvzWxkCu0cBuAMAI8XcVvKAdhgZu0APAvghRTbuRZAJzNrCOBFAH2S3S75SyZ0oAsQ+34mX8PgvkIL/oJ8B8BrZvZuittzMICTSc5F7HRyB5KvptBODoCcuL9w30asQy2sowHMMbOlZrYZwLsADkqhnXxLSNYDgOD/7unOREj2AHAigHOCTqIwdkXsj4Kfg33dEMA4knVT2JQcAO8Gp7N+ROzMgXtB0jacj9j+BYBBiH1QuxK87gq1nxO9dgu7j7fRTkr7OcH2FHo/J2gnpf0MAMHXIMMAHAggO/haAyjk50ZcOx2D7bwTQC3Ern1IWlw7RwJoBmBmsJ8rkpyZwrbk4K99MxjAnilsy/EA9or77HkLRfvMKLEyoQMdDaA5Y1fUlQXQDcCQwjYS/CX8PIApZpbyX1tmdquZNTSzJsG2fGVmhT7iM7PFAOaTbBncdRSAySls0q8ADiBZMfgdj0Lsu6RUDUHsAwzB/99PpRGSHRE7zX2yma0r7OPNbIKZ1TazJsG+zkHsgpPFKWzOe4h9gIFkC8Qu2FqWQjsLARwe/NwBwAzvASGvu6T3c6I2CruPt9VOKvs55Hd6D4XYzyHtFGo/k6zF4ApkkhUAHIPYe2AYgNODmPtaTtDOVJIXAzgOwFn53z2m0M5YM6sbt5/XmVnCK18TbQvi9jFi+2h6CtsyBUDV4DlC3H1SWFYMrmTybohdBTcdse81bk+xjUMQO032C4Dxwa1TEbfrCBTtKty2AMYE2/QegGoptnM3Ym+uiYhd4Vcuyce9gdj3ppsR++C8CEANAF8i9qH1BYDqKbYzE7HvrvP39dOFbaPA8rlI7ircbW1LWQCvBvtnHIAOKbZzCICxiF0JPgrAvqm+7gqzn0PaKOw+dt8DyeznkO0p1H4OaadQ+xmxo7CfgnYmAugV3L8LYhf9zETsSDb0fRHSzhbEPnvyt7FXKu0UyHhX4SbalmwAHwGYAOB7xI4kU2nnlKCNnwEMB7BLMp8Zum1900xEIiIiKciEU7giIiLFjjpQERGRFKgDFRERSYE6UBERkRSoAxUREUmBOlCRAkjWJfkmyVkkx5IcGjdmrmC2CbdRQUZEdnyl/YhIyREM7h8M4CUz6xbctxdic9WGDloXkZJFR6AiWzsSwGYzezr/DotNsv4NY/U3J5KcQLJrwQeS7EHyibh/f0jyiODntcHjJ5H8gmR7ksNJziZ5ctzj3yX5CWN1Qh8K7s8iOSBu3ddGuwtEJBk6AhXZ2u6IzYJT0KmIzRy1F2Lzu44m+XUh2t0JsWkfbyQ5GMB9iE2h1gaxUlv501O2RawqyUYA00g+DqA2YuXLdgdihdgL9yuJSBR0BCqSnEMAvGGxyhZLAIwAUJiyaJvwVxmsCQBGWGzy/wkAmsTlvjSz381sA2JzIzcGMBvALiQfD+a/LXQVIRH556kDFdnaJAD7pvjYLdj6PVU+7ufN9te8mXmIHWHCYpOTx58J2hj3cy6A0ma2ErEj3+EA/o0iFHIXkX+OOlCRrX0FoBzJnvl3kNwTwCrEChlnkawF4DDEJiqPNxdAW5KlSDZCIcpwhSFZE0ApM3sHwB1IreydiPzD9B2oSBwzM5KnAHiU5M0ANiDWMV4DoBJi1SsMwE1mtphkk7iHfwtgDmKnXqcgVpHkn9AAwIsk8//gvfUfaldEikDVWERERFKgU7giIiIpUAcqIiKSAnWgIiIiKVAHKiIikgJ1oCIiIilQByoiIpICdaAiIiIpUAcqIiKSgv8H9LCe87HhNfkAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 576x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Create a heatmap using seaborn\n",
    "sns.heatmap(fwdM, annot=False, fmt=\".2f\", cmap='viridis')\n",
    "\n",
    "# Add labels and title\n",
    "plt.title('Heatmap of the Given Matrix')\n",
    "plt.xlabel('Columns')\n",
    "plt.ylabel('Rows')\n",
    "\n",
    "# Show the heatmap\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7dd63455-c54c-4f7c-98cf-7ff17fbbc2b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Create a heatmap using seaborn\n",
    "sns.heatmap(bwdM, annot=False, fmt=\".2f\", cmap='viridis')\n",
    "\n",
    "# Add labels and title\n",
    "plt.title('Heatmap of the Given Matrix')\n",
    "plt.xlabel('Columns')\n",
    "plt.ylabel('Rows')\n",
    "\n",
    "# Show the heatmap\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b03f9c-0e37-4a0b-a6b3-95c896a333ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d63418-ab7f-4cc4-87b3-c65ebfc9466d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple identity model that returns input as output\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, input_size)  # A linear layer that learns a transformation\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# Instantiate model, loss function, and optimizer\n",
    "model = SimpleModel(len(feature_list))\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Run training loop\n",
    "train(model, dataloader, criterion, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37e61a6-c360-4388-be75-e641b157ce8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
